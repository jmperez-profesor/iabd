{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apuntes y ejercicios pr\u00e1cticas del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data","text":"<p>Apuntes, pr\u00e1cticas y ejercicios del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data en el IES Severo Ochoa (Elche).</p> <p>Los ejercicios y conocimientos contenidos en las pr\u00e1cticas y/o apuntes de  todos los m\u00f3dulos tienen exclusivamente prop\u00f3sito formativo, por lo que  nunca se deber\u00e1n utilizar con fines maliciosos o delictivos.</p> <p>Ning\u00fan alumno o alumna de este curso, ni profesor o profesora como  docentes, ser\u00e1n responsables de los da\u00f1os directos o indirectos que  pudieran derivarse del uso inadecuado de las herramientas y mecanismos  expuestos.</p>"},{"location":"Datasets/Actividad1/","title":"Actividad 1 - Datasets de Hugging Face","text":""},{"location":"Datasets/Actividad1/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"Datasets/Actividad1/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p>"},{"location":"Datasets/Actividad1/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"Datasets/Actividad1/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\ndataset = load_dataset(\"PlanTL-GOB-ES/squad-es\")\nprint(dataset)\n\n# Nivel 2: Dividir en train/test\ntrain_dataset = dataset[\"train\"]\nsplit_dataset = train_dataset.train_test_split(test_size=0.2)\ntrain_split = split_dataset[\"train\"]\ntest_split = split_dataset[\"test\"]\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\ndef add_num_paragraphs(example):\n    return {\"num_paragraphs\": len(example[\"paragraphs\"])}\ntrain_split = train_split.map(add_num_paragraphs)\n\n# Nivel 4: Filtrar y persistir\nfiltered_train = train_split.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nfiltered_train = filtered_train.remove_columns([\"num_paragraphs\"])\nfiltered_train.to_parquet(\"filtered_train.parquet\")\n\n# Nivel 5: Publicar en Hugging Face\n# filtered_train.push_to_hub(\"usuario/nombre-dataset\")\n</code></pre>"},{"location":"Datasets/Actividad1/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"Datasets/Actividad1_solution/","title":"1. Descargar los datos de SquadES desde fuente remota","text":"<p>El primer paso es cargar los archivos de entrenamiento y validaci\u00f3n desde URLs remotas. Utilizaremos la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Hugging Face Datasets, indicando que queremos cargar archivos JSON alojados en GitHub. Los datos remotos se corresponden con <code>train-v2.0-es.json</code> (entrenamiento) y <code>dev-v2.0-es.json</code> (validaci\u00f3n).</p> <p><pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\ndata_files = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\"\n}\nsquad_es = load_dataset(\"json\", data_files=data_files, field=\"data\")  # field=\"data\" porque los datos est\u00e1n bajo esa clave\nprint(squad_es)\n</code></pre> Esto produce un objeto DatasetDict con splits train y val, donde cada elemento tiene las claves title y paragraphs.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#2-dividir-los-datos-de-entrenamiento-en-entrenamiento-y-prueba","title":"2. Dividir los datos de entrenamiento en entrenamiento y prueba","text":"<p>Para realizar una partici\u00f3n del <code>split</code> de entrenamiento en dos partes (por ejemplo, 90% entrenamiento y 10% prueba), usamos el m\u00e9todo <code>train_test_split()</code>:</p> <p><pre><code>squad_train_full = squad_es[\"train\"]\nsplit_dataset = squad_train_full.train_test_split(test_size=0.1, seed=42)\nsquad_train = split_dataset[\"train\"]\nsquad_test = split_dataset[\"test\"]\nprint(squad_train)\nprint(squad_test)\n</code></pre> Ahora disponemos de <code>squad_train</code> (entrenamiento 90%) y <code>squad_test</code> (prueba 10%).\u200b</p>"},{"location":"Datasets/Actividad1_solution/#3-anadir-una-columna-con-el-numero-de-parrafos","title":"3. A\u00f1adir una columna con el n\u00famero de p\u00e1rrafos","text":"<p>Podemos emplear el m\u00e9todo <code>map()</code> para agregar una columna llamada, por ejemplo, <code>num_paragraphs</code>, contando los elementos en la clave <code>paragraphs</code> de cada ejemplo.</p> <p><pre><code>squad_train = squad_train.map(lambda x: {\"num_paragraphs\": len(x[\"paragraphs\"])})\nprint(squad_train.column_names)  # Debe incluir 'num_paragraphs'\nprint(squad_train[0][\"num_paragraphs\"])\n</code></pre> De este modo, cada registro en el <code>split</code> de entrenamiento tiene la columna con el n\u00famero de p\u00e1rrafos.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#4-filtrar-los-ejemplos-con-mas-de-10-parrafos","title":"4. Filtrar los ejemplos con m\u00e1s de 10 p\u00e1rrafos","text":"<p>Usaremos el m\u00e9todo <code>filter()</code>, pasando una funci\u00f3n lambda que conserve solo aquellos ejemplos cuya columna <code>num_paragraphs</code> sea mayor que 10:</p> <p><pre><code>squad_train_large = squad_train.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nprint(squad_train_large)\n</code></pre> As\u00ed, el dataset de entrenamiento contiene \u00fanicamente los registros relevantes para el criterio pedido.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#5-eliminar-la-columna-num_paragraphs","title":"5. Eliminar la columna num_paragraphs","text":"<p>Para dejar el dataset limpio, eliminamos la columna extra:</p> <p><pre><code>squad_train_final = squad_train_large.remove_columns(\"num_paragraphs\")\nprint(squad_train_final.column_names)\n</code></pre> Esto deja \u00fanicamente las columnas originales: <code>title</code> y <code>paragraphs</code>.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#6-persistir-el-dataset-en-formato-parquet","title":"6. Persistir el dataset en formato Parquet","text":"<p>El m\u00e9todo to_parquet() permite guardar el dataset resultante en disco en formato Parquet, que es eficiente y compatible para grandes vol\u00famenes de datos.</p> <p><pre><code>squad_train_final.to_parquet(\"squad_train_filtered.parquet\")\n</code></pre> Esto crea el archivo Parquet con los ejemplos filtrados.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#7-publicar-el-dataset-en-hugging-face","title":"7. Publicar el dataset en Hugging Face","text":"<p>Antes de publicar necesitas autenticarte con tu cuenta (aseg\u00farate de tener instalado huggingface_hub y un token de escritura):</p> <p><pre><code>from huggingface_hub import login\nlogin()  # Te pedir\u00e1 el token\n</code></pre> A continuaci\u00f3n, puedes usar el m\u00e9todo <code>push_to_hub</code> del dataset. Opcionalmente, crea primero un <code>DatasetDict</code> si quieres incluir tambi\u00e9n el <code>split</code> de validaci\u00f3n o test:</p> <pre><code>from datasets import DatasetDict\n\nfinal_dataset = DatasetDict({\n    \"train\": squad_train_final,\n    \"test\": squad_test\n})\n</code></pre> <p>Sube el dataset (reemplaza /squad_es_filtrado por tu nombre de usuario/repositorio en Hugging Face) <code>final_dataset.push_to_hub(\"&lt;tu_usuario&gt;/squad_es_filtrado\")</code>"},{"location":"Datasets/Actividad1_solution/#opcionalmente-anade-un-ejemplo-en-la-documentacion-editando-la-dataset-card-en-la-propia-web-de-hugging-face-tal-como-recomienda-la-sesionattached_file1","title":"Opcionalmente, a\u00f1ade un ejemplo en la documentaci\u00f3n editando la \"Dataset Card\" en la propia web de Hugging Face, tal como recomienda la sesi\u00f3n[attached_file:1].","text":"<p>Notas finales - Durante el proceso, imprime ejemplos y utiliza peque\u00f1os prints para comprobar cada paso. - La edici\u00f3n de la tarjeta del dataset (\"Dataset Card\") se realiza desde la web de Hugging Face: ah\u00ed puedes a\u00f1adir un ejemplo, uso previsto y detalles del proceso seguido, favoreciendo la comprensi\u00f3n de terceros usuarios.</p>"},{"location":"Datasets/Actividad1_solution/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"Datasets/Datasets/","title":"\ud83d\udcd8 Hugging Face Datasets: Apuntes + Reto Gamificado","text":""},{"location":"Datasets/Datasets/#1-introduccion","title":"1\ufe0f\u20e3 Introducci\u00f3n","text":"<p>El paquete <code>datasets</code> de Hugging Face es una potente herramienta para acceder, compartir y procesar conjuntos de datos (datasets) de IA para una amplia gama de tareas, que incluyen:</p> <ul> <li>Procesamiento del Lenguaje Natural (PLN)</li> <li>Visi\u00f3n por computador</li> <li>Procesamiento de audio</li> </ul> <p>Est\u00e1 dise\u00f1ado para manejar grandes vol\u00famenes de datos de manera eficiente mediante el uso de mapeo de memoria y el formato Apache Arrow, lo que permite trabajar con datos que superan la RAM disponible.</p> <p>Arrow Apache Arrow define un formato de memoria columnar independiente del lenguaje para datos planos y anidados, organizado para operaciones anal\u00edticas eficientes en hardware moderno como CPU y GPU. El formato de memoria Arrow tambi\u00e9n admite lecturas sin copia para un acceso a datos ultrarr\u00e1pido sin sobrecarga de serializaci\u00f3n.</p> <p>El proyecto del formato Apache Arrow comenz\u00f3 en febrero de 2016, centr\u00e1ndose en cargas de trabajo de an\u00e1lisis columnar en memoria. A diferencia de formatos de archivo como Parquet o CSV, que especifican c\u00f3mo se organizan los datos en el disco, Arrow se centra en c\u00f3mo se organizan los datos en la memoria. </p> <p>Los creadores buscan consolidar Arrow como un formato est\u00e1ndar en memoria para el an\u00e1lisis de cargas de trabajo. Estos fundamentos atraen a numerosos colaboradores de proyectos como Pandas, Spark, Cassandra, Apache Calcite, Dremio e Ibis.</p>"},{"location":"Datasets/Datasets/#caracteristicas-clave","title":"\ud83d\udd11 Caracter\u00edsticas Clave","text":"<ul> <li>Vasto Repositorio (Hub): Gran cantidad de datasets p\u00fablicos y privados.</li> <li>F\u00e1cil Acceso: Carga en una sola l\u00ednea de c\u00f3digo con <code>load_dataset</code>.</li> <li>Procesamiento Eficiente: M\u00e9todos como <code>map()</code> paralelizados.</li> <li>Escalabilidad: Objetos <code>Dataset</code> y <code>IterableDataset</code>.</li> <li>Gesti\u00f3n de Datos: Crear y subir datasets propios al Hub de Hugging    Face.</li> </ul> <p>Los datasets de Hugging Face sirven para:</p>"},{"location":"Datasets/Datasets/#1-acceder-a-datos-listos-para-ia","title":"\u2705 1. Acceder a datos listos para IA","text":"<p>Hugging Face ofrece un repositorio enorme de conjuntos de datos p\u00fablicos y privados para tareas como:</p> <ul> <li>Procesamiento del Lenguaje Natural (PLN)</li> <li>Visi\u00f3n por computadora</li> <li>Audio y multimodalidad</li> </ul>"},{"location":"Datasets/Datasets/#2-facilitar-el-preprocesamiento","title":"\u2705 2. Facilitar el preprocesamiento","text":"<p>Permite aplicar transformaciones como:</p> <ul> <li>Tokenizaci\u00f3n de texto</li> <li>Filtrado y remuestreo</li> <li>Conversi\u00f3n a formatos como Pandas, NumPy, PyTorch y TensorFlow</li> </ul>"},{"location":"Datasets/Datasets/#3-escalabilidad-y-eficiencia","title":"\u2705 3. Escalabilidad y eficiencia","text":"<p>Usa Apache Arrow y mapeo de memoria, lo que permite trabajar con datasets que superan la RAM disponible. Soporta dos tipos:</p> <ul> <li>Dataset (acceso aleatorio r\u00e1pido)</li> <li>IterableDataset (para streaming de datos grandes)</li> </ul>"},{"location":"Datasets/Datasets/#4-compartir-y-colaborar","title":"\u2705 4. Compartir y colaborar","text":"<p>Podemos crear y subir nuestros propios datasets al Hugging Face Hub, con documentaci\u00f3n y ejemplos. Esto fomenta la reproducibilidad y el trabajo en equipo.</p>"},{"location":"Datasets/Datasets/#5-integracion-directa-con-modelos","title":"\u2705 5. Integraci\u00f3n directa con modelos","text":"<p>Los datasets se integran f\u00e1cilmente con transformers y otros frameworks para entrenamiento y evaluaci\u00f3n.</p>"},{"location":"Datasets/Datasets/#instalacion","title":"\u2699\ufe0f Instalaci\u00f3n","text":"<pre><code>pip install datasets\npip install datasets[audio]\npip install datasets[vision]\n</code></pre>"},{"location":"Datasets/Datasets/#ejemplo-cargar-un-dataset-local","title":"\ud83e\udde9 Ejemplo: Cargar un dataset local","text":"<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Salida esperada: <pre><code>DatasetDict({\n    train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 })\n})\n</code></pre></p>"},{"location":"Datasets/Datasets/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"Datasets/Datasets/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p> <ol> <li>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y solo empleando Python:<ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con los datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset solo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> </ol>"},{"location":"Datasets/Datasets/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"Datasets/Datasets/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\n\n\n# Nivel 2: Dividir en train/test\n\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\n\n\n# Nivel 4: Filtrar y persistir\n\n# Nivel 5: Publicar en Hugging Face\n</code></pre> <p>Ya hemos visto que podemos utilizar diferentes datasets existentes en la plataforma para re-entrenar nuestros modelos.</p> <p>En esta sesi\u00f3n vamos a estudiar c\u00f3mo crear un dataset a partir de nuestros datos, limpiar un dataset, a reducirlo para que quepa en RAM y como tras crear nuestro dataset, subirlo al Hub.</p>"},{"location":"Datasets/Datasets/#cargando-datos","title":"Cargando datos","text":"<p>Para cargar datos, haremos uso de la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Datasets. Si fuera necesaria instalarla, mediante <code>pip</code> har\u00edamos:</p> <pre><code>pip install datasets\n</code></pre> <p>Podemos cargar archivos con diferentes formatos indicando el formato y los datos:</p> <pre><code># CSV &amp; TSV\nload_dataset(\"csv\", data_files=\"mis_datos.csv\")\n# Texto\nload_dataset(\"text\", data_files=\"mis_datos.txt\")\n# JSON &amp; JSONL\nload_dataset(\"json\", data_files=\"mis_datos.jsonl\")\n</code></pre> <p>Para este apartado nos vamos a centrar en los datos de SQuAD (Stanford Question Answering Dataset), compuesto de un conjunto de art\u00edculos de Wikipedia, con respuestas a diferentes preguntas sobre los art\u00edculos. En nuestro caso, nos vamos a centrar en una versi\u00f3n en castellano que podemos ver en https://huggingface.co/datasets/squad_es o descargarlo de desde https://github.com/ccasimiro88/TranslateAlignRetrieve/tree/master/SQuAD-es-v2.0.</p> <p>Si descargamos los archivos y los colocamos en la misma carpeta que el siguiente script, podemos realizar una carga local de los datos:</p> <p>squad-es-local.py <pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre></p> <p>Tras ejecutarlo, nos muestra que al cargar el dataset ha creado un split para train, con la cantidad de filas y las columnas contenidas dentro de un <code>DatasetDict</code>:</p> <pre><code>Generating train split: 442 examples [00:00, 1211.02 examples/s]\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n})\n</code></pre> <p>Si queremos obtener m\u00e1s informaci\u00f3n, podemos mostrar las caracter\u00edsticas:</p> <pre><code>print(squad_dataset[\"train\"].features)\n# {'title': Value(dtype='string', id=None),\n#  'paragraphs': [\n#     {'context': Value(dtype='string', id=None),\n#      'qas': [\n#         {'answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'id': Value(dtype='string', id=None),\n#          'is_impossible': Value(dtype='bool', id=None),\n#          'plausible_answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'question': Value(dtype='string', id=None)}]\n#     }\n#  ]\n# }\n</code></pre> <p>Y al tratarse de un diccionario, podemos navegar mediante los corchetes para, por ejemplo, recuperar parte de los datos:</p> <pre><code>print(squad_dataset[\"train\"][0][\"title\"])\n# Beyonc\u00e9 Knowles\nprint(squad_dataset[\"train\"][0][\"paragraphs\"][0])\n# {'context': 'Beyonc\u00e9 Giselle Knowles-Carter (nacida el 4 de septiembre de 1981)..',\n # 'qas': [\n    # {'answers': [{'answer_start': 246, 'text': 'a finales de 1990'}],\n    #  'id': '56be85543aeaaa14008c9063', 'is_impossible': False, 'plausible_answers': None, 'question': '\u00bfCu\u00e1ndo Beyonce comenz\u00f3 a ser popular?'},\n    # {'answers': ...\n</code></pre> <p>Cuando cargamos un dataset, realmente queremos cargar los datos de entrenamiento y los de validaci\u00f3n. Para ello, le pasaremos un diccionario con los diferentes archivos para <code>train</code> y para <code>val</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n</code></pre> <p>Obteniendo:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    val: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 35\n    })\n})\n</code></pre> <p>Si queremos cargar \u00fanicamente una de las partes, le pasaremos el par\u00e1metro <code>split</code> con el valor deseado:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\", split=\"train\")\nprint(squad_dataset_train)\n</code></pre> <p>Si el dataset est\u00e1 en un archivo comprimido en gzip, zip o tar, la librer\u00eda descomprimir\u00e1 los datos autom\u00e1ticamente:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json.zip\", \"val\": \"dev-v2.0-es.json.zip\"}\nsquad_dataset_trainval = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset_trainval)\n</code></pre>"},{"location":"Datasets/Datasets/#carga-remota","title":"Carga remota","text":"<p>Si el dataset est\u00e1 almacenado en una URL remota, podemos cargarlos directamente:</p> <p>squad-es-remoto.py <pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\n\nficheros_datos = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\",\n}\nsquad_remote_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n\nprint(squad_remote_dataset)\n</code></pre></p>"},{"location":"Datasets/Datasets/#dividiendo-el-dataset","title":"Dividiendo el dataset","text":"<p>Si en vez de cargar por separado los datos de entrenamiento y los de test, tenemos un \u00fanico conjunto de datos, podemos dividirlo mediante el m\u00e9todo <code>Dataset.train_test_split()</code> indicando el tama\u00f1o, por ejemplo, de los datos de test:</p> <p>squad-es-split.py <pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\", split=\"train\")\nsquad_split_dataset = squad_dataset.train_test_split(test_size=0.1)\nprint(squad_split_dataset)\n</code></pre></p>"},{"location":"Datasets/Datasets/#tamano-de-la-division-de-prueba-test_size-numpyrandomgenerator-opcional","title":"Tama\u00f1o de la divisi\u00f3n de prueba: <code>test_size (numpy.random.Generator, opcional)</code>","text":"<ul> <li>Si es <code>float</code>, debe estar entre 0.0 y 1.0 y representar la proporci\u00f3n del conjunto de datos que se incluir\u00e1 en la divisi\u00f3n de prueba. Si es int, representa el n\u00famero absoluto de muestras de prueba. </li> <li>Si es <code>None</code>, el valor se establece en el complemento del tama\u00f1o de entrenamiento.    Si <code>train_size</code> tambi\u00e9n es <code>None</code>, se establecer\u00e1 en 0.25.</li> </ul> <p>De manera que pasamos de un dataset de 442 filas a uno dividido en 397 para entrenamiento y 45 para test:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 397\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 45\n    })\n})\n</code></pre>"},{"location":"Datasets/Datasets/#cargando-en-pandas","title":"Cargando en Pandas","text":"<p>Si estamos trabajando con Pandas y queremos cargar un archivo que est\u00e1 en un dataset de Hugging Face, podemos utilizar la librer\u00eda de <code>huggingface_hub</code> para crear, eliminar, actualizar y recuperar informaci\u00f3n de los repositorios.</p> <p>Por ejemplo, para cargar un dataset CSV con Pandas podr\u00edamos hacer:</p> <p>cargando_en_pandas.py <pre><code>from huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"aitor-medrano/iabd\"\nFICHERO = \"cp_train.csv\"\n\ndataset = pd.read_csv(\n    hf_hub_download(repo_id=REPO_ID, filename=FICHERO, repo_type=\"dataset\")\n)\nprint(dataset)\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 3214 entries, 0 to 3213\n# Data columns (total 3 columns):\n#  #   Column   Non-Null Count  Dtype  \n# ---  ------   --------------  -----  \n#  0   formula  3214 non-null   object \n#  1   T        3214 non-null   float64\n#  2   Cp       3214 non-null   float64\n# dtypes: float64(2), object(1)\n# memory usage: 75.5+ KB\n</code></pre></p>"},{"location":"Datasets/Datasets/#cargando-en-streaming","title":"Cargando en streaming","text":"<p>Si el dataset ocupa varios gigas, con modelos como BERT o GPT, puede que no quepan en memoria RAM. Para ello, la librer\u00eda datasets gestiona los datos como ficheros mapeados en memoria realizando un streaming de los datos.</p> Streaming de un dataset - https://huggingface.co/docs/datasets/stream <p>Para este apartado, vamos a utilizar parte de un dataset con c\u00f3digo encontrado en GitHub de los diferentes lenguajes de programaci\u00f3n, conocido como BigCode.</p> <p>Gated dataset</p> <p>El dataset de BigCode est\u00e1 configurado para pedir las credenciales de los usuarios que los utilizan. Esto se conoce como un Gated dataset. Es por ello que tenemos que hacer login en Hugging Face antes de poder descargarlo.</p> <p>En nuestro caso, nos vamos a centrar en archivos <code>DockerFile</code> para que el dataset sea asumible (alrededor de 1Gb). As\u00ed pues, cargamos los datos:</p> <p>cargando_en_streaming.py <pre><code>from datasets import load_dataset\n\nbigcode_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\")\nprint(bigcode_dataset)\n# Dataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     num_rows: 4155925\n# })\n</code></pre></p> <p>Y el contenido de un documento:</p> <pre><code>{\n   \"blob_id\":\"a29dd2b33082d2b98541c66ba3620ae054991503\",\n   \"directory_id\":\"71568d223947f51cb007a8564e5f808e0987779e\",\n   \"path\":\"/src/Dapr/GameServer.Host/Dockerfile\",\n   \"content_id\":\"072b3138be512a71cf4e8bbbdb657497e808d2f6\",\n   \"detected_licenses\":[\n      \"MIT\",\n      \"LicenseRef-scancode-unknown-license-reference\"\n   ],\n   \"license_type\":\"permissive\",\n   \"repo_name\":\"devblack/OpenMU\",\n   \"snapshot_id\":\"8cdc521178da47c9c7d2daa502dc71688835fd0a\",\n   \"revision_id\":\"1064b0dca1a491bc28f325e42ca6b97d406d6558\",\n   \"branch_name\":\"refs/heads/master\",\n   \"visit_date\":Timestamp(\"2023-04-07 10:56:30.043316\"),\n   \"revision_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"committer_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"github_id\":None,\n   \"star_events_count\":0,\n   \"fork_events_count\":0,\n   \"gha_license_id\":None,\n   \"gha_event_created_at\":None,\n   \"gha_created_at\":None,\n   \"gha_language\":None,\n   \"src_encoding\":\"UTF-8\",\n   \"language\":\"Dockerfile\",\n   \"is_vendor\":false,\n   \"is_generated\":false,\n   \"length_bytes\":709,\n   \"extension\":\"\"\n}\n</code></pre> <p>Si ahora cargamos el dataset mediante streaming, pas\u00e1ndole el par\u00e1metros <code>streaming=True</code>, en vez de recuperar un Dataset, obtendremos un <code>IterableDataset</code>:</p> <pre><code>from datasets import load_dataset\n\nstreaming_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\", streaming=True)\nprint(streaming_dataset)\n# Resolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 976/976 [00:00&lt;00:00, 1064.36it/s]\n# IterableDataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     n_shards: 1\n# })\n</code></pre> <p>Como sugiere el nombre, para acceder a los elementos del <code>IterableDataset</code> necesitamos iterar sobre \u00e9l:</p> <pre><code>print(next(iter(streaming_dataset)))\n</code></pre> <p>Los elementos de un dataset en streaming  se pueden procesar al vuelo mediante la funci\u00f3n <code>map</code> o <code>filter</code>. Por ejemplo, para pasar a may\u00fasculas el campo <code>license_type</code> har\u00edamos:</p> <pre><code>def mayusLicencias(registro):\n    registro[\"license_type\"] = registro[\"license_type\"].upper()\n    return registro\n\nmapped_dataset = streaming_dataset.map(mayusLicencias)\n\nprint(next(iter(streaming_dataset)))\n# ... 'detected_licenses': ['MIT', ...], 'license_type': 'PERMISSIVE', 'repo_name': 'devblack/OpenMU' ...\n</code></pre> <p>Al vuelo</p> <p>Debemos tener en cuenta que la transformaci\u00f3n se realizar\u00e1 al recorrer el dataset, no al invocar a los m\u00e9todos <code>map</code> o <code>filter</code>.</p> <p>Otras opciones son utilizar las operaciones <code>take()</code>, <code>shuffle()</code> o <code>skip()</code> dentro del <code>IterableDataset</code> para trabajar con muestras peque\u00f1as de los datos cargados:</p> <pre><code>muestra = streaming_dataset.take(100)\nmuestra_aleatoria = streaming_dataset.shuffle(seed=42, buffer_size=1000).take(500)\nrango = streaming_dataset.skip(1000).take(100)\n</code></pre> <p>M\u00e1s informaci\u00f3n en https://huggingface.co/docs/datasets/stream y https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable.</p>"},{"location":"Datasets/Datasets/#limpiando","title":"Limpiando","text":"<p>De forma similar a Pandas o Spark, podemos manipular el contenido de los objetos <code>Dataset</code>.</p> <p>El primer paso deber\u00eda ser barajar los datos para desordenarlos y coger una muestra. Para ello, emplearemos <code>Dataset.shuffle()</code>:</p> <pre><code>squad_train = squad_dataset_trainval[\"train\"]\nprint(squad_train[0][\"title\"]) # Beyonc\u00e9 Knowles\n\nsquad_train_shuffled = squad_train.shuffle(seed=333)\nprint(squad_train_shuffled[0][\"title\"]) # Carnaval\n</code></pre>"},{"location":"Datasets/Datasets/#por-que-se-usa-seed-semilla-fija-de-333-en-el-metodo-suffle","title":"\u00bfPor qu\u00e9 se usa seed (semilla) fija de 333 en el m\u00e9todo suffle()?","text":"<p><pre><code>squad_train_shuffled = squad_train.shuffle(seed=333)\n</code></pre> Se usa una semilla fija (seed) en el m\u00e9todo shuffle() para garantizar reproducibilidad.</p> <p>\u2705 \u00bfQu\u00e9 significa esto?</p> <ul> <li>Cuando barajas datos, el orden resultante depende de un generador aleatorio.</li> <li>Si no se fija una semilla, cada ejecuci\u00f3n produce un orden distinto.</li> <li>Al establecer <code>seed=333</code>:<ul> <li>El generador aleatorio se inicializa siempre igual.</li> <li>El orden barajado ser\u00e1 id\u00e9ntico en cada ejecuci\u00f3n, lo que permite reproducir experimentos.</li> </ul> </li> </ul> <p>\u2705 \u00bfPor qu\u00e9 es importante en Machine Learning?</p> <ul> <li>Consistencia: Si compartes c\u00f3digo con otros, obtendr\u00e1n el mismo resultado.</li> <li>Depuraci\u00f3n: Puedes repetir pruebas sin que el orden cambie.</li> <li>Comparaci\u00f3n justa: Cuando eval\u00faas modelos, necesitas que los datos sean los mismos en cada experimento.</li> </ul>"},{"location":"Datasets/Datasets/#seleccionando-filas","title":"Seleccionando filas","text":"<p>Para recuperar filas, usaremos el m\u00e9todo <code>Dataset.select()</code> pas\u00e1ndole un iterador con las posiciones a seleccionar:</p> <pre><code>tres_filas = squad_train_shuffled.select([5,10,15])\nseis_filas = squad_train_shuffled.select(range(6))\n</code></pre> <p>Si queremos seleccionar las filas por alg\u00fan criterio espec\u00edfico, debemos emplear <code>Dataset.filter()</code> y funciones lambda:</p> <pre><code>empieza_por_b_filas = squad_train_shuffled.filter(lambda x: x[\"title\"].startswith(\"B\"))\nfor i in range(empieza_por_b_filas.num_rows):\n    print(empieza_por_b_filas[i][\"title\"])\n# Beidou _ Navigation _ Satellite _ System (en ingl\u00e9s)\n# Biodiversidad\n# Boston ...\n</code></pre>"},{"location":"Datasets/Datasets/#trabajando-con-columnas","title":"Trabajando con columnas","text":"<p>Al trabajar con columnas, podemos a\u00f1adir una nueva mediante <code>Dataset.add_column()</code>, cambiarles el nombre con <code>DatasetDict.rename_column()</code> o eliminar una columna con <code>DatasetDict.remove_column()</code>:</p> <pre><code>nueva_columna = [\"nueva\"] * squad_train_shuffled.num_rows\nsquad_train_shuffled = squad_train_shuffled.add_column(name=\"inicial\",column=nueva_columna)\n# Dataset({\n#     features: ['title', 'paragraphs', 'inicial'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.rename_column(\n    original_column_name=\"inicial\", new_column_name=\"modificada\"\n)\n# Dataset({\n#     features: ['title', 'paragraphs', 'modificada'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.remove_columns([\"modificada\"])\n# Dataset({\n#     features: ['title', 'paragraphs'],\n#     num_rows: 442\n# })\n</code></pre>"},{"location":"Datasets/Datasets/#trabajando-con-map","title":"Trabajando con map","text":"<p>Y la joya de la corona es la funci\u00f3n <code>Dataset.map()</code> para aplicar una transformaci\u00f3n a medida. Por ejemplo, si queremos modificar todos los t\u00edtulos y pasarlos a min\u00fascula haremos:</p> <p>squad-map.py <pre><code>from datasets import load_dataset\n\nficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nsquad_dataset = squad_dataset[\"train\"]\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\nsquad_minus = squad_dataset.map(titulo_minus)\n\n# Mostramos 5 elementos\nprint(squad_minus.shuffle(seed=987)[\"title\"][:5])\n# ['marshall', 'ascensor', 'kanye', 'bbc televisi\u00f3n', 'florida']\n</code></pre></p> <p>Mediante la funci\u00f3n <code>map</code> tambi\u00e9n podemos crear nuevas columnas a partir de los valores de otras. Por ejemplo, a\u00f1adimos una nueva caracter\u00edstica con la cantidad de p\u00e1rrafos y quitamos toda la informaci\u00f3n existente previamente de los p\u00e1rrafos:</p> <pre><code># A\u00f1adimos nueva columna\nsquad_col_num_parrafos = squad_dataset.map(lambda x: {\"num_paragraphs\":len(x[\"paragraphs\"])})\n# Borramos la antigua\nsquad_col_num_parrafos = squad_col_num_parrafos.remove_columns(\"paragraphs\")\nprint(squad_col_num_parrafos.shuffle(seed=987)[:5])\n# {'title': ['Marshall', 'Ascensor', 'Kanye', 'BBC Televisi\u00f3n', 'Florida'], 'num_paragraphs': [51, 52, 79, 31, 35]}\n</code></pre> <p>\u00bfY si queremos ordenar los p\u00e1rrafos por la cantidad de p\u00e1rrafos? Para ello, podemos utilizar <code>Dataset.sort()</code>:</p> <pre><code>squad_num_parrafos_ordenados = squad_col_num_parrafos.sort(\"num_paragraphs\")\n\nprint(squad_num_parrafos_ordenados[:3]) # tres primeros\n# {'title': ['Tono _ (m\u00fasica)', 'Uva', 'Letra _ case'], 'num_paragraphs': [10, 10, 12]}\n\nprint(squad_num_parrafos_ordenados[-3:]) # tres \u00faltimos\n# {'title': ['American Idol (en ingl\u00e9s).', 'Nueva York', 'Budismo'], 'num_paragraphs': [127, 148, 149]}\n</code></pre>"},{"location":"Datasets/Datasets/#trabajando-con-batches","title":"Trabajando con batches","text":"<p>El m\u00e9todo <code>Dataset.map()</code> admite el argumento <code>batched</code> que, si se establece en <code>True</code>, hace que env\u00ede un lote de datos a la funci\u00f3n <code>map</code> a la vez (el tama\u00f1o del lote es configurable mediante el par\u00e1metro <code>batch_size</code>, pero por defecto es de 1000). Al indicar <code>batched=True</code>, la funci\u00f3n <code>map</code> recibe un diccionario con los campos del conjunto de datos, pero cada valor es ahora una lista de valores, y no un \u00fanico valor, y del mismo modo, devuelve un diccionario con los campos que queremos actualizar o a\u00f1adir a nuestro conjunto de datos, y una lista de valores.</p> <p>Por ello, debemos modificar la funci\u00f3n que realiza el <code>map</code> para que trabaje con un diccionario de listas.</p> <p>Para el siguiente ejemplo, vamos a coger el dataset de SQuAD_es desde Hugging Face que contiene m\u00e1s datos, y vamos a comparar el tiempo de ejecuci\u00f3n, recodificando la funci\u00f3n para trabajar con un diccionario que contiene una lista por cada campo:</p> <p>squad-map-batch.py <pre><code>from datasets import load_dataset\nimport time\n\n# Cargamos el dataset desde HF\nsquad_dataset = load_dataset(\"squad_es\", \"v2.0.0\", split=\"train\")  # (1)!\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus) # (2)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 37926.07 examples/s]   \nfin = time.time()\nprint(fin - inicio) # 3.008699893951416\n\ndef titulo_minus_batch(batch):\n    return {\"title\":[titulo.lower() for titulo in batch[\"title\"]]}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus_batch, batched=True) # (3)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 535210.67 examples/s]\nfin = time.time()\nprint(fin - inicio) # 0.19133710861206055\n</code></pre></p> <ol> <li>Cargamos los datos desde el dataset de Hugging Face</li> <li>El mapper normal realiza casi 40.000 registros por segundo</li> <li>Al hacerlo con batches realiza m\u00e1s de 500.000 por segundo</li> </ol>"},{"location":"Datasets/Datasets/#uso-de-pandas","title":"Uso de Pandas","text":"<p>Para facilitar la conversi\u00f3n entre diferentes formados, podemos usar el m\u00e9todo <code>Dataset.set_format()</code> para modificar el formato de salida (por ejemplo, a <code>pandas</code>, <code>numpy</code>, <code>torch</code>, <code>tensorflow</code>, <code>jax</code>, ...), sin modificar el formato original del dataset (el cual es Apache Arrow - <code>arrow</code>).</p> <p>As\u00ed, si por ejemplo queremos pasar los datos a Pandas para trabajar con ellos, transformarlos o visualizarlos, podemos hacer:</p> <pre><code>print(squad_dataset[0])\nsquad_dataset.set_format(\"pandas\")\ndf = squad_dataset[:]\nprint(df.shape) # (130313, 5)\n\n# Otra forma\ndf2 = squad_dataset.to_pandas()\nprint(df2.info())\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 130313 entries, 0 to 130312\n# Data columns (total 5 columns):\n#  #   Column    Non-Null Count   Dtype \n# ---  ------    --------------   ----- \n#  0   id        130313 non-null  object\n#  1   title     130313 non-null  object\n#  2   context   130313 non-null  object\n#  3   question  130313 non-null  object\n#  4   answers   130313 non-null  object\n# dtypes: object(5)\n# memory usage: 5.0+ MB\n\n# Y realizar consultas sobre los datos, por ejemplo, la 5 preguntas con m\u00e1s respuestas\nprint(df2.groupby(\"title\")[\"answers\"].count().nlargest(5))\n# title\n# Reina                         883\n# Nueva York                    817\n# American Idol (en ingl\u00e9s).    790\n# Beyonc\u00e9 Knowles               753\n# Universidad                   738\n</code></pre> <p>Y para volver a tener un objeto <code>Dataset</code>, podemos utilizar el m\u00e9todo <code>Dataset.from_pandas()</code> o resetear el dataset que ten\u00edamos mediante <code>Dataset.reset_format()</code>:</p> <pre><code>squad_transformed = Dataset.from_pandas(df)\nsquad_original = squad_dataset.reset_format()\n</code></pre>"},{"location":"Datasets/Datasets/#creando-un-dataset-desde-cero","title":"Creando un dataset desde cero","text":"<p>Si en vez de cargar un dataset, tenemos que crear uno, el primer paso ser\u00e1 obtener los datos.</p> <p>Ya sea mediante peticiones a URL externas con la librer\u00eda <code>request</code>, accediendo a bases de datos relacionales, por ejemplo, mediante SQLAlchemy o a MongoDB mediante <code>PyMongo</code>, lo m\u00e1s normal es crear una estructura de datos mediante una lista o un diccionario y luego crear el dataset a partir del mismo.</p> <p>As\u00ed pues, si en vez de cargar un dataset queremos crear uno desde cero, podemos hacer uso de <code>Dataset.from_dict()</code> o <code>Dataset.from_list()</code> dependiendo de donde tengamos los datos. Por ejemplo, vamos a conectarnos a MongoDB y creamos una lista con todos los datos. A partir de la lista, generamos un Dataset:</p> <p>dataset-mongodb.py <pre><code>from datasets import Dataset\nfrom pymongo import MongoClient\n\ncliente = MongoClient('mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net')\n\n# Vamos a trabajar con la colecci\u00f3n grades de sample_training\niabd_db = cliente.sample_training\ngrades_coll = iabd_db.grades\n\n# Creamos un dataset vac\u00edo mediante una lista\ndocumentos = []\n\n# Recuperamos todas las calificaciones\ncursor = grades_coll.find({})\nfor document in cursor:\n  del document[\"_id\"]  # Quitamos el ObjectId\n  documentos.append(document)\n\nmongo_dataset = Dataset.from_list(documentos)\nprint(mongo_dataset)\n# Dataset({\n#     features: ['student_id', 'scores', 'class_id'],\n#     num_rows: 100000\n# })\n</code></pre></p> <p>O si queremos almacenar los datos de forma temporal, por ejemplo, podemos hacer uso de Pandas:</p> <pre><code>df = pd.DataFrame.from_records(documentos)\ndf.to_json(\"grades-docs.jsonl\", orient=\"records\", lines=True)\njsonl_dataset = load_dataset(\"json\", data_files=\"grades-docs.jsonl\", split=\"train\")\n</code></pre>"},{"location":"Datasets/Datasets/#persistiendo","title":"Persistiendo","text":"<p>Cuando cargamos un dataset se cachean los datos para evitar tener que descargar los datos de forma remota y crear las tablas de PyArrow. Podemos averiguar donde se almacenan los datos mediante la propiedad <code>cache_files</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset.cache_files)\n# {'train': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-train.arrow'}],\n#  'test': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-test.arrow'}]}\n</code></pre> <p>Si necesitamos persistir los datos de nuevo con las transformaciones realizadas, podemos hacerlo en diferentes formatos:</p> <ul> <li>Arrow: <code>Dataset.save_to_disk()</code></li> <li>CSV: <code>Dataset.to_csv()</code></li> <li>Pandas: <code>Dataset.to_pandas()</code></li> <li>JSON: <code>Dataset.to_json()</code></li> <li>Parquet: <code>Dataset.to_parquet()</code></li> </ul> <p>Si por ejemplo persistimos el dataset en Arrow:</p> <pre><code>squad_dataset.save_to_disk(\"squad_train_test\")\n</code></pre> <p>Se crear\u00e1 la siguiente estructura en las carpetas, donde para cada split se ha creado su propia carpeta, as\u00ed como un par de archivos con metadatos:</p> <pre><code>squad_train_test\n\u251c\u2500\u2500 dataset_dict.json\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 data-00000-of-00001.arrow\n\u2502   \u251c\u2500\u2500 dataset_info.json\n\u2502   \u2514\u2500\u2500 state.json\n\u2514\u2500\u2500 train\n    \u251c\u2500\u2500 data-00000-of-00001.arrow\n    \u251c\u2500\u2500 dataset_info.json\n    \u2514\u2500\u2500 state.json\n</code></pre> <p>Una vez que hemos persistido el dataset, lo podemos volver a cargar mediante <code>load_from_disk</code>:</p> <pre><code>from datasets import load_from_disk\nsquad_disk = load_from_disk(\"squad_train_test\")\n</code></pre> <p>Si queremos persistirlos en formato CSV o JSON, por cada uno de los splits se crea un archivo separado. Para ello, podemos iterar sobre las claves mediante <code>DatasetDict.items()</code>:</p> <pre><code>for split, dataset in squad_dataset.items():\n    dataset.to_json(f\"squad_{split}.jsonl\")\n</code></pre>"},{"location":"Datasets/Datasets/#publicando","title":"Publicando","text":"<p>Pues una vez hemos creado nuestro Dataset y lo hemos persistido en disco, el \u00fanico paso que nos queda es publicar en Hugging Face. Para ello, en vez de subir los archivos a mano como hicimos en la sesi\u00f3n inicial desde el interfaz web o hacer uso de Git y copiar los archivos, desde Python es mucho m\u00e1s c\u00f3modo.</p> <p>Previamente debemos realizar login para autenticarnos, tal como vimos en la secci\u00f3n de Login de la sesi\u00f3n anterior y usar nuestro token de acceso (en este caso, necesitamos que nuestro token sea de escritura):</p> <pre><code>hf auth login\n</code></pre> <p>Login desde un cuaderno Jupyter / Colab</p> <p>Si trabajamos con un cuaderno de Jupyter o Colab, podemos utilizar la librer\u00eda de huggingface_hub y al ejecutarlo, nos aparecer\u00e1 una ventana donde introducir el token de acceso:</p> <pre><code>from huggingface_hub import login\nlogin()\n</code></pre> <p> Login desde un notebook </p> <p>Tambi\u00e9n podemos hacer uso de la variable de entorno <code>HF_TOKEN</code> para almacenar el token de acceso, ya sea con un Space almacen\u00e1ndolo en un secret de Spaces o en claves privadas de Google Colab.</p> <p>Una vez autenticado, es tan sencillo como usar el m\u00e9todo <code>push_to_hub()</code> (si queremos publicar el dataset como privado, le a\u00f1adimos el par\u00e1metro <code>private=True</code>):</p> <pre><code>midataset.push_to_hub(\"aitor-medrano/midatasetpushed\")\n# midataset.push_to_hub(\"aitor-medrano/midatasetpushed\", private=True)\n</code></pre> <p>Si tuvi\u00e9ramos diferentes splits podr\u00edamos hacer:</p> <pre><code>train_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"train\")\nval_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"validation\")\n</code></pre> <p>Creando el dataset card</p> <p>Para crear el dataset card, podemos emplear la aplicaci\u00f3n https://huggingface.co/spaces/huggingface/datasets-tagging que facilita rellenar los campos e incluye recomendaciones de otros datasets.</p> <p>Tambi\u00e9n se recomienda leer la Dataset Card Creation Guide</p>"},{"location":"Datasets/Datasets/#datasets-multimedia","title":"Datasets multimedia","text":"<p>Si nuestro dataset va a contener elementos multimedia con im\u00e1genes o audios, podemos subir los archivos en crudo, lo cual es lo m\u00e1s practico en la mayor\u00eda de los casos. Si los dataset de im\u00e1genes o audios son muy grandes, se recomienda el formato WebDataset, aunque lo m\u00e1s normal, es almacenar el dataset en formato Parquet.</p> <p>Dicho esto, vamos a ver como trabajar con datasets que contienen elementos multimedia.</p>"},{"location":"Datasets/Datasets/#audios","title":"Audios","text":"<p>Para trabajar con audios, necesitaremos instalar las librer\u00edas de audio:</p> <pre><code>pip install datasets[audio]\n</code></pre> <p>Una vez instalado, vamos a cargar un dataset que contiene audios, como es PolyAI/minds14:</p> <p>audio_minds.py <pre><code>from datasets import load_dataset\n\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\nprint(minds)\n# Dataset({\n#     features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n#     num_rows: 486\n# })\n</code></pre></p> <p>El dataset contiene 486 archivos de audio, cada uno de los cuales va acompa\u00f1ado de una transcripci\u00f3n, una traducci\u00f3n al ingl\u00e9s y una etiqueta que indica la intenci\u00f3n de la consulta de la persona. La columna de audio contiene los datos de audio sin procesar. Veamos uno de los ejemplos:</p> <pre><code>{'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n 'audio': {'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n    'array': array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            -0.00024414, -0.00024414]),\n    'sampling_rate': 8000\n },\n 'transcription': 'hola buenas a ver tengo un problema con vuestra aplicaci\u00f3n resulta que quiero hacer una transferencia bancaria a una cuenta conocida',\n 'english_transcription': 'hello good to see I have a problem with your application it turns out that I want to make a bank transfer to a known account',\n 'intent_class': 2,\n 'lang_id': 5}\n</code></pre> <p>Si nos centramos en el campo <code>audio</code>, vemos como contiene diferentes valores, los cuales forman parte de un objeto <code>Audio</code>:</p> <ul> <li><code>path</code>: ruta al archivo de audio (en este ejemplo <code>*.wav</code>).</li> <li><code>array</code>: los datos del audio decodificados, representados por un array de NumPy</li> <li><code>sampling_rate</code>: la frecuencia de muestreo (8,000 Hz).</li> </ul> <pre><code>ejemplo = minds.shuffle()[0]\n\nid2label = minds.features[\"intent_class\"].int2str\nlabel = id2label(ejemplo[\"intent_class\"])\n</code></pre> <p>Y si queremos escuchar el audio, usando Gradio podemos crear un componente:</p> <p>audio_app.py</p> <pre><code>import gradio as gr\nfrom datasets import load_dataset\n\n# Cargar el dataset\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\n\n# Obtener un audio aleatorio del dataset\naudio = minds.shuffle()[0]\n\n#obtenemos el audio decodificado (que devuelve un objeto AudioDecoder)\naudio_decode = audio[\"audio\"]\n\n# Etiqueta de intenci\u00f3n\nid2label = minds.features[\"intent_class\"].int2str\nprint(id2label)\n#Usando la funci\u00f3n anterior, toma el identificador de intenciones del ejemplo \n# y lo convierte en su nombre textual \n# para saber qu\u00e9 clase de intento representa ese dato.\nintent_id = audio[\"intent_class\"]\nlabel = id2label(intent_id)\n\n#print(label.names[])\n\ndef load_audio():\n    return (audio_decode[\"sampling_rate\"], audio_decode[\"array\"])\n\n# Crear la interfaz de Gradio\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Reproductor de Audio - Dataset PolyAI/minds14\")\n\n    audio_component = gr.Audio(\n        value=load_audio(),\n        label=label,\n        interactive=False\n    )\n\ndemo.launch(share=True)\n</code></pre> <p>Y reproducir el audio seleccionado:</p> Reproduciendo un audio desde Gradio <p>Finalmente, si queremos mostrar un gr\u00e1fico con el espectograma del audio, y haciendo uso de la librer\u00eda Librosa:</p> <pre><code>import librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\n\narray = ejemplo[\"audio\"][\"array\"]\nsampling_rate = ejemplo[\"audio\"][\"sampling_rate\"]\n\nplt.figure().set_figwidth(12)\nlibrosa.display.waveshow(array, sr=sampling_rate)\n</code></pre> <p>Obteniendo:</p> Representaci\u00f3n gr\u00e1fica de un audio"},{"location":"Datasets/Datasets/#creando-un-dataset-desde-python","title":"Creando un dataset desde Python","text":"<p>Si queremos crear un dataset a partir de audios propios, necesitamos crear un diccionario o una lista con la ruta a nuestros audios, y a continuaci\u00f3n, llamar al m\u00e9todo <code>cast_column</code> para transformar la columna a tipo <code>Audio</code> con las caracter\u00edsticas que hemos visto antes.</p> <p>As\u00ed pues, si tenemos los datos en un diccionario podemos hacer:</p> <pre><code>audio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", \"path/to/audio_2\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\n</code></pre>"},{"location":"Datasets/Datasets/#audiofolder","title":"Audiofolder","text":"<p>Otra posibilidad es crear el dataset a partir de una carpeta con todos los audios, y un archivo <code>metadata.csv</code> con los datos:</p> <pre><code>folder/train/metadata.csv\nfolder/train/first_audio_file.mp3\nfolder/train/second_audio_file.mp3\nfolder/train/third_audio_file.mp3\n</code></pre> <p>Y un archivo de metadatos con:</p> <p>metadata.csv <pre><code>file_name,transcription\nfirst_audio_file.mp3,este es el texto del primer audio\nsecond_audio_file.mp3,en el segundo audio cuento un chiste\nthird_audio_file.mp3,y en este audio agradezco al p\u00fablico sus abucheos\n</code></pre></p> <p>Y a continuaci\u00f3n, indicamos como primer par\u00e1metro <code>autofolder</code> y con <code>data_dir</code> indicamos la carpeta que contiene los audios:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n</code></pre>"},{"location":"Datasets/Datasets/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> <li>Documentaci\u00f3n oficial de dataset</li> <li>El cap\u00edtulo La librer\u00eda Dataset del curso NLP de Hugging Face.</li> </ul>"},{"location":"Datasets/Datasets/#actividades","title":"Actividades","text":"<ol> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y s\u00f3lo empleando Python:</p> <ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con lo datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset s\u00f3lo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 0.5p) Carga en streaming el dataset de imagenet-1k, y muestra la etiqueta de los primeros 5 elementos.</p> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 1p) Tenemos un dataset con informaci\u00f3n sobre vuelos retrasados en Espa\u00f1a y otro con informaci\u00f3n general sobre los aeropuertos. Se pide:</p> <ol> <li>Une ambos datasets para que la informaci\u00f3n de los vuelos contengan el nombre y la ciudad de los aeropuertos.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato Arrow.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / opcional ) Sobre la base de datos de MongoDB de <code>sample_training</code> disponible en los datos de muestra de MongoAtlas:</p> <ol> <li>Carga los de estados de <code>states.js</code>.</li> <li>Realiza una join entre las colecciones <code>zips</code> y <code>states</code>.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato JSON.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> </ol>"},{"location":"Datasets/datasets_aitor/","title":"Cargando datasets en HuggingFace (apuntes de Aitor Medrano)","text":"<p>Ya hemos visto que podemos utilizar diferentes datasets existentes en la plataforma para re-entrenar nuestros modelos.</p> <p>En esta sesi\u00f3n vamos a estudiar c\u00f3mo crear un dataset a partir de nuestros datos, limpiar un dataset, a reducirlo para que quepa en RAM y como tras crear nuestro dataset, subirlo al Hub.</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#cargando-datos","title":"Cargando datos","text":"<p>Para cargar datos, haremos uso de la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Datasets. Si fuera necesaria instalarla, mediante <code>pip</code> har\u00edamos:</p> <pre><code>pip install datasets\n</code></pre> <p>Podemos cargar archivos con diferentes formatos indicando el formato y los datos:</p> <pre><code># CSV &amp; TSV\nload_dataset(\"csv\", data_files=\"mis_datos.csv\")\n# Texto\nload_dataset(\"text\", data_files=\"mis_datos.txt\")\n# JSON &amp; JSONL\nload_dataset(\"json\", data_files=\"mis_datos.jsonl\")\n</code></pre> <p>Para este apartado nos vamos a centrar en los datos de SQuAD (Stanford Question Answering Dataset), compuesto de un conjunto de art\u00edculos de Wikipedia, con respuestas a diferentes preguntas sobre los art\u00edculos. En nuestro caso, nos vamos a centrar en una versi\u00f3n en castellano que podemos ver en https://huggingface.co/datasets/squad_es o descargarlo de desde https://github.com/ccasimiro88/TranslateAlignRetrieve/tree/master/SQuAD-es-v2.0.</p> <p>Si descargamos los archivos y los colocamos en la misma carpeta que el siguiente script, podemos realizar una carga local de los datos:</p> squad-es-local.py<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Tras ejecutarlo, nos muestra que al cargar el dataset ha creado un split para train, con la cantidad de filas y las columnas contenidas dentro de un <code>DatasetDict</code>:</p> <pre><code>Generating train split: 442 examples [00:00, 1211.02 examples/s]\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n})\n</code></pre> <p>Si queremos obtener m\u00e1s informaci\u00f3n, podemos mostrar las caracter\u00edsticas:</p> <pre><code>print(squad_dataset[\"train\"].features)\n# {'title': Value(dtype='string', id=None),\n#  'paragraphs': [\n#     {'context': Value(dtype='string', id=None),\n#      'qas': [\n#         {'answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'id': Value(dtype='string', id=None),\n#          'is_impossible': Value(dtype='bool', id=None),\n#          'plausible_answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'question': Value(dtype='string', id=None)}]\n#     }\n#  ]\n# }\n</code></pre> <p>Y al tratarse de un diccionario, podemos navegar mediante los corchetes para, por ejemplo, recuperar parte de los datos:</p> <pre><code>print(squad_dataset[\"train\"][0][\"title\"])\n# Beyonc\u00e9 Knowles\nprint(squad_dataset[\"train\"][0][\"paragraphs\"][0])\n# {'context': 'Beyonc\u00e9 Giselle Knowles-Carter (nacida el 4 de septiembre de 1981)..',\n # 'qas': [\n    # {'answers': [{'answer_start': 246, 'text': 'a finales de 1990'}],\n    #  'id': '56be85543aeaaa14008c9063', 'is_impossible': False, 'plausible_answers': None, 'question': '\u00bfCu\u00e1ndo Beyonce comenz\u00f3 a ser popular?'},\n    # {'answers': ...\n</code></pre> <p>Cuando cargamos un dataset, realmente queremos cargar los datos de entrenamiento y los de validaci\u00f3n. Para ello, le pasaremos un diccionario con los diferentes archivos para <code>train</code> y para <code>val</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n</code></pre> <p>Obteniendo:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    val: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 35\n    })\n})\n</code></pre> <p>Si queremos cargar \u00fanicamente una de las partes, le pasaremos el par\u00e1metro <code>split</code> con el valor deseado:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\", split=\"train\")\nprint(squad_dataset_train)\n</code></pre> <p>Si el dataset est\u00e1 en un archivo comprimido en gzip, zip o tar, la librer\u00eda descomprimir\u00e1 los datos autom\u00e1ticamente:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json.zip\", \"val\": \"dev-v2.0-es.json.zip\"}\nsquad_dataset_trainval = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset_trainval)\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#carga-remota","title":"Carga remota","text":"<p>Si el dataset est\u00e1 almacenado en una URL remota, podemos cargarlos directamente:</p> squad-es-remoto.py<pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\n\nficheros_datos = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\",\n}\nsquad_remote_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n\nprint(squad_remote_dataset)\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#dividiendo-el-dataset","title":"Dividiendo el dataset","text":"<p>Si en vez de cargar por separado los datos de entrenamiento y los de test, tenemos un \u00fanico conjunto de datos, podemos dividirlo mediante el m\u00e9todo <code>Dataset.train_test_split()</code> indicando el tama\u00f1o, por ejemplo, de los datos de test:</p> squad-es-split.py<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\", split=\"train\")\nsquad_split_dataset = squad_dataset.train_test_split(test_size=0.1)\nprint(squad_split_dataset)\n</code></pre> <p>De manera que pasamos de un dataset de 442 filas a uno dividido en 397 para entrenamiento y 45 para test:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 397\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 45\n    })\n})\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#cargando-en-pandas","title":"Cargando en Pandas","text":"<p>Si estamos trabajando con Pandas y queremos cargar un archivo que est\u00e1 en un dataset de Hugging Face, podemos utilizar la librer\u00eda de <code>huggingface_hub</code> para crear, eliminar, actualizar y recuperar informaci\u00f3n de los repositorios.</p> <p>Por ejemplo, para cargar un dataset CSV con Pandas podr\u00edamos hacer:</p> <pre><code>from huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"aitor-medrano/iabd\"\nFICHERO = \"cp_train.csv\"\n\ndataset = pd.read_csv(\n    hf_hub_download(repo_id=REPO_ID, filename=FICHERO, repo_type=\"dataset\")\n)\nprint(dataset)\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 3214 entries, 0 to 3213\n# Data columns (total 3 columns):\n#  #   Column   Non-Null Count  Dtype  \n# ---  ------   --------------  -----  \n#  0   formula  3214 non-null   object \n#  1   T        3214 non-null   float64\n#  2   Cp       3214 non-null   float64\n# dtypes: float64(2), object(1)\n# memory usage: 75.5+ KB\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#cargando-en-streaming","title":"Cargando en streaming","text":"<p>Si el dataset ocupa varios gigas, con modelos como BERT o GPT, puede que no quepan en memoria RAM. Para ello, la librer\u00eda datasets gestiona los datos como ficheros mapeados en memoria realizando un streaming de los datos.</p> Streaming de un dataset - https://huggingface.co/docs/datasets/stream <p>Para este apartado, vamos a utilizar parte de un dataset con c\u00f3digo encontrado en GitHub de los diferentes lenguajes de programaci\u00f3n, conocido como BigCode.</p> <p>Gated dataset</p> <p>El dataset de BigCode est\u00e1 configurado para pedir las credenciales de los usuarios que los utilizan. Esto se conoce como un Gated dataset. Es por ello que tenemos que hacer login en Hugging Face antes de poder descargarlo.</p> <p>En nuestro caso, nos vamos a centrar en archivos <code>DockerFile</code> para que el dataset sea asumible (alrededor de 1Gb). As\u00ed pues, cargamos los datos:</p> <pre><code>from datasets import load_dataset\n\nbigcode_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\")\nprint(bigcode_dataset)\n# Dataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     num_rows: 4155925\n# })\n</code></pre> <p>Y el contenido de un documento:</p> <pre><code>{\n   \"blob_id\":\"a29dd2b33082d2b98541c66ba3620ae054991503\",\n   \"directory_id\":\"71568d223947f51cb007a8564e5f808e0987779e\",\n   \"path\":\"/src/Dapr/GameServer.Host/Dockerfile\",\n   \"content_id\":\"072b3138be512a71cf4e8bbbdb657497e808d2f6\",\n   \"detected_licenses\":[\n      \"MIT\",\n      \"LicenseRef-scancode-unknown-license-reference\"\n   ],\n   \"license_type\":\"permissive\",\n   \"repo_name\":\"devblack/OpenMU\",\n   \"snapshot_id\":\"8cdc521178da47c9c7d2daa502dc71688835fd0a\",\n   \"revision_id\":\"1064b0dca1a491bc28f325e42ca6b97d406d6558\",\n   \"branch_name\":\"refs/heads/master\",\n   \"visit_date\":Timestamp(\"2023-04-07 10:56:30.043316\"),\n   \"revision_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"committer_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"github_id\":None,\n   \"star_events_count\":0,\n   \"fork_events_count\":0,\n   \"gha_license_id\":None,\n   \"gha_event_created_at\":None,\n   \"gha_created_at\":None,\n   \"gha_language\":None,\n   \"src_encoding\":\"UTF-8\",\n   \"language\":\"Dockerfile\",\n   \"is_vendor\":false,\n   \"is_generated\":false,\n   \"length_bytes\":709,\n   \"extension\":\"\"\n}\n</code></pre> <p>Si ahora cargamos el dataset mediante streaming, pas\u00e1ndole el par\u00e1metros <code>streaming=True</code>, en vez de recuperar un Dataset, obtendremos un <code>IterableDataset</code>:</p> <pre><code>from datasets import load_dataset\n\nstreaming_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\", streaming=True)\nprint(streaming_dataset)\n# Resolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 976/976 [00:00&lt;00:00, 1064.36it/s]\n# IterableDataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     n_shards: 1\n# })\n</code></pre> <p>Como sugiere el nombre, para acceder a los elementos del <code>IterableDataset</code> necesitamos iterar sobre \u00e9l:</p> <pre><code>print(next(iter(streaming_dataset)))\n</code></pre> <p>Los elementos de un dataset en streaming  se pueden procesar al vuelo mediante la funci\u00f3n <code>map</code> o <code>filter</code>. Por ejemplo, para pasar a may\u00fasculas el campo <code>license_type</code> har\u00edamos:</p> <pre><code>def mayusLicencias(registro):\n    registro[\"license_type\"] = registro[\"license_type\"].upper()\n    return registro\n\nmapped_dataset = streaming_dataset.map(mayusLicencias)\n\nprint(next(iter(streaming_dataset)))\n# ... 'detected_licenses': ['MIT', ...], 'license_type': 'PERMISSIVE', 'repo_name': 'devblack/OpenMU' ...\n</code></pre> <p>Al vuelo</p> <p>Debemos tener en cuenta que la transformaci\u00f3n se realizar\u00e1 al recorrer el dataset, no al invocar a los m\u00e9todos <code>map</code> o <code>filter</code>.</p> <p>Otras opciones son utilizar las operaciones <code>take()</code>, <code>shuffle()</code> o <code>skip()</code> dentro del <code>IterableDataset</code> para trabajar con muestras peque\u00f1as de los datos cargados:</p> <pre><code>muestra = streaming_dataset.take(100)\nmuestra_aleatoria = streaming_dataset.shuffle(seed=42, buffer_size=1000).take(500)\nrango = streaming_dataset.skip(1000).take(100)\n</code></pre> <p>M\u00e1s informaci\u00f3n en https://huggingface.co/docs/datasets/stream y https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable.</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#limpiando","title":"Limpiando","text":"<p>De forma similar a Pandas o Spark, podemos manipular el contenido de los objetos <code>Dataset</code>.</p> <p>El primer paso deber\u00eda ser barajar los datos para desordenarlos y coger una muestra. Para ello, emplearemos <code>Dataset.shuffle()</code>:</p> <pre><code>squad_train = squad_dataset_trainval[\"train\"]\nprint(squad_train[0][\"title\"]) # Beyonc\u00e9 Knowles\n\nsquad_train_shuffled = squad_train.shuffle(seed=333)\nprint(squad_train_shuffled[0][\"title\"]) # Carnaval\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#seleccionando-filas","title":"Seleccionando filas","text":"<p>Para recuperar filas, usaremos el m\u00e9todo <code>Dataset.select()</code> pas\u00e1ndole un iterador con las posiciones a seleccionar:</p> <pre><code>tres_filas = squad_train_shuffled.select([5,10,15])\nseis_filas = squad_train_shuffled.select(range(6))\n</code></pre> <p>Si queremos seleccionar las filas por alg\u00fan criterio espec\u00edfico, debemos emplear <code>Dataset.filter()</code> y funciones lambda:</p> <pre><code>empieza_por_b_filas = squad_train_shuffled.filter(lambda x: x[\"title\"].startswith(\"B\"))\nfor i in range(empieza_por_b_filas.num_rows):\n    print(empieza_por_b_filas[i][\"title\"])\n# Beidou _ Navigation _ Satellite _ System (en ingl\u00e9s)\n# Biodiversidad\n# Boston ...\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#trabajando-con-columnas","title":"Trabajando con columnas","text":"<p>Al trabajar con columnas, podemos a\u00f1adir una nueva mediante <code>Dataset.add_column()</code>, cambiarles el nombre con <code>DatasetDict.rename_column()</code> o eliminar una columna con <code>DatasetDict.remove_column()</code>:</p> <pre><code>nueva_columna = [\"nueva\"] * squad_train_shuffled.num_rows\nsquad_train_shuffled = squad_train_shuffled.add_column(name=\"inicial\",column=nueva_columna)\n# Dataset({\n#     features: ['title', 'paragraphs', 'inicial'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.rename_column(\n    original_column_name=\"inicial\", new_column_name=\"modificada\"\n)\n# Dataset({\n#     features: ['title', 'paragraphs', 'modificada'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.remove_columns([\"modificada\"])\n# Dataset({\n#     features: ['title', 'paragraphs'],\n#     num_rows: 442\n# })\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#trabajando-con-map","title":"Trabajando con map","text":"<p>Y la joya de la corona es la funci\u00f3n <code>Dataset.map()</code> para aplicar una transformaci\u00f3n a medida. Por ejemplo, si queremos modificar todos los t\u00edtulos y pasarlos a min\u00fascula haremos:</p> squad-map.py<pre><code>from datasets import load_dataset\n\nficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nsquad_dataset = squad_dataset[\"train\"]\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\nsquad_minus = squad_dataset.map(titulo_minus)\n\n# Mostramos 5 elementos\nprint(squad_minus.shuffle(seed=987)[\"title\"][:5])\n# ['marshall', 'ascensor', 'kanye', 'bbc televisi\u00f3n', 'florida']\n</code></pre> <p>Mediante la funci\u00f3n <code>map</code> tambi\u00e9n podemos crear nuevas columnas a partir de los valores de otras. Por ejemplo, a\u00f1adimos una nueva caracter\u00edstica con la cantidad de p\u00e1rrafos y quitamos toda la informaci\u00f3n existente previamente de los p\u00e1rrafos:</p> <pre><code># A\u00f1adimos nueva columna\nsquad_col_num_parrafos = squad_dataset.map(lambda x: {\"num_paragraphs\":len(x[\"paragraphs\"])})\n# Borramos la antigua\nsquad_col_num_parrafos = squad_col_num_parrafos.remove_columns(\"paragraphs\")\nprint(squad_col_num_parrafos.shuffle(seed=987)[:5])\n# {'title': ['Marshall', 'Ascensor', 'Kanye', 'BBC Televisi\u00f3n', 'Florida'], 'num_paragraphs': [51, 52, 79, 31, 35]}\n</code></pre> <p>\u00bfY si queremos ordenar los p\u00e1rrafos por la cantidad de p\u00e1rrafos? Para ello, podemos utilizar <code>Dataset.sort()</code>:</p> <pre><code>squad_num_parrafos_ordenados = squad_col_num_parrafos.sort(\"num_paragraphs\")\n\nprint(squad_num_parrafos_ordenados[:3]) # tres primeros\n# {'title': ['Tono _ (m\u00fasica)', 'Uva', 'Letra _ case'], 'num_paragraphs': [10, 10, 12]}\n\nprint(squad_num_parrafos_ordenados[-3:]) # tres \u00faltimos\n# {'title': ['American Idol (en ingl\u00e9s).', 'Nueva York', 'Budismo'], 'num_paragraphs': [127, 148, 149]}\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#trabajando-con-batches","title":"Trabajando con batches","text":"<p>El m\u00e9todo <code>Dataset.map()</code> admite el argumento <code>batched</code> que, si se establece en <code>True</code>, hace que env\u00ede un lote de datos a la funci\u00f3n <code>map</code> a la vez (el tama\u00f1o del lote es configurable mediante el par\u00e1metro <code>batch_size</code>, pero por defecto es de 1000). Al indicar <code>batched=True</code>, la funci\u00f3n <code>map</code> recibe un diccionario con los campos del conjunto de datos, pero cada valor es ahora una lista de valores, y no un \u00fanico valor, y del mismo modo, devuelve un diccionario con los campos que queremos actualizar o a\u00f1adir a nuestro conjunto de datos, y una lista de valores.</p> <p>Por ello, debemos modificar la funci\u00f3n que realiza el <code>map</code> para que trabaje con un diccionario de listas.</p> <p>Para el siguiente ejemplo, vamos a coger el dataset de SQuAD_es desde Hugging Face que contiene m\u00e1s datos, y vamos a comparar el tiempo de ejecuci\u00f3n, recodificando la funci\u00f3n para trabajar con un diccionario que contiene una lista por cada campo:</p> squad-map-batch.py<pre><code>from datasets import load_dataset\nimport time\n\n# Cargamos el dataset desde HF\nsquad_dataset = load_dataset(\"squad_es\", \"v2.0.0\", split=\"train\")  # (1)!\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus) # (2)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 37926.07 examples/s]   \nfin = time.time()\nprint(fin - inicio) # 3.008699893951416\n\ndef titulo_minus_batch(batch):\n    return {\"title\":[titulo.lower() for titulo in batch[\"title\"]]}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus_batch, batched=True) # (3)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 535210.67 examples/s]\nfin = time.time()\nprint(fin - inicio) # 0.19133710861206055\n</code></pre> <ol> <li>Cargamos los datos desde el dataset de Hugging Face</li> <li>El mapper normal realiza casi 40.000 registros por segundo</li> <li>Al hacerlo con batches realiza m\u00e1s de 500.000 por segundo</li> </ol>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#uso-de-pandas","title":"Uso de Pandas","text":"<p>Para facilitar la conversi\u00f3n entre diferentes formados, podemos usar el m\u00e9todo <code>Dataset.set_format()</code> para modificar el formato de salida (por ejemplo, a <code>pandas</code>, <code>numpy</code>, <code>torch</code>, <code>tensorflow</code>, <code>jax</code>, ...), sin modificar el formato original del dataset (el cual es Apache Arrow - <code>arrow</code>).</p> <p>As\u00ed, si por ejemplo queremos pasar los datos a Pandas para trabajar con ellos, transformarlos o visualizarlos, podemos hacer:</p> <pre><code>print(squad_dataset[0])\nsquad_dataset.set_format(\"pandas\")\ndf = squad_dataset[:]\nprint(df.shape) # (130313, 5)\n\n# Otra forma\ndf2 = squad_dataset.to_pandas()\nprint(df2.info())\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 130313 entries, 0 to 130312\n# Data columns (total 5 columns):\n#  #   Column    Non-Null Count   Dtype \n# ---  ------    --------------   ----- \n#  0   id        130313 non-null  object\n#  1   title     130313 non-null  object\n#  2   context   130313 non-null  object\n#  3   question  130313 non-null  object\n#  4   answers   130313 non-null  object\n# dtypes: object(5)\n# memory usage: 5.0+ MB\n\n# Y realizar consultas sobre los datos, por ejemplo, la 5 preguntas con m\u00e1s respuestas\nprint(df2.groupby(\"title\")[\"answers\"].count().nlargest(5))\n# title\n# Reina                         883\n# Nueva York                    817\n# American Idol (en ingl\u00e9s).    790\n# Beyonc\u00e9 Knowles               753\n# Universidad                   738\n</code></pre> <p>Y para volver a tener un objeto <code>Dataset</code>, podemos utilizar el m\u00e9todo <code>Dataset.from_pandas()</code> o resetear el dataset que ten\u00edamos mediante <code>Dataset.reset_format()</code>:</p> <pre><code>squad_transformed = Dataset.from_pandas(df)\nsquad_original = squad_dataset.reset_format()\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#creando-un-dataset-desde-cero","title":"Creando un dataset desde cero","text":"<p>Si en vez de cargar un dataset, tenemos que crear uno, el primer paso ser\u00e1 obtener los datos.</p> <p>Ya sea mediante peticiones a URL externas con la librer\u00eda <code>request</code>, accediendo a bases de datos relacionales, por ejemplo, mediante SQLAlchemy o a MongoDB mediante <code>PyMongo</code>, lo m\u00e1s normal es crear una estructura de datos mediante una lista o un diccionario y luego crear el dataset a partir del mismo.</p> <p>As\u00ed pues, si en vez de cargar un dataset queremos crear uno desde cero, podemos hacer uso de <code>Dataset.from_dict()</code> o <code>Dataset.from_list()</code> dependiendo de donde tengamos los datos. Por ejemplo, vamos a conectarnos a MongoDB y creamos una lista con todos los datos. A partir de la lista, generamos un Dataset:</p> dataset-mongodb.py<pre><code>from datasets import Dataset\nfrom pymongo import MongoClient\n\ncliente = MongoClient('mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net')\n\n# Vamos a trabajar con la colecci\u00f3n grades de sample_training\niabd_db = cliente.sample_training\ngrades_coll = iabd_db.grades\n\n# Creamos un dataset vac\u00edo mediante una lista\ndocumentos = []\n\n# Recuperamos todas las calificaciones\ncursor = grades_coll.find({})\nfor document in cursor:\n  del document[\"_id\"]  # Quitamos el ObjectId\n  documentos.append(document)\n\nmongo_dataset = Dataset.from_list(documentos)\nprint(mongo_dataset)\n# Dataset({\n#     features: ['student_id', 'scores', 'class_id'],\n#     num_rows: 100000\n# })\n</code></pre> <p>O si queremos almacenar los datos de forma temporal, por ejemplo, podemos hacer uso de Pandas:</p> <pre><code>df = pd.DataFrame.from_records(documentos)\ndf.to_json(\"grades-docs.jsonl\", orient=\"records\", lines=True)\njsonl_dataset = load_dataset(\"json\", data_files=\"grades-docs.jsonl\", split=\"train\")\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#persistiendo","title":"Persistiendo","text":"<p>Cuando cargamos un dataset se cachean los datos para evitar tener que descargar los datos de forma remota y crear las tablas de PyArrow. Podemos averiguar donde se almacenan los datos mediante la propiedad <code>cache_files</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset.cache_files)\n# {'train': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-train.arrow'}],\n#  'test': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-test.arrow'}]}\n</code></pre> <p>Si necesitamos persistir los datos de nuevo con las transformaciones realizadas, podemos hacerlo en diferentes formatos:</p> <ul> <li>Arrow: <code>Dataset.save_to_disk()</code></li> <li>CSV: <code>Dataset.to_csv()</code></li> <li>Pandas: <code>Dataset.to_pandas()</code></li> <li>JSON: <code>Dataset.to_json()</code></li> <li>Parquet: <code>Dataset.to_parquet()</code></li> </ul> <p>Si por ejemplo persistimos el dataset en Arrow:</p> <pre><code>squad_dataset.save_to_disk(\"squad_train_test\")\n</code></pre> <p>Se crear\u00e1 la siguiente estructura en las carpetas, donde para cada split se ha creado su propia carpeta, as\u00ed como un par de archivos con metadatos:</p> <pre><code>squad_train_test\n\u251c\u2500\u2500 dataset_dict.json\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 data-00000-of-00001.arrow\n\u2502   \u251c\u2500\u2500 dataset_info.json\n\u2502   \u2514\u2500\u2500 state.json\n\u2514\u2500\u2500 train\n    \u251c\u2500\u2500 data-00000-of-00001.arrow\n    \u251c\u2500\u2500 dataset_info.json\n    \u2514\u2500\u2500 state.json\n</code></pre> <p>Una vez que hemos persistido el dataset, lo podemos volver a cargar mediante <code>load_from_disk</code>:</p> <pre><code>from datasets import load_from_disk\nsquad_disk = load_from_disk(\"squad_train_test\")\n</code></pre> <p>Si queremos persistirlos en formato CSV o JSON, por cada uno de los splits se crea un archivo separado. Para ello, podemos iterar sobre las claves mediante <code>DatasetDict.items()</code>:</p> <pre><code>for split, dataset in squad_dataset.items():\n    dataset.to_json(f\"squad_{split}.jsonl\")\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#publicando","title":"Publicando","text":"<p>Pues una vez hemos creado nuestro Dataset y lo hemos persistido en disco, el \u00fanico paso que nos queda es publicar en Hugging Face. Para ello, en vez de subir los archivos a mano como hicimos en la sesi\u00f3n inicial desde el interfaz web o hacer uso de Git y copiar los archivos, desde Python es mucho m\u00e1s c\u00f3modo.</p> <p>Previamente debemos realizar login para autenticarnos, tal como vimos en la secci\u00f3n de Login de la sesi\u00f3n anterior y usar nuestro token de acceso (en este caso, necesitamos que nuestro token sea de escritura):</p> <pre><code>hf auth login\n</code></pre> <p>Login desde un cuaderno Jupyter / Colab</p> <p>Si trabajamos con un cuaderno de Jupyter o Colab, podemos utilizar la librer\u00eda de huggingface_hub y al ejecutarlo, nos aparecer\u00e1 una ventana donde introducir el token de acceso:</p> <pre><code>from huggingface_hub import login\nlogin()\n</code></pre> <p> Login desde un notebook </p> <p>Tambi\u00e9n podemos hacer uso de la variable de entorno <code>HF_TOKEN</code> para almacenar el token de acceso, ya sea con un Space almacen\u00e1ndolo en un secret de Spaces o en claves privadas de Google Colab.</p> <p>Una vez autenticado, es tan sencillo como usar el m\u00e9todo <code>push_to_hub()</code> (si queremos publicar el dataset como privado, le a\u00f1adimos el par\u00e1metro <code>private=True</code>):</p> <pre><code>midataset.push_to_hub(\"aitor-medrano/midatasetpushed\")\n# midataset.push_to_hub(\"aitor-medrano/midatasetpushed\", private=True)\n</code></pre> <p>Si tuvi\u00e9ramos diferentes splits podr\u00edamos hacer:</p> <pre><code>train_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"train\")\nval_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"validation\")\n</code></pre> <p>Creando el dataset card</p> <p>Para crear el dataset card, podemos emplear la aplicaci\u00f3n https://huggingface.co/spaces/huggingface/datasets-tagging que facilita rellenar los campos e incluye recomendaciones de otros datasets.</p> <p>Tambi\u00e9n se recomienda leer la Dataset Card Creation Guide</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#datasets-multimedia","title":"Datasets multimedia","text":"<p>Si nuestro dataset va a contener elementos multimedia con im\u00e1genes o audios, podemos subir los archivos en crudo, lo cual es lo m\u00e1s practico en la mayor\u00eda de los casos. Si los dataset de im\u00e1genes o audios son muy grandes, se recomienda el formato WebDataset, aunque lo m\u00e1s normal, es almacenar el dataset en formato Parquet.</p> <p>Dicho esto, vamos a ver como trabajar con datasets que contienen elementos multimedia.</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#audios","title":"Audios","text":"<p>Para trabajar con audios, necesitaremos instalar las librer\u00edas de audio:</p> <pre><code>pip install datasets[audio]\n</code></pre> <p>Una vez instalado, vamos a cargar un dataset que contiene audios, como es PolyAI/minds14:</p> <pre><code>from datasets import load_dataset\n\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\nprint(minds)\n# Dataset({\n#     features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n#     num_rows: 486\n# })\n</code></pre> <p>El dataset contiene 486 archivos de audio, cada uno de los cuales va acompa\u00f1ado de una transcripci\u00f3n, una traducci\u00f3n al ingl\u00e9s y una etiqueta que indica la intenci\u00f3n de la consulta de la persona. La columna de audio contiene los datos de audio sin procesar. Veamos uno de los ejemplos:</p> <pre><code>{'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n 'audio': {'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n    'array': array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            -0.00024414, -0.00024414]),\n    'sampling_rate': 8000\n },\n 'transcription': 'hola buenas a ver tengo un problema con vuestra aplicaci\u00f3n resulta que quiero hacer una transferencia bancaria a una cuenta conocida',\n 'english_transcription': 'hello good to see I have a problem with your application it turns out that I want to make a bank transfer to a known account',\n 'intent_class': 2,\n 'lang_id': 5}\n</code></pre> <p>Si nos centramos en el campo <code>audio</code>, vemos como contiene diferentes valores, los cuales forman parte de un objeto <code>Audio</code>:</p> <ul> <li><code>path</code>: ruta al archivo de audio (en este ejemplo <code>*.wav</code>).</li> <li><code>array</code>: los datos del audio decodificados, representados por un array de NumPy</li> <li><code>sampling_rate</code>: la frecuencia de muestreo (8,000 Hz).</li> </ul> <pre><code>ejemplo = minds.shuffle()[0]\n\nid2label = minds.features[\"intent_class\"].int2str\nlabel = id2label(ejemplo[\"intent_class\"])\n</code></pre> <p>Y si queremos escuchar el audio, usando Gradio podemos crear un componente:</p> <pre><code> with gr.Blocks() as demo:\n    with gr.Column():\n        gr.Audio(audio[\"path\"], label=label)\ndemo.launch(share=True)\n</code></pre> <p>Y reproducir el audio seleccionado:</p> Reproduciendo un audio desde Gradio <p>Finalmente, si queremos mostrar un gr\u00e1fico con el espectograma del audio, y haciendo uso de la librer\u00eda Librosa:</p> <pre><code>import librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\n\narray = ejemplo[\"audio\"][\"array\"]\nsampling_rate = ejemplo[\"audio\"][\"sampling_rate\"]\n\nplt.figure().set_figwidth(12)\nlibrosa.display.waveshow(array, sr=sampling_rate)\n</code></pre> <p>Obteniendo:</p> Representaci\u00f3n gr\u00e1fica de un audio","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#creando-un-dataset-desde-python","title":"Creando un dataset desde Python","text":"<p>Si queremos crear un dataset a partir de audios propios, necesitamos crear un diccionario o una lista con la ruta a nuestros audios, y a continuaci\u00f3n, llamar al m\u00e9todo <code>cast_column</code> para transformar la columna a tipo <code>Audio</code> con las caracter\u00edsticas que hemos visto antes.</p> <p>As\u00ed pues, si tenemos los datos en un diccionario podemos hacer:</p> <pre><code>audio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", \"path/to/audio_2\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#audiofolder","title":"Audiofolder","text":"<p>Otra posibilidad es crear el dataset a partir de una carpeta con todos los audios, y un archivo <code>metadata.csv</code> con los datos:</p> <pre><code>folder/train/metadata.csv\nfolder/train/first_audio_file.mp3\nfolder/train/second_audio_file.mp3\nfolder/train/third_audio_file.mp3\n</code></pre> <p>Y un archivo de metadatos con:</p> metadata.csv<pre><code>file_name,transcription\nfirst_audio_file.mp3,este es el texto del primer audio\nsecond_audio_file.mp3,en el segundo audio cuento un chiste\nthird_audio_file.mp3,y en este audio agradezco al p\u00fablico sus abucheos\n</code></pre> <p>Y a continuaci\u00f3n, indicamos como primer par\u00e1metro <code>autofolder</code> y con <code>data_dir</code> indicamos la carpeta que contiene los audios:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#referencias","title":"Referencias","text":"<ul> <li>Documentaci\u00f3n oficial de dataset</li> <li>El cap\u00edtulo La librer\u00eda Dataset del curso NLP de Hugging Face.</li> </ul>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#actividades","title":"Actividades","text":"<ol> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y s\u00f3lo empleando Python:</p> <ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con lo datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset s\u00f3lo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 0.5p) Carga en streaming el dataset de imagenet-1k, y muestra la etiqueta de los primeros 5 elementos.</p> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 1p) Tenemos un dataset con informaci\u00f3n sobre vuelos retrasados en Espa\u00f1a y otro con informaci\u00f3n general sobre los aeropuertos. Se pide:</p> <ol> <li>Une ambos datasets para que la informaci\u00f3n de los vuelos contengan el nombre y la ciudad de los aeropuertos.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato Arrow.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / opcional ) Sobre la base de datos de MongoDB de <code>sample_training</code> disponible en los datos de muestra de MongoAtlas:</p> <ol> <li>Carga los de estados de <code>states.js</code>.</li> <li>Realiza una join entre las colecciones <code>zips</code> y <code>states</code>.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato JSON.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> </ol>","tags":["HuggingFace"]},{"location":"bedrock/sesion1/","title":"\ud83d\udcd8 Sesi\u00f3n 1: Introducci\u00f3n a Amazon Bedrock","text":""},{"location":"bedrock/sesion1/#objetivos","title":"Objetivos:","text":"<ul> <li>Comprender qu\u00e9 es Amazon Bedrock, sus funcionalidades principales y su prop\u00f3sito.</li> <li>Explorar los conceptos de \u201cmodelos fundacionales\u201d (FMs), \u201cRAG\u201d (generaci\u00f3n aumentada por recuperaci\u00f3n), \u201cfine-tuning\u201d y \u201cguards / guardrails\u201d para una IA responsable.</li> <li>Formular hip\u00f3tesis sobre c\u00f3mo una empresa (hotel, restaurante, ...) o centros educativos, ayuntamientos, etc\u00e9tera podr\u00edan beneficiarse de la IA generativa.</li> <li>Dise\u00f1ar, en equipo, una propuesta de aplicaci\u00f3n concreta usando Amazon Bedrock adaptada a un caso real.</li> </ul>"},{"location":"bedrock/sesion1/#1-introduccion-a-amazon-bedrock","title":"1. Introducci\u00f3n a Amazon Bedrock","text":"<p>Amazon Bedrock es un servicio de AWS que permite a los desarrolladores construir aplicaciones generativas utilizando modelos fundacionales (FMs) sin necesidad de gestionar infraestructura. Ofrece acceso a modelos como Claude (Anthropic), Titan (Amazon) y Stable Diffusion (Stability AI).</p>"},{"location":"bedrock/sesion1/#capacidades-destacadas","title":"Capacidades destacadas","text":"<ul> <li>RAG (Retrieval-Augmented Generation): podemos conectar Bedrock a nuestras propias fuentes de datos (documentos, bases de conocimiento) de modo que las respuestas del modelo est\u00e9n informadas por datos reales de nuestra empresa. Esto ayuda a responder consultas concretas basadas en informaci\u00f3n actualizada.  Amazon Web Services, Inc. </li> <li>Fine-tuning / personalizaci\u00f3n privada: es posible adaptar un modelo para tareas espec\u00edficas o para un dominio concreto (por ejemplo, hoteler\u00eda, turismo, restaurante, etc.), usando nuestros propios datos, sin que esos datos entren a formar parte del modelo base. </li> <li>Seguridad, privacidad e IA responsable: Bedrock integra funcionalidades de protecci\u00f3n, guardrails, control de contenidos y privacidad de datos, para reducir riesgos \u2014 por ejemplo, filtrado de contenido inapropiado, protecci\u00f3n de datos, auditor\u00eda\u2026 </li> <li>Flexibilidad de modelos: podemos elegir entre muchos FMs de distintos proveedores seg\u00fan el uso: algunos ser\u00e1n mejores para generaci\u00f3n de texto creativa; otros para respuestas precisas; otros para integraci\u00f3n con datos. </li> </ul>"},{"location":"bedrock/sesion1/#que-es-retrieval-augmented-generation-rag-o-generacion-aumentada-por-recuperacion","title":"\u00bfQu\u00e9 es \"Retrieval Augmented Generation\" (RAG) o Generaci\u00f3n Aumentada por Recuperaci\u00f3n ?","text":"<p>La Retrieval Augmented Generation (RAG) o Generaci\u00f3n Aumentada por Recuperaci\u00f3n en espa\u00f1ol, es una t\u00e9cnica de IA que combina dos componentes: un sistema de recuperaci\u00f3n de informaci\u00f3n (bases de datos vectoriales) y un LLM (modelo de lenguaje grande).  El objetivo es mejorar la precisi\u00f3n y la actualidad de las respuestas del LLM al permitirle acceder y utilizar informaci\u00f3n de fuentes de datos externas y espec\u00edficas antes de generar su respuesta, sin necesidad de reentrenamiento. </p> <ul> <li>Retrieval (Recuperaci\u00f3n): Esta etapa consiste en indexar, recuperar los segmentos de texto creados (embeddings) que son relevantes en funci\u00f3n de la similutd sem\u00e1ntica.</li> <li>Augmentation (aumentar): Incrementar con informaci\u00f3n adicional los conocimientos del LLM.</li> <li>Generation: Generar o elaborar respuestas a partir de los conocimientos del LLM. </li> </ul>"},{"location":"bedrock/sesion1/#como-funciona","title":"C\u00f3mo funciona","text":"<ol> <li>Recuperaci\u00f3n: Cuando se hace una consulta, un sistema de recuperaci\u00f3n busca y selecciona los fragmentos de informaci\u00f3n m\u00e1s relevantes de una base de conocimiento externa (que puede incluir documentos privados, bases de datos o fuentes de noticias).</li> <li>Generaci\u00f3n: El modelo de lenguaje grande (LLM) toma la consulta original junto con la informaci\u00f3n recuperada para generar una respuesta m\u00e1s precisa, actualizada y contextualizada.</li> <li>Ejemplo: Si un usuario pregunta sobre un producto espec\u00edfico, el sistema RAG puede buscar en la base de datos de la empresa informaci\u00f3n sobre ese producto y luego usarla para que el LLM genere una respuesta detallada y precisa. </li> </ol>"},{"location":"bedrock/sesion1/#beneficios-de-rag","title":"Beneficios de RAG","text":"<ul> <li>Acceso a datos actualizados: Permite a los LLM acceder a informaci\u00f3n m\u00e1s reciente que la que ten\u00edan durante su entrenamiento inicial.</li> <li>Reducci\u00f3n de alucinaciones : Disminuye la probabilidad de que el modelo \"invente\" informaci\u00f3n, ya que se basa en datos concretos.</li> <li>Adaptaci\u00f3n a dominios espec\u00edficos: Facilita la creaci\u00f3n de chatbots o aplicaciones que pueden responder preguntas sobre temas muy espec\u00edficos o propietarios, como el conocimiento interno de una empresa.</li> <li>Referencia de fuentes: Puede citar las fuentes de informaci\u00f3n utilizadas, lo que aumenta la transparencia y la confianza en las respuestas.</li> <li>Eficiencia: Es una forma m\u00e1s r\u00e1pida y econ\u00f3mica de actualizar la informaci\u00f3n de un LLM en comparaci\u00f3n con el reentrenamiento completo del modelo. </li> </ul> <p>M\u00e1s informaci\u00f3n en este v\u00eddeo: https://www.youtube.com/watch?v=-NqZehslaNk</p>"},{"location":"bedrock/sesion1/#ventajas","title":"Ventajas","text":"<ul> <li>Sin necesidad de entrenar modelos desde cero.</li> <li>Integraci\u00f3n nativa con servicios AWS.</li> <li>Escalabilidad y seguridad empresarial.</li> </ul>"},{"location":"bedrock/sesion1/#casos-de-uso","title":"Casos de uso","text":"<ul> <li>Chatbots inteligentes.</li> <li>Generaci\u00f3n de contenido.</li> <li>Res\u00famenes autom\u00e1ticos.</li> <li>Recuperaci\u00f3n aumentada (RAG).</li> </ul>"},{"location":"bedrock/sesion1/#ejemplos-clave-de-uso-de-bedrock","title":"\ud83d\udca1 Ejemplos Clave de Uso de Bedrock","text":"\u00c1rea de Uso Descripci\u00f3n Modelo Fundacional T\u00edpico Generaci\u00f3n de contenido Crear art\u00edculos, descripciones de productos o scripts autom\u00e1ticamente. Amazon Titan Text, Anthropic Claude B\u00fasqueda y resumen Crear chatbots que responden preguntas bas\u00e1ndose en documentos internos (patrones RAG). Amazon Titan Embeddings, Meta Llama 2 Automatizaci\u00f3n de agentes Construir agentes de IA que pueden realizar tareas complejas de varios pasos (ej. procesar reclamaciones). Agents for Amazon Bedrock (usando modelos como Claude) Generaci\u00f3n de c\u00f3digo Asistencia para desarrolladores que genera fragmentos de c\u00f3digo, traduce lenguajes o explica funciones. Anthropic Claude Code"},{"location":"bedrock/sesion1/#ejemplo-ilustrativo","title":"Ejemplo ilustrativo","text":"<p>Imagina que gestinamos un hotel y queremos ofrecer a nuestros clientes un \u201casistente inteligente\u201d (chatbot) que:</p> <ul> <li>Responda preguntas frecuentes: horarios, servicios, recomendaciones locales.</li> <li>Sugiera experiencias seg\u00fan perfil del cliente (familia, pareja, negocios, con mascotas).</li> <li>Responda en varios idiomas.</li> </ul> <p>Con Amazon Bedrock podr\u00edamos:</p> <ul> <li>Crear una base de conocimiento con informaci\u00f3n propia del hotel: descripciones de servicios, normas, tarifas, actividades, recomendaciones locales.</li> <li>Usar RAG (Retrieval-Augmented Generation) para que el modelo se base en esa informaci\u00f3n interna cuando responda.</li> <li>Si queremos precisi\u00f3n en el estilo de las respuestas (por ejemplo, tono amable, cercano, profesional), har\u00edamos un *fine-tuning con ejemplos de interacciones t\u00edpicas*.</li> <li>Publicar ese asistente como chatbot web, Bot de Telegram, WhatsApp o similar sin tener que disponer de servidores propios: Bedrock lo gestiona.</li> </ul>"},{"location":"bedrock/sesion1/#2-parametros-de-inferencia-y-experimentacion","title":"2. Par\u00e1metros de Inferencia y Experimentaci\u00f3n","text":"<p>Podemos experimentar con las configuraciones del prompt para controlar el comportamiento del modelo.</p> Par\u00e1metro Descripci\u00f3n Impacto en el resultado Temperatura Controla la creatividad y la diversidad de las respuestas. Un valor superior genera respuestas m\u00e1s creativas y diversas. P Superior (Top P) Permite seleccionar palabras m\u00e1s probables. Permite variar entre respuestas m\u00e1s probables o menos probables. Longitud M\u00e1xima (MaxTokenCount) Define el tama\u00f1o m\u00e1ximo de la respuesta generada. Limita el coste y la extensi\u00f3n de la respuesta."},{"location":"bedrock/sesion1/#3-eleccion-estrategica-del-modelo","title":"3. Elecci\u00f3n estrat\u00e9gica del modelo","text":"<p>Amazon Bedrock ofrece flexibilidad para elegir el modelo que mejor se adapte a cada necesidad.</p> <ul> <li>Modelos de Amazon (Titan/Nova): Modelos propietarios que ofrecen inteligencia multimodal r\u00e1pida y rentable, incluyendo generaci\u00f3n de texto, im\u00e1genes, comprensi\u00f3n de documentos y c\u00f3digo. El modelo Nova Lite es multimodal y sensible a los costes, mientras que Nova Pro es competente para tareas m\u00e1s complejas. Los modelos Titan Text Express son recomendados para tareas de alto volumen y bajo coste como el resumen.</li> <li>Anthropic (Claude): Modelos que destacan en razonamiento complejo, generaci\u00f3n de c\u00f3digo y seguimiento de instrucciones, adecuados para industrias que exigen cumplimiento y confianza.</li> <li>Stability AI: Conocidos por sus modelos de generaci\u00f3n de im\u00e1genes, como Stable Diffusion 3.5 Large.</li> <li>DeepSeek: Modelos avanzados de razonamiento que resuelven problemas complejos paso a paso.</li> <li>Mistral AI: Modelos especializados para el razonamiento agentic y tareas multimodales.</li> </ul>"},{"location":"bedrock/sesion1/#ejemplo-basico-generar-texto-con-claude","title":"Ejemplo b\u00e1sico: Generar texto con Claude","text":"<p>C\u00f3digo en Python usando boto3:</p> <pre><code>import boto3\n\nclient = boto3.client('bedrock-runtime', region_name='us-east-1')\n\nprompt = \"Resume en 3 puntos las ventajas de Amazon Bedrock\"\nresponse = client.invoke_model(\n    modelId=\"anthropic.claude-v2\",\n    body={\"input\": prompt}\n)\n\nprint(response['body'])\n</code></pre>"},{"location":"bedrock/sesion1/#hello-bedrock-primeros-pasos-20-minutos","title":"HELLO BEDROCK - PRIMEROS PASOS (20 minutos)","text":""},{"location":"bedrock/sesion1/#objetivos_1","title":"Objetivos:","text":"<ul> <li>Entender la interfaz b\u00e1sica de Amazon Bedrock</li> <li>Realizar primeras interacciones con modelos fundacionales</li> <li>Identificar limitaciones de los modelos sin base de conocimiento</li> </ul>"},{"location":"bedrock/sesion1/#actividades-practicas","title":"Actividades pr\u00e1cticas:","text":""},{"location":"bedrock/sesion1/#configuracion-inicial","title":"Configuraci\u00f3n inicial:","text":"<ul> <li>Gu\u00eda paso a paso para acceder a la consola AWS</li> <li>\"Vamos a seleccionar el modelo Nova Pro para nuestros primeros ejemplos\"</li> <li>Explicaci\u00f3n de la interfaz del playground de Bedrock</li> </ul>"},{"location":"bedrock/sesion1/#ejercicio-hello-bedrock","title":"Ejercicio \"Hello Bedrock\":","text":"<p>Ejemplos de prompts para demostraci\u00f3n: 1. Prompt b\u00e1sico de presentaci\u00f3n: <pre><code>Pres\u00e9ntate y explica brevemente qu\u00e9 puedes hacer como asistente de IA.\n</code></pre> 2. Prompt de conocimiento general: <pre><code>Explica en 5 puntos clave qu\u00e9 es la Inteligencia Artificial Generativa y c\u00f3mo est\u00e1 cambiando el sector educativo.\n</code></pre></p> <ol> <li>Prompt con instrucciones espec\u00edficas: <pre><code>Act\u00faa como un experto en formaci\u00f3n profesional y crea un plan de estudios breve para un m\u00f3dulo de introducci\u00f3n a la IA. El plan debe incluir 3 unidades con sus respectivos objetivos y actividades principales.\n</code></pre></li> <li>Prompt para probar par\u00e1metros: <pre><code>Genera tres ideas creativas para utilizar la IA en el aula. S\u00e9 muy conciso. \n</code></pre> (Demostrar c\u00f3mo cambia la respuesta modificando la temperatura)</li> </ol> <p>Preguntas para la audiencia: - \"\u00bfQu\u00e9 diferencias not\u00e1is entre un prompt simple y uno m\u00e1s estructurado?\" - \"\u00bfC\u00f3mo cre\u00e9is que afecta la temperatura a la creatividad de las respuestas?\"</p>"},{"location":"bedrock/sesion1/#analisis-de-limitaciones","title":"An\u00e1lisis de limitaciones:","text":"<p>Ejemplos de prompts que muestran limitaciones:</p> <ol> <li> <p>Conocimiento actualizado: \u00bfCu\u00e1les son las \u00faltimas normativas de la Generalitat Valenciana sobre formaci\u00f3n profesional publi cadas este a\u00f1o?</p> </li> <li> <p>Informaci\u00f3n espec\u00edfica local: Describe el proceso actual para solicitar una beca de formaci\u00f3n profesional en la Comunidad Valenciana, incluyendo plazos y requisitos espec\u00edficos.</p> </li> <li> <p>Datos t\u00e9cnicos precisos: \u00bfCu\u00e1l es el presupuesto exacto asignado a formaci\u00f3n profesional por la Generalitat Valenciana para el a\u00f1o actual?</p> </li> </ol> <p>Discusi\u00f3n guiada: - \"\u00bfPor qu\u00e9 cre\u00e9is que el modelo no puede responder con precisi\u00f3n a estas preguntas?\" - \"\u00bfQu\u00e9 consecuencias podr\u00eda tener confiar en estas respuestas en un entorno profesional?\" - Explicaci\u00f3n del concepto de \"conocimiento limitado al entrenamiento\" y \"fecha de corte\"</p>"},{"location":"bedrock/sesion1/#bases-de-conocimiento-kb","title":"BASES DE CONOCIMIENTO (KB)","text":""},{"location":"bedrock/sesion1/#objetivos_2","title":"Objetivos:","text":"<ul> <li>Comprender qu\u00e9 es una base de conocimiento y su importancia</li> <li>Identificar los componentes necesarios para crear una KB</li> <li>Aprender a preparar documentos para su ingesta</li> </ul> <p>Knowledge Basement (KB): Una base de conocimiento es un repositorio que almacena informaci\u00f3n estructurada y permite a los modelos de IA acceder a datos espec\u00edficos fuera de su entrenamiento original.</p>"},{"location":"bedrock/sesion1/#elementos-clave-explicados","title":"Elementos clave explicados:","text":"<ul> <li>Presentaci\u00f3n: https://docs.google.com/presentation/d/1lRpMixrurXHReZgOSvTRono_rKKaiqhfuf5uFSFOP_U/edit?slide=id.g3347f17ef2a_0_14#slide=id.g3347f17ef2a_0_14</li> <li>Fuentes de datos compatibles: \"Bedrock puede procesar PDFs, documentos de texto, HTML, y otros formatos\"</li> <li>Vector store y embeddings: \"Los embeddings son representaciones num\u00e9ricas del significado de un texto\" * Analog\u00eda visual: \"Imaginad una biblioteca donde cada libro est\u00e1 ubicado junto a otros con temas similares, no por orden alfab\u00e9tico\"</li> <li>Chunking: \"Dividimos los documentos en fragmentos manejables para el modelo\"* Pregunta: \"\u00bfPor qu\u00e9 cre\u00e9is que es necesario dividir los documentos en fragmentos m\u00e1s peque\u00f1os?\"</li> <li>Metadatos: \"Informaci\u00f3n adicional que nos ayuda a filtrar y organizar el conocimiento\"</li> </ul>"},{"location":"bedrock/sesion1/#actividad-practica","title":"Actividad pr\u00e1ctica:","text":"<p>Preparaci\u00f3n de documentos: - An\u00e1lisis de documentos de muestra:  - Mostrar ejemplos de documentos administrativos de la Generalitat  - \"Vamos a analizar este documento sobre procedimientos de contrataci\u00f3n p\u00fablica\" - Mejores pr\u00e1cticas para estructurar informaci\u00f3n:  - \"Los documentos bien estructurados mejoran la precisi\u00f3n de las respuestas\"  - Ejemplos de buena vs. mala estructuraci\u00f3n  - Importancia de los t\u00edtulos, subt\u00edtulos y formato consistente - Consideraciones ling\u00fc\u00edsticas:  - \"Nuestra KB debe manejar documentos en castellano y valenciano\"  - Pregunta: \"\u00bfQu\u00e9 desaf\u00edos cre\u00e9is que plantea trabajar con documentos biling\u00fces?\"</p>"},{"location":"bedrock/sesion1/#creacion-de-una-kb-basica","title":"Creaci\u00f3n de una KB b\u00e1sica:","text":"<ul> <li>Demostraci\u00f3n paso a paso:</li> <li>Creaci\u00f3n de un data source</li> <li>Configuraci\u00f3n del vector store</li> <li>Selecci\u00f3n de opciones de chunking (tama\u00f1o, solapamiento)</li> <li>Proceso de ingesta con ejemplos visuales</li> <li>Verificaci\u00f3n:</li> <li>\"As\u00ed podemos comprobar que nuestra KB se ha creado correctamente\"</li> <li>Demostraci\u00f3n de b\u00fasqueda b\u00e1sica para verificar la ingesta</li> </ul>"},{"location":"bedrock/sesion1/#rag-bedrock-teoria-y-practica","title":"RAG BEDROCK: TEOR\u00cdA Y PR\u00c1CTICA","text":""},{"location":"bedrock/sesion1/#objetivos_3","title":"Objetivos:","text":"<ul> <li>Entender el flujo RAG (Retrieval Augmented Generation)</li> <li>Visualizar el proceso de razonamiento del modelo</li> <li>Comprender la memoria conversacional</li> </ul>"},{"location":"bedrock/sesion1/#5-aprendizaje-basado-en-retos","title":"5. Aprendizaje basado en retos","text":""},{"location":"bedrock/sesion1/#reto-1","title":"\u2705 Reto 1","text":"<p>Crear un prompt que genere un plan de marketing para un producto tecnol\u00f3gico.</p>"},{"location":"bedrock/sesion1/#reto-2","title":"\u2705 Reto 2","text":"<p>Implementar un flujo con Bedrock Agents para responder preguntas sobre documentos internos.</p>"},{"location":"bedrock/sesion1/#reto-3","title":"\u2705 Reto 3","text":"<p>Conectar Bedrock con Amazon S3 para usar datos propios en la generaci\u00f3n de respuestas.</p>"},{"location":"bedrock/sesion1/#6-proyecto-integrador-real","title":"6. Proyecto integrador real","text":""},{"location":"bedrock/sesion1/#caso","title":"Caso","text":"<p>Automatizar res\u00famenes de informes t\u00e9cnicos en una empresa manufacturera.</p>"},{"location":"bedrock/sesion1/#arquitectura","title":"Arquitectura","text":"<ul> <li>AWS Lambda + API Gateway + Amazon Bedrock.</li> </ul>"},{"location":"bedrock/sesion1/#flujo","title":"Flujo","text":"<ol> <li>T\u00e9cnico sube informe a S3.</li> <li>Lambda invoca Bedrock para generar resumen.</li> <li>API Gateway expone endpoint para consultar resumen.</li> </ol>"},{"location":"bedrock/sesion1/#codigo-lambda-python","title":"C\u00f3digo Lambda (Python)","text":"<pre><code>import json\nimport boto3\n\ndef lambda_handler(event, context):\n    client = boto3.client('bedrock-runtime')\n    report_text = event['body']\n\n    response = client.invoke_model(\n        modelId=\"anthropic.claude-v2\",\n        body={\"input\": f\"Resume el siguiente informe t\u00e9cnico: {report_text}\"}\n    )\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps({'summary': response['body']})\n    }\n</code></pre>"},{"location":"bedrock/sesion1/#resultado-esperado","title":"Resultado esperado","text":"<ul> <li>Endpoint <code>/summarize</code> devuelve resumen en segundos.</li> <li>Beneficio: reduce tiempo de an\u00e1lisis en un 70%.</li> </ul>"},{"location":"bedrock/sesion1/#7-recursos-adicionales","title":"7. Recursos adicionales","text":"<ul> <li>Documentaci\u00f3n oficial</li> <li>Gu\u00eda r\u00e1pida AWS</li> </ul>"},{"location":"hf/","title":"Apuntes y ejercicios pr\u00e1cticas del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data","text":"<p>Apuntes, pr\u00e1cticas y ejercicios del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data en el IES Severo Ochoa (Elche).</p> <p>Los ejercicios y conocimientos contenidos en las pr\u00e1cticas y/o apuntes de  todos los m\u00f3dulos tienen exclusivamente prop\u00f3sito formativo, por lo que  nunca se deber\u00e1n utilizar con fines maliciosos o delictivos.</p> <p>Ning\u00fan alumno o alumna de este curso, ni profesor o profesora como  docentes, ser\u00e1n responsables de los da\u00f1os directos o indirectos que  pudieran derivarse del uso inadecuado de las herramientas y mecanismos  expuestos.</p>"},{"location":"hf/Actividades/","title":"Actividades","text":""},{"location":"hf/Actividades/#actividades","title":"Actividades","text":"<ol> <li>Estimaci\u00f3n de profundidad Utiliza el pipeline:</li> </ol> <pre><code>from transformers import pipeline\n\ndepth = pipeline(\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n\nresult = depth(\"ruta_o_url_imagen\")\n</code></pre> <ol> <li> <p>Clasificaci\u00f3n de im\u00e1genes Usa el pipeline: <pre><code>from transformers import pipeline\nclassifier = pipeline(\"image-classification\")\nres = classifier(\"ruta_o_url_imagen\")\n\nprint(res)\n</code></pre></p> </li> <li> <p>Avanzado (Optativo): Integrar clasificaci\u00f3n y segmentaci\u00f3n </p> </li> </ol> <p>Ejecuta ambos pipelines y visualiza el resultado conjunto.</p>"},{"location":"hf/Datasets/","title":"\ud83d\udcd8 Hugging Face Datasets: Gu\u00eda + Reto Gamificado","text":""},{"location":"hf/Datasets/#1-introduccion","title":"1\ufe0f\u20e3 Introducci\u00f3n","text":"<p>El paquete <code>datasets</code> de Hugging Face es una potente herramienta para acceder, compartir y procesar conjuntos de datos (datasets) de IA para una amplia gama de tareas, que incluyen:</p> <ul> <li>Procesamiento del Lenguaje Natural (PLN)</li> <li>Visi\u00f3n por computadora</li> <li>Procesamiento de audio</li> </ul> <p>Est\u00e1 dise\u00f1ado para manejar grandes vol\u00famenes de datos de manera eficiente mediante el uso de mapeo de memoria y el formato Apache Arrow, lo que permite trabajar con datos que superan la RAM disponible.</p> <p>Arrow Apache Arrow define un formato de memoria columnar independiente del lenguaje para datos planos y anidados, organizado para operaciones anal\u00edticas eficientes en hardware moderno como CPU y GPU. El formato de memoria Arrow tambi\u00e9n admite lecturas sin copia para un acceso a datos ultrarr\u00e1pido sin sobrecarga de serializaci\u00f3n.</p> <p>El proyecto del formato Apache Arrow comenz\u00f3 en febrero de 2016, centr\u00e1ndose en cargas de trabajo de an\u00e1lisis columnar en memoria. A diferencia de formatos de archivo como Parquet o CSV, que especifican c\u00f3mo se organizan los datos en el disco, Arrow se centra en c\u00f3mo se organizan los datos en la memoria.</p> <p>Los creadores buscan consolidar Arrow como un formato est\u00e1ndar en memoria para el an\u00e1lisis de cargas de trabajo. Estos fundamentos atraen a numerosos colaboradores de proyectos como Pandas, Spark, Cassandra, Apache Calcite, Dremio e Ibis.</p>"},{"location":"hf/Datasets/#caracteristicas-clave","title":"\ud83d\udd11 Caracter\u00edsticas Clave","text":"<ul> <li>Vasto Repositorio (Hub): Gran cantidad de datasets p\u00fablicos y privados.</li> <li>F\u00e1cil Acceso: Carga en una sola l\u00ednea de c\u00f3digo.</li> <li>Procesamiento Eficiente: M\u00e9todos como <code>map()</code> paralelizados.</li> <li>Escalabilidad: Objetos <code>Dataset</code> y <code>IterableDataset</code>.</li> <li>Gesti\u00f3n de Datos: Crear y subir datasets propios al Hub.</li> </ul>"},{"location":"hf/Datasets/#instalacion","title":"\u2699\ufe0f Instalaci\u00f3n","text":"<pre><code>pip install datasets\npip install datasets[audio]\npip install datasets[vision]\n</code></pre>"},{"location":"hf/Datasets/#ejemplo-cargar-un-dataset-local","title":"\ud83e\udde9 Ejemplo: Cargar un dataset local","text":"<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Salida esperada: <pre><code>DatasetDict({\n    train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 })\n})\n</code></pre></p>"},{"location":"hf/Datasets/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"hf/Datasets/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p>"},{"location":"hf/Datasets/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"hf/Datasets/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\ndataset = load_dataset(\"PlanTL-GOB-ES/squad-es\")\nprint(dataset)\n\n# Nivel 2: Dividir en train/test\ntrain_dataset = dataset[\"train\"]\nsplit_dataset = train_dataset.train_test_split(test_size=0.2)\ntrain_split = split_dataset[\"train\"]\ntest_split = split_dataset[\"test\"]\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\ndef add_num_paragraphs(example):\n    return {\"num_paragraphs\": len(example[\"paragraphs\"])}\ntrain_split = train_split.map(add_num_paragraphs)\n\n# Nivel 4: Filtrar y persistir\nfiltered_train = train_split.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nfiltered_train = filtered_train.remove_columns([\"num_paragraphs\"])\nfiltered_train.to_parquet(\"filtered_train.parquet\")\n\n# Nivel 5: Publicar en Hugging Face\n# filtered_train.push_to_hub(\"usuario/nombre-dataset\")\n</code></pre>"},{"location":"hf/Datasets/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"hf/Ejemplo_gradio/","title":"Aplicaci\u00f3n web Gradio + Modelo previamente entrenado","text":"<p>Vamos a crear una aplicaci\u00f3n web con Gradio que use el modelo creado en una sesi\u00f3n anterior: \u200bomarques/autotrain-dogs-and-cats-1527055142</p> <p>Ejemplo de aplicaci\u00f3n Gradio con una imagen de entrada y un Label como componente de salida: </p> <p>Etiquetado de la imagen de entrada: </p>"},{"location":"hf/Ejemplo_gradio/#codigo-de-ejemplo","title":"C\u00f3digo de ejemplo","text":"<pre><code>import gradio as gr\u200b\n\u200b\ndef image_classifier(inp):\u200b\n\u2003 return {'cat': 0.3, 'dog': 0.7}\u200b\n\ndemo = gr.Interface(fn=image_classifier, inputs='image', outputs='label')\u200b\n\ndemo.launch()\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#anadimos-el-modelo","title":"A\u00f1adimos el modelo","text":"<pre><code>import gradio as gr\u200b\nfrom transformers import pipeline\u200b\n\u200b\ndef image_classifier(inp):\u200b\n   pipe = pipeline(\"image-classification\", model=\"omarques/autotrain-dogs-and-cats-1527055142\")\u200b\n   pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\")\u200b\n   return {'cat': 0.3, 'dog': 0.7}\u200b\n\u200b\ndemo = gr.Interface(fn=image_classifier, inputs='image', outputs='label')\u200b\n\n\u200bdemo.launch()\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#instalamos-las-librerias-transformers-y-torch","title":"Instalamos las librer\u00edas transformers y torch\u200b","text":"<p><pre><code>pip install transformers torch\u200b\n</code></pre> Volvemos a probar y comprobamos que funciona correctamente.</p> <p>Dentro del componente Image, por defecto Gradio pasa un objeto tipo <code>numpy.ndarray</code> (la imagen como matriz) a las funciones de Gradio, por lo que debemos especificar el tipo con <code>gr.Image(type=\"filepath\")</code> en la creaci\u00f3n de la interfaz de Gradio\u200b. \u200b</p>"},{"location":"hf/Ejemplo_gradio/#especificar-el-tipo-en-image","title":"Especificar el tipo en Image","text":"<pre><code>demo = gr.Interface(fn=image_classifier, inputs=gr.Image(type=\"filepath\"), outputs='label')\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#codigo-actualizado","title":"C\u00f3digo actualizado","text":"<pre><code>import gradio as gr\u200b\nfrom transformers import pipeline\u200b\n\ndef image_classifier(inp):\u200b\n   #/tmp/gradio/b7be1455904a47b7fb3d953514163c828cc46e093fe7ba8bdeb950039a8e870e/1.png\u200b\n   print(inp) \n   pipe = pipeline(\"image-classification\", model=\"omarques/autotrain-dogs-and-cats-1527055142\")\u200b\n   #[{'label': 'cat', 'score': 0.6151219010353088}, {'label': 'dog', 'score': 0.38487812876701355}]\u200b\n   print(pipe(inp)) \n   return {'cat': 0.3, 'dog': 0.7}\u200b\n\ndemo = gr.Interface(fn=image_classifier, inputs=gr.Image(type=\"filepath\"), outputs='label')\u200b\n\ndemo.launch()\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#de-lista-a-diccionario-para-el-output-label","title":"De lista a diccionario para el output label","text":"<p>Para convertir la lista <code>[{'label': 'cat', 'score': 0.6151219010353088}, {'label': 'dog', 'score': 0.38487812876701355}]\u200b</code> al formato requerido por el componente de salida Label <code>(dict[str, float])</code> de Gradio, debemos crear un diccionario donde las claves son los valores de <code>label</code> y los valores son los correspondientes a <code>score</code>\u200b</p> <p>Ejemplo sencillo:\u200b <pre><code>lista = [\u200b\n\u2003\u2003{'label': 'cat', 'score': 0.6151219010353088},\u200b\n\u2003 {'label': 'dog', 'score': 0.38487812876701355}\u200b\n]\u200b\nresultado = {d['label']: d['score'] for d in lista}\u200b\u200b\n# Resultado: {'cat': 0.8, 'dog': 0.2}\u200b\nprint(resultado)\u200b\n</code></pre></p>"},{"location":"hf/Ejemplo_gradio/#codigo-final","title":"C\u00f3digo final","text":"<p><pre><code>import gradio as gr\u200b\nfrom transformers import pipeline\u200b\n\u200b\ndef image_classifier(inp):\u200b\n   pipe = pipeline(\"image-classification\", model=\"omarques/autotrain-dogs-and-cats-1527055142\")\u200b\n   #[{'label': 'cat', 'score': 0.6151219010353088}, {'label': 'dog', 'score': 0.38487812876701355}]\u200b\n   etiquetas = pipe(inp)\n   #[{'cat': 0.6151219010353088}, {'dog': 0.38487812876701355}]\u200b\n   resultado = {d['label']: d['score'] for d in etiquetas} \n   return resultado\u200b\n\ndemo = gr.Interface(fn=image_classifier, inputs=gr.Image(type=\"filepath\"), outputs='label')\u200b\n\ndemo.launch()\u200b\n</code></pre> \u200b</p> <p>\u200b</p>"},{"location":"hf/Referencias/","title":"\ud83d\udcce Referencias:","text":"<ul> <li>Tasks de HuggingFace</li> <li>Apuntes de HuggingFace elaborados por Aitor Medrano</li> </ul>"},{"location":"hf/Tasks_texts/","title":"Tasks de Hugging face","text":""},{"location":"hf/Tasks_texts/#objetivos","title":"Objetivos","text":"<ul> <li>Diferenciar qu\u00e9 es un \"task\" en Machine Learning seg\u00fan Hugging Face.</li> <li>Aprender los conceptos y ejemplos de estimaci\u00f3n de profundidad, clasificaci\u00f3n y segmentaci\u00f3n de im\u00e1genes.</li> <li>Probar ejemplos pr\u00e1cticos con pipelines de Hugging Face.</li> </ul> <p>Hugging Face es el portal para todas las tareas de aprendizaje autom\u00e1tico. Aqu\u00ed encontraremos todo lo necesario para empezar con una tarea: demostraciones, casos de uso, modelos, conjuntos de datos y mucho m\u00e1s.</p>"},{"location":"hf/Tasks_texts/#que-es-un-task","title":"\u00bfQu\u00e9 es un task?","text":"<p>Un \"task\" en Hugging Face describe el tipo de problema que un modelo puede resolver. Permite buscar, probar y reutilizar modelos seg\u00fan la tarea (task) deseada.</p> <p> Tasks (tareas) en Hugging Face</p>"},{"location":"hf/Tasks_texts/#uso-de-hugging-face-para-tareas-de-vision-por-computadora","title":"Uso de Hugging Face para tareas de visi\u00f3n por computadora","text":"<p>Hugging Face tambi\u00e9n proporciona una amplia colecci\u00f3n de modelos preentrenados para tareas de visi\u00f3n artificial. Con todos estos modelos alojados previamente entrenados, podemos crear aplicaciones interesantes que detectan objetos en im\u00e1genes, la edad de una persona y m\u00e1s. En este tema, aprenderemos a realizar las primeras cuatro tareas utilizando modelos de Hugging Face. </p>"},{"location":"hf/Tasks_texts/#1-clasificacion-de-imagenes-image-classification","title":"1. Clasificaci\u00f3n de Im\u00e1genes (Image Classification)","text":"<p>La clasificaci\u00f3n de im\u00e1genes es una tarea de visi\u00f3n artificial que implica categorizar o etiquetar una imagen en una o varias clases o categor\u00edas predefinidas. El objetivo de la clasificaci\u00f3n de im\u00e1genes es reconocer y asignar la etiqueta m\u00e1s adecuada a una imagen determinada en funci\u00f3n de su contenido. </p> <p></p>"},{"location":"hf/Tasks_texts/#ejemplos-de-aplicaciones","title":"Ejemplos de aplicaciones:","text":"<ul> <li>Diagn\u00f3stico m\u00e9dico (clasificar radiograf\u00edas)</li> <li>Reconocimiento de objetos</li> <li>Clasificaci\u00f3n de productos en e-commerce</li> <li>Moderaci\u00f3n de contenido visual</li> </ul>"},{"location":"hf/Tasks_texts/#modelos-disponibles-en-hugging-face","title":"Modelos disponibles en Hugging Face","text":"<p>Hugging Face ofrece m\u00faltiples modelos preentrenados para clasificaci\u00f3n de im\u00e1genes. Algunos destacados:</p> Modelo Arquitectura Dataset de entrenamiento Enlace <code>google/vit-base-patch16-224</code> Vision Transformer (ViT) ImageNet \ud83d\udd17 Ver modelo <code>microsoft/resnet-50</code> ResNet-50 ImageNet \ud83d\udd17 Ver modelo <code>facebook/deit-base-patch16-224</code> DeiT ImageNet \ud83d\udd17 Ver modelo"},{"location":"hf/Tasks_texts/#quick-draw-de-google","title":"\"Quick, Draw!\" de Google","text":"<p>Este juego se ha creado con aprendizaje autom\u00e1tico. Cuando dibujas algo, una red neuronal intenta adivinar qu\u00e9 est\u00e1s dibujando. Evidentemente, no siempre funciona; pero cuanto m\u00e1s juegues, m\u00e1s aprender\u00e1. Ya reconoce cientos de conceptos y esperamos poder a\u00f1adir m\u00e1s en el futuro. Nuestro objetivo es mostrar un ejemplo de c\u00f3mo se puede usar el aprendizaje autom\u00e1tico de forma divertida. Mira el siguiente v\u00eddeo para saber c\u00f3mo funciona y</p> <p>Caracter\u00edsticas clave</p> <ul> <li> <p>Juego con IA: El juego es un experimento de aprendizaje autom\u00e1tico. El jugador dibuja y la red neuronal intenta adivinar el dibujo en tiempo real.</p> </li> <li> <p>Aprendizaje continuo: La IA aprende de cada dibujo, mejorando su capacidad para adivinar correctamente en el futuro. Esto ayuda a Google a recopilar uno de los conjuntos de datos de garabatos m\u00e1s grandes del mundo para la investigaci\u00f3n en aprendizaje autom\u00e1tico.</p> </li> <li> <p>Mec\u00e1nica simple: El juego es similar al Pictionary. Consiste en seis rondas, y en cada una se nos pide dibujar un objeto diferente en 20 segundos. Al final, podemos ver nuestros dibujos y los resultados.</p> </li> <li> <p>Accesibilidad: El juego es gratuito y se puede jugar directamente en el navegador web desde cualquier dispositivo (smartphone, tablet, ordenador, etc.). </p> </li> </ul> <p>Podemos acceder al juego en el sitio web oficial: Web oficial. </p> <p>Importancia de los datos - BigData</p> <p>Los datos recopilados en el juego \"Quick, Draw!\" son fundamentales en el \u00e1mbito del Big Data y el aprendizaje autom\u00e1tico porque conforman el conjunto de datos de garabatos m\u00e1s grande del mundo, esencial para entrenar y mejorar los modelos de inteligencia artificial de Google.  Su importancia radica en varios puntos clave:</p> <ul> <li> <p>Entrenamiento de IA: Los millones de dibujos (actualmente m\u00e1s de 50 millones en 345 categor\u00edas) sirven como un vasto corpus de datos para entrenar redes neuronales, ense\u00f1\u00e1ndoles a reconocer e interpretar garabatos de formas muy diversas. La IA aprende a identificar patrones visuales, sin importar el estilo individual del dibujante.</p> </li> <li> <p>Diversidad y variabilidad: A diferencia de conjuntos de datos de im\u00e1genes tradicionales, los garabatos muestran una enorme variabilidad en c\u00f3mo las personas de diferentes culturas y con distintas habilidades dibujan un mismo objeto. Esta diversidad es crucial para crear modelos de IA m\u00e1s robustos y menos sesgados que puedan funcionar globalmente.</p> </li> <li> <p>Datos en tiempo real y secuenciales: Los dibujos se capturan como series temporales de posiciones del l\u00e1piz (vectores con marca de tiempo), no solo como im\u00e1genes est\u00e1ticas. Esto permite a los investigadores comprender no solo el resultado final, sino tambi\u00e9n el proceso de dibujo (qu\u00e9 trazo se hizo primero, en qu\u00e9 direcci\u00f3n), lo cual es valioso para desarrollar modelos de IA m\u00e1s avanzados, como el modelo Sketch-RNN.</p> </li> <li> <p>Investigaci\u00f3n abierta: Google ha hecho p\u00fablico este conjunto de datos para que investigadores de todo el mundo puedan utilizarlo en sus propios proyectos y estudios de aprendizaje autom\u00e1tico, fomentando la innovaci\u00f3n en el campo.</p> </li> <li> <p>Ejemplo de gamificaci\u00f3n para la recolecci\u00f3n de datos: El juego es un excelente ejemplo de c\u00f3mo la gamificaci\u00f3n puede motivar a un gran n\u00famero de usuarios a generar datos valiosos de forma divertida y a gran escala, un desaf\u00edo com\u00fan en el Big Data</p> </li> </ul> <p>Datos de entrenamiento</p> <p></p> <p>En esta p\u00e1gina podemos ver, en el momento en el que se redactaban estos apuntes, 126.372 dibujos de pelotas de baloncesto hechas por personales reales...en Internet. Incluso, podemos ver los trazos que han realizado estas personas hasta que el modelo ha sido capaz de adivinar el dibujo.  Destacar la importancia del Big Data, ya que, los datos de entrenamiento son muy importantes para cualquier modelo de aprendizaje. </p> <p>Datos de entrenamiento para la pelota de baloncesto</p> <p></p>"},{"location":"hf/Tasks_texts/#desarrollo-de-nuestro-propio-pictionary-con-gradio","title":"Desarrollo de nuestro propio Pictionary con Gradio","text":"<p>Vamos a crear una aplicaci\u00f3n web con Gradio que use el modelo creado en una sesi\u00f3n anterior: \u200bomarques/autotrain-dogs-and-cats-1527055142</p> <p>Ejemplo de aplicaci\u00f3n Gradio con una imagen de entrada y un Label como componente de salida: </p> <p>Etiquetado de la imagen de entrada: </p>"},{"location":"hf/Tasks_texts/#2-estimacion-de-profundidad-depth-estimation","title":"2. Estimaci\u00f3n de Profundidad (Depth Estimation)","text":"<ul> <li>Definici\u00f3n: Predice la distancia de cada p\u00edxel respecto a la c\u00e1mara usando solo una imagen.</li> <li>Aplicaciones: Rob\u00f3tica, realidad aumentada, veh\u00edculos aut\u00f3nomos, etc.</li> <li>Modelos populares: DPT, MiDaS</li> </ul> <pre><code># Utiliza el pipeline:\n\nfrom transformers import pipeline\n\ndepth = pipeline(\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n\nresult = depth(\"ruta_o_url_imagen\")\n</code></pre>"},{"location":"hf/Tasks_vc/","title":"Tasks de Hugging face relacionadas con la Visi\u00f3n por computador","text":""},{"location":"hf/Tasks_vc/#objetivos","title":"Objetivos","text":"<ul> <li>Diferenciar qu\u00e9 es un \"task\" en Machine Learning seg\u00fan Hugging Face.</li> <li>Aprender los conceptos y ejemplos de estimaci\u00f3n de profundidad, clasificaci\u00f3n y segmentaci\u00f3n de im\u00e1genes.</li> <li>Probar ejemplos pr\u00e1cticos con pipelines de Hugging Face.</li> </ul> <p>Hugging Face es el portal para todas las tareas de aprendizaje autom\u00e1tico. Aqu\u00ed encontraremos todo lo necesario para empezar con una tarea: demostraciones, casos de uso, modelos, conjuntos de datos y mucho m\u00e1s.</p>"},{"location":"hf/Tasks_vc/#que-es-un-task","title":"\u00bfQu\u00e9 es un task?","text":"<p>Un task en Hugging Face define el tipo de problema que un modelo est\u00e1 dise\u00f1ado para resolver. Esta clasificaci\u00f3n facilita la b\u00fasqueda, prueba y reutilizaci\u00f3n de modelos seg\u00fan la tarea espec\u00edfica que se desea abordar. Tasks (tareas) en Hugging Face </p>"},{"location":"hf/Tasks_vc/#uso-de-hugging-face-para-tareas-de-vision-por-computadora","title":"Uso de Hugging Face para tareas de Visi\u00f3n por Computadora","text":"<p>Hugging Face tambi\u00e9n proporciona una amplia colecci\u00f3n de modelos preentrenados para tareas de visi\u00f3n artificial. Con todos estos modelos alojados previamente entrenados, podemos crear aplicaciones interesantes que detectan objetos en im\u00e1genes, la edad de una persona y m\u00e1s. En este tema, aprenderemos a realizar las primeras cuatro tareas utilizando modelos de Hugging Face. </p>"},{"location":"hf/Tasks_vc/#1-clasificacion-de-imagenes-image-classification","title":"1. Clasificaci\u00f3n de Im\u00e1genes (Image Classification)","text":"<p>La clasificaci\u00f3n de im\u00e1genes es una tarea de visi\u00f3n por computador que consiste en asignar una o varias etiquetas predefinidas a una imagen, seg\u00fan su contenido. </p>"},{"location":"hf/Tasks_vc/#ejemplos-de-aplicaciones","title":"Ejemplos de aplicaciones","text":"<ul> <li>Diagn\u00f3stico m\u00e9dico: clasificaci\u00f3n de radiograf\u00edas para detectar enfermedades.</li> <li>Reconocimiento de objetos</li> <li>Clasificaci\u00f3n de productos en e-commerce</li> <li>Moderaci\u00f3n de contenido visual</li> </ul>"},{"location":"hf/Tasks_vc/#modelos-disponibles-en-hugging-face","title":"Modelos disponibles en Hugging Face","text":"<p>Hugging Face ofrece m\u00faltiples modelos preentrenados para clasificaci\u00f3n de im\u00e1genes. Estos modelos han sido entrenados con grandes conjuntos de datos, como ImageNet, lo que les permite reconocer una amplia variedad de objetos y escenas. Algunos destacados:</p> Modelo Arquitectura Dataset de entrenamiento Enlace <code>google/vit-base-patch16-224</code> Vision Transformer (ViT) ImageNet \ud83d\udd17 Ver modelo <code>microsoft/resnet-50</code> ResNet-50 ImageNet \ud83d\udd17 Ver modelo <code>facebook/deit-base-patch16-224</code> DeiT ImageNet \ud83d\udd17 Ver modelo"},{"location":"hf/Tasks_vc/#quick-draw-de-google","title":"\"Quick, Draw!\" de Google","text":"<p>Quick, Draw! es un juego basado en aprendizaje autom\u00e1tico en el que una red neuronal intenta adivinar el objeto que el usuario est\u00e1 dibujando. Evidentemente, no siempre funciona; pero cuanto m\u00e1s tiempo pasemos jugando, m\u00e1s aprender\u00e1. Destacar que ya reconoce cientos de conceptos y esperan poder a\u00f1adir m\u00e1s en el futuro. El gran objetivo de esta aplicaci\u00f3n, es mostrar un ejemplo de c\u00f3mo se puede usar el aprendizaje autom\u00e1tico de forma divertida. </p> <p>Caracter\u00edsticas clave</p> <ul> <li> <p>Juego con IA: El juego es un experimento de aprendizaje autom\u00e1tico. El jugador dibuja y la red neuronal intenta adivinar el dibujo en tiempo real.</p> </li> <li> <p>Aprendizaje continuo: La IA aprende de cada dibujo, mejorando su capacidad para adivinar correctamente en el futuro. Esto ayuda a Google a recopilar uno de los conjuntos de datos de garabatos m\u00e1s grandes del mundo para la investigaci\u00f3n en aprendizaje autom\u00e1tico.</p> </li> <li> <p>Mec\u00e1nica simple: El juego es similar al Pictionary. Consiste en seis rondas, y en cada una se nos pide dibujar un objeto diferente en 20 segundos. Al final, podemos ver nuestros dibujos y los resultados.</p> </li> </ul> <p>Podemos acceder al juego en el sitio web oficial: Web oficial. </p> <p>Importancia de los datos - BigData</p> <p>Los datos recopilados en Quick, Draw! son fundamentales para el Big Data y el aprendizaje autom\u00e1tico, ya que constituyen el conjunto de datos de garabatos m\u00e1s grande del mundo, esencial para entrenar y mejorar modelos de IA.  Su importancia radica en varios puntos clave:</p> <ul> <li> <p>Entrenamiento de IA: Los millones de dibujos (actualmente m\u00e1s de 50 millones en 345 categor\u00edas) sirven como un vasto corpus de datos para entrenar redes neuronales, ense\u00f1\u00e1ndoles a reconocer e interpretar garabatos de formas muy diversas. La IA aprende a identificar patrones visuales, sin importar el estilo individual del dibujante.</p> </li> <li> <p>Diversidad y variabilidad: A diferencia de conjuntos de datos de im\u00e1genes tradicionales, los garabatos muestran una enorme variabilidad en c\u00f3mo las personas de diferentes culturas y con distintas habilidades dibujan un mismo objeto. Esta diversidad es crucial para crear modelos de IA m\u00e1s robustos y menos sesgados que puedan funcionar globalmente.</p> </li> <li> <p>Datos en tiempo real y secuenciales: Los dibujos se capturan como series temporales de posiciones del l\u00e1piz (vectores con marca de tiempo), no solo como im\u00e1genes est\u00e1ticas. Esto permite a los investigadores comprender no solo el resultado final, sino tambi\u00e9n el proceso de dibujo (qu\u00e9 trazo se hizo primero, en qu\u00e9 direcci\u00f3n), lo cual es valioso para desarrollar modelos de IA m\u00e1s avanzados, como el modelo Sketch-RNN (Recurrent Neural Network para Bocetos es un modelo generativo de aprendizaje autom\u00e1tico desarrollado por David Ha y Douglas Eck en Google Brain, que es capaz de crear, completar y manipular bocetos vectoriales de objetos comunes)</p> </li> <li> <p>Investigaci\u00f3n abierta: Google ha hecho p\u00fablico este conjunto de datos para que investigadores de todo el mundo puedan utilizarlo en sus propios proyectos y estudios de aprendizaje autom\u00e1tico, fomentando la innovaci\u00f3n en el campo.</p> </li> <li> <p>Ejemplo de gamificaci\u00f3n para la recolecci\u00f3n de datos: El juego es un excelente ejemplo de c\u00f3mo la gamificaci\u00f3n puede motivar a un gran n\u00famero de usuarios a generar datos valiosos de forma divertida y a gran escala, un desaf\u00edo com\u00fan en el Big Data.</p> </li> </ul> <p>Datos de entrenamiento</p> <p></p> <p>En esta p\u00e1gina podemos ver, en el momento en el que se redactaban estos apuntes, 126.372 dibujos de pelotas de baloncesto hechas por personales reales...en Internet. Incluso, podemos ver los trazos que han realizado estas personas hasta que el modelo ha sido capaz de adivinar el dibujo.  Destacar la importancia del Big Data, ya que, los datos de entrenamiento son muy importantes para cualquier modelo de aprendizaje. </p> <p>Datos de entrenamiento para la pelota de baloncesto</p> <p></p>"},{"location":"hf/Tasks_vc/#desarrollo-de-nuestro-propio-pictionary-con-gradio","title":"Desarrollo de nuestro propio Pictionary con Gradio","text":"<p>Vamos a desarrollar nuestra propia aplicaci\u00f3n Pictionary utilizando Gradio, basada en el siguiente v\u00eddeo: https://www.youtube.com/watch?v=LS9Y2wDVI0k</p> <p>Todos los ficheros se encuentran en el siguiente espacio de Hugging Face: https://huggingface.co/spaces/nateraw/quickdraw</p> <p>Lo primero que debemos es, descargar los ficheros siguientes: <code>class_names.txt</code>, <code>pytorch_model.bin</code> y <code>app.py</code></p> <p>Analizamos el c\u00f3digo elaborado por el usuario:</p> <pre><code>from pathlib import Path  \nimport torch             \n\nimport gradio as gr       \nfrom torch import nn      \n\n# Lee las etiquetas/clases del archivo de texto, una por l\u00ednea. \n# Cada l\u00ednea es una categor\u00eda que el modelo puede predecir.\nLABELS = Path('class_names.txt').read_text().splitlines()\n\n# Definimos la arquitectura de la red neuronal convolucional (CNN) ya entrenada:\nmodel = nn.Sequential(\n    # Primera capa: 1 canal de entrada, 32 filtros, tama\u00f1o de filtro 3x3\n    nn.Conv2d(1, 32, 3, padding='same'),  \n    # Funci\u00f3n de activaci\u00f3n no lineal ReLU (acelera y facilita el aprendizaje)\n    nn.ReLU(),                            \n    # Max Pooling: reduce la resoluci\u00f3n espacial de las caracter\u00edsticas\n    #  (comprime la imagen a la vez que mantiene zonas m\u00e1s \u201cactivas\u201d)\n    nn.MaxPool2d(2),                      \n    nn.Conv2d(32, 64, 3, padding='same'), # Segunda capa: 32\u219264 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),# Tercera capa: 64\u2192128 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    # Aplana los datos resultantes para prepararlos para las capas densas \n    # (total elementos = 128 canales * 3 * 3)\n    nn.Flatten(),                         \n    # Capa totalmente conectada: de 1152 (productos anteriores) a 256 neuronas\n    nn.Linear(1152, 256),                 \n    nn.ReLU(),\n    # Capa de salida: 1 neurona por clase del archivo de etiquetas\n    nn.Linear(256, len(LABELS)),          \n)\n# Carga los pesos entrenados previamente desde \n# el archivo binario (estado del modelo)\nstate_dict = torch.load('pytorch_model.bin', map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\n# Coloca el modelo en modo \"solo inferencia\" \n# (no entrenamiento): no calcula gradientes ni actualiza pesos\nmodel.eval() \n\n# Funci\u00f3n de predicci\u00f3n principal: toma una imagen (array) \n# y devuelve las top-5 categor\u00edas con su probabilidad\ndef predict(im):\n    # Convierte el array de la imagen en un tensor, escala los valores a rango [0,1] \n    # y a\u00f1ade dimensiones de batch y canal\n    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.\n\n    # Desactiva el c\u00e1lculo de gradientes (m\u00e1s r\u00e1pido, no entrena)\n    with torch.no_grad():            \n        # Hacemos pasar la imagen por el modelo (forward pass)\n        out = model(x)               \n\n    # Calcula las probabilidades (softmax)\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)  \n\n    # Obtiene las 5 clases m\u00e1s probables\n    values, indices = torch.topk(probabilities, 5)              \n\n    # Devuelve un diccionario {clase: probabilidad} para las 5 mejores\n    return {LABELS[i]: v.item() for i, v in zip(indices, values)}\n\n# Creamos la interfaz web con Gradio:\n#   - predict: funci\u00f3n a ejecutar al recibir la entrada.\n#   - inputs: 'sketchpad', una zona para que el usuario dibuje a mano alzada.\n#   - outputs: 'label', salida tipo clasificaci\u00f3n de etiquetas.\n#   - live=True: muestra predicciones en tiempo real mientras dibujas.\ninterface = gr.Interface(predict, inputs='sketchpad', outputs='label', live=True)\n\ninterface.launch(debug=True)\n</code></pre>"},{"location":"hf/Tasks_vc/#que-es-una-red-neuronal-convolucional-cnn","title":"\u00bfQu\u00e9 es una red neuronal convolucional (CNN)?","text":"<p>Una red neuronal convolucional (CNN, por sus siglas en ingl\u00e9s, Convolutional Neural Network) es un tipo de red neuronal artificial especialmente dise\u00f1ada para procesar datos que tienen una estructura en forma de cuadr\u00edcula, como im\u00e1genes, audio o v\u00eddeo.</p>"},{"location":"hf/Tasks_vc/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ul> <li> <p>Inspiraci\u00f3n biol\u00f3gica:: Las CNNs se inspiran en la corteza visual de los mam\u00edferos. Primero detectan reglas simples (l\u00edneas, bordes) y despu\u00e9s patrones m\u00e1s complejos (formas, objetos).</p> </li> <li> <p>Arquitectura en capas:   Una CNN est\u00e1 compuesta por diferentes capas conectadas:</p> <ul> <li>Capas convolucionales: Aplican filtros o \u201ckernels\u201d para extraer patrones y caracter\u00edsticas locales (bordes, texturas, esquinas).</li> <li>Capas de activaci\u00f3n (ReLU): Introducen no linealidad, permitiendo que la red aprenda funciones m\u00e1s complejas.</li> <li>Capas de agrupamiento (pooling): Reducen la resoluci\u00f3n espacial y la cantidad de computaci\u00f3n, logrando robustez ante desplazamientos.</li> <li>Capas totalmente conectadas: Integran toda la informaci\u00f3n para tomar decisiones y realizar la predicci\u00f3n final.</li> </ul> </li> <li> <p>Aprendizaje jer\u00e1rquico:   Las CNNs aprenden jerarqu\u00edas de caracter\u00edsticas: Las primeras capas detectan elementos simples, las siguientes combinan estos elementos y las \u00faltimas reconocen patrones complejos y abstractos.</p> </li> <li> <p>Campos receptivos y par\u00e1metros compartidos: Los filtros se aplican en toda la imagen usando los mismos par\u00e1metros, lo que permite detectar el mismo patr\u00f3n en distintas posiciones. As\u00ed, el n\u00famero de par\u00e1metros y el coste de memoria disminuyen en comparaci\u00f3n con una red completamente conectada.</p> </li> </ul>"},{"location":"hf/Tasks_vc/#aplicaciones-tipicas","title":"Aplicaciones t\u00edpicas","text":"<ul> <li>Reconocimiento y clasificaci\u00f3n de im\u00e1genes: Detecci\u00f3n de objetos, diagn\u00f3stico m\u00e9dico, moderaci\u00f3n de contenido, etc.</li> <li>Visi\u00f3n por computador: Conducci\u00f3n aut\u00f3noma, videovigilancia, an\u00e1lisis de tr\u00e1fico.</li> <li>Procesamiento de v\u00eddeo: Reconocimiento de acciones, seguimiento de objetos en secuencias de im\u00e1genes, an\u00e1lisis deportivo.</li> </ul>"},{"location":"hf/Tasks_vc/#ejemplo-didactico-sencillo","title":"Ejemplo did\u00e1ctico sencillo","text":"<p>Cuando pasas una imagen por una CNN:</p> <ul> <li>Las primeras capas detectan bordes y formas sencillas.</li> <li>Las siguientes detectan partes m\u00e1s grandes (ruedas, patas, ojos).</li> <li>Al final, la red puede identificar el objeto completo (ej. \u201cbicicleta\u201d, \u201cgato\u201d, \u201cpersona\u201d) en la imagen.</li> </ul> <p>Como hemos comprobado en el ejemplo, el c\u00f3digo desarrollado por el usuario no funciona actualmente, por lo que debemos realizar algunas mejoras para que el c\u00f3digo original funcione. A continuaci\u00f3n podemos visualizar la soluci\u00f3n final:</p> <pre><code>from pathlib import Path\nfrom PIL import Image\nfrom torch import nn\n\nimport torch\nimport gradio as gr\nimport numpy as np\n\n# Leemos las etiquetas de clases (categor\u00edas) desde un fichero de texto\nLABELS = Path('class_names.txt').read_text().splitlines()\n\n# Definimos la arquitectura de la red neuronal convolucional (CNN) ya entrenada:\nmodel = nn.Sequential(\n    # Primera capa: 1 canal de entrada, 32 filtros, tama\u00f1o de filtro 3x3\n    nn.Conv2d(1, 32, 3, padding='same'),  \n    # Funci\u00f3n de activaci\u00f3n no lineal ReLU (acelera y facilita el aprendizaje)\n    nn.ReLU(),                            \n    # Max Pooling: reduce la resoluci\u00f3n espacial de las caracter\u00edsticas \n    # (comprime la imagen a la vez que mantiene zonas m\u00e1s \u201cactivas\u201d)\n    nn.MaxPool2d(2),                      \n    nn.Conv2d(32, 64, 3, padding='same'), # Segunda capa: 32\u219264 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),# Tercera capa: 64\u2192128 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    # Aplana los datos resultantes para prepararlos para las capas\n    # densas (total elementos = 128 canales * 3 * 3)\n    nn.Flatten(),                         \n    # Capa totalmente conectada: de 1152 (productos anteriores) \n    # a 256 neuronas\n    nn.Linear(1152, 256),                 \n    nn.ReLU(),\n    # Capa de salida: 1 neurona por clase del archivo de etiquetas\n    nn.Linear(256, len(LABELS)),          \n)\n# Cargamos los pesos previamente entrenados del modelo\nstate_dict = torch.load('pytorch_model.bin', map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\n# Ponemos el modelo en modo inferencia (no entrenamiento)\nmodel.eval()  \n\n# Funci\u00f3n principal de predicci\u00f3n, procesar\u00e1 el dibujo \n# de Gradio y calcular\u00e1 su clase\ndef predict(img):   \n    # Si no hay dibujo o la clave 'composite' no existe o est\u00e1 vac\u00eda, avisamos:\n    if img is None or \"composite\" not in img or img[\"composite\"] is None:\n        return {\"Por favor, dibuja algo\": 1.0}\n    # Extraemos la imagen resultado del canvas, canal RGBA\n    # Array con forma (ej. [800, 800, 4]), tipo uint8\n    arr = img[\"composite\"]        \n    # Convertimos de RGBA a escala de grises (Quick Draw es gris)\n    arr_gray = arr[..., :3].mean(axis=2)\n    # Convertimos a uint8 por si PIL lo necesita\n    arr_gray_uint8 = arr_gray.astype(\"uint8\")\n    # Redimensionamos a 28x28 p\u00edxeles (tama\u00f1o de entrada del modelo)\n    arr_img = Image.fromarray(arr_gray_uint8)\n    arr_resized = np.array(arr_img.resize((28, 28), resample=Image.BILINEAR))\n    # Escalamos a rango [0,1]\n    arr_normalized = arr_resized / 255.0\n    # A\u00f1adimos dimensiones de batch y canal: (1, 1, 28, 28)\n    x = torch.tensor(arr_normalized, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n    # Ejecutamos inferencia sin calcular gradientes (m\u00e1s eficiente)\n    with torch.no_grad():\n        out = model(x)\n    # Calculamos probabilidades con softmax\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n    # Obtenemos las 5 clases m\u00e1s probables (top-5)\n    values, indices = torch.topk(probabilities, 5)\n    # Devolvemos un diccionario: categor\u00eda : probabilidad (~confianza)\n    return {LABELS[i]: v.item() for i, v in zip(indices, values)}\n\n# Creamos la interfaz Gradio:\n# - El input es un sketchpad (zona para dibujar)\n# - El output son etiquetas: las categor\u00edas predecidas\n# - live=True: actualiza la predicci\u00f3n en tiempo real al dibujar\ndemo = gr.Interface(\n    predict,     \n    inputs='sketchpad',\n    outputs='label', \n    live=True)\n\ndemo.launch(share=True)\n</code></pre> <p>La funci\u00f3n softmax de torch (PyTorch) es una operaci\u00f3n matem\u00e1tica que transforma un vector de valores reales \u2014normalmente llamados \"logits\"\u2014 en una distribuci\u00f3n de probabilidades sobre diferentes clases, donde todos los elementos resultantes est\u00e1n entre 0 y 1 y la suma es exactamente 1. Por ejemplo, si tu modelo clasifica im\u00e1genes en tres clases, la salida softmax ser\u00e1 un vector con tres valores que representan la probabilidad atribuida a cada clase.\u200b</p> <p>En PyTorch, podemos usar esta funci\u00f3n tanto como capa de activaci\u00f3n en la salida de nuestro modelo, como directamente llamando torch.nn.functional.softmax() sobre un tensor de logits. Es com\u00fan utilizar softmax en la inferencia para obtener probabilidades interpretables, mientras que durante el entrenamiento suele usarse CrossEntropyLoss, que incorpora la softmax de forma interna y m\u00e1s eficiente.\u200b</p> <p>Aplicaciones comunes:</p> <ul> <li>Clasificaci\u00f3n multiclase: transforma las salidas del modelo en probabilidades para cada categor\u00eda.\u200b</li> <li>Visualizaci\u00f3n de la confianza del modelo en cada posible resultado. En resumen, softmax convierte los resultados num\u00e9ricos en probabilidades \u00fatiles para tomar decisiones y analizar resultados en Deep learning.</li> </ul>"},{"location":"hf/Tasks_vc/#actividad-1-usar-un-space-de-hugging-face","title":"\ud83d\udcdd Actividad 1. Usar un Space de Hugging Face","text":"<p>Bas\u00e1ndote en lo aprendido a partir de los casos de uso de Hola Spaces y Hola Spaces 2.0 trabajadas en una sesi\u00f3n anterior, mediante Gradio en Hugging Face crea un nuevo espacio p\u00fablico en tu cuenta que permita probar la aplicaci\u00f3n del pictionary desarrollada de forma local en un Space de Hugging Face. </p> <p>Entrega la url del espacio y algunas capturas de pantalla usando la aplicaci\u00f3n. </p>"},{"location":"hf/Tasks_vc/#2-deteccion-de-objetos","title":"2. Detecci\u00f3n de objetos","text":"<p>La detecci\u00f3n de objetos predice la distancia de cada p\u00edxel respecto a la c\u00e1mara usando solo una imagen. Es una t\u00e9cnica fundamental en visi\u00f3n computacional que permite identificar y localizar instancias de objetos definidos dentro de im\u00e1genes. Es ampliamente utilizada en aplicaciones como conducci\u00f3n aut\u00f3noma, seguimiento de objetos en deportes, b\u00fasqueda de im\u00e1genes y conteo de objetos en diferentes escenarios. </p> <p>Hugging Face alberga varios modelos que han sido entrenados previamente para detectar objetos en im\u00e1genes. Podemos ver una lista de modelos en https://huggingface.co/models?pipeline_tag=object-detection&amp;sort=trending </p> <p>En la figura siguiente podemos visualizar un listado de la categor\u00eda Object Detection:</p> <p></p> <p>Ejemplo de uso del modelo facebook/detr-resnet-50 para la detecci\u00f3n de objetos:</p> <p></p> <p>Podemos probar el modelo directamente utilizando la API de inferencia alojada en Hugging Face. Para ello, usaremos una imagen de una oficina con algunas mujeres: </p> <p>Fuente: https://en.wikipedia.org/wiki/Office#/media/File:Good_Smile_Company_offices_ladies.jpg </p> <p>Al arrastrar y soltar la imagen en la secci\u00f3n \"Inference API\" alojada en la p\u00e1gina del modelo en Hugging Face, veremos la lista de objetos detectados, as\u00ed como sus probabilidades correspondientes: </p> <p>Al pasar el rat\u00f3n por encima del nombre de un objeto detectado, la imagen resalta el cuadro delimitador del objeto seleccionado.</p>"},{"location":"hf/Tasks_vc/#algunos-modelos-disponibles-en-hugging-face","title":"Algunos modelos disponibles en Hugging Face","text":"<p>Hugging Face ofrece modelos preentrenados que permiten realizar detecci\u00f3n de objetos sin necesidad de entrenamiento adicional.</p> Modelo Arquitectura Dataset Enlace <code>facebook/detr-resnet-50</code> DETR (DEtection TRansformer) COCO \ud83d\udd17 Ver modelo <code>hustvl/yolos-small</code> YOLOS (Vision Transformer) COCO \ud83d\udd17 Ver modelo"},{"location":"hf/Tasks_vc/#principales-aplicaciones","title":"Principales Aplicaciones","text":"<ul> <li>Conducci\u00f3n aut\u00f3noma: Los coches sin conductor usan la detecci\u00f3n de objetos para reconocer peatones, bicicletas, sem\u00e1foros y se\u00f1ales de tr\u00e1fico, ayudando a la toma de decisiones en tiempo real.</li> <li>Seguimiento en deportes: En partidos de f\u00fatbol o tenis se rastrea el bal\u00f3n o los jugadores para mejorar el arbitraje y el an\u00e1lisis estad\u00edstico.</li> <li>B\u00fasqueda de im\u00e1genes: Los tel\u00e9fonos inteligentes permiten buscar lugares u objetos directamente en internet mediante la detecci\u00f3n de entidades en fotos.</li> <li>Conteo de objetos: La detecci\u00f3n ayuda a contar existencias en almacenes, tiendas, o personas en eventos.</li> </ul>"},{"location":"hf/Tasks_vc/#metricas-de-evaluacion","title":"M\u00e9tricas de Evaluaci\u00f3n","text":"<ul> <li>Precisi\u00f3n media promedio (AP): \u00c1rea bajo la curva de precisi\u00f3n versus recall para cada clase.</li> <li>mAP (mean Average Precision): Promedio de AP en todas las clases.</li> <li>AP\u03b1: Precisi\u00f3n promedio seg\u00fan el umbral de IoU (por ejemplo, AP50 muestra AP cuando el IoU es &gt;0,5).</li> </ul>"},{"location":"hf/Tasks_vc/#ejemplo-de-uso-con-gradio","title":"Ejemplo de uso con Gradio","text":"<p>Vamos a crear una aplicaci\u00f3n web con Gradio que use un objeto pipeline del modelo <code>facebook/detr-resnet-50</code>.</p> <p>As\u00ed es como se carga:  <pre><code>from transformers import pipeline\n\ndetection = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n</code></pre> Una vez que hayamos creado el objeto tipo pipeline (detecci\u00f3n en este caso), podemos pasar directamente la imagen (en formato PIL) al pipeline y obtener el resultado: </p> <p><pre><code>results = detection(image)\nresults\n</code></pre> Debemos tener en cuenta que el objeto de tipo pipeline (detecci\u00f3n) tambi\u00e9n puede incluir una URL de una imagen, no solo un objeto de imagen tipo PIL. Es decir, tambi\u00e9n podemos llamar al objeto pipeline de la siguiente manera: </p> <p><pre><code>results = detection('http://bit.ly/46xv3sL')\n</code></pre> <pre><code># Si no funcionara, prueba a descargar el fichero y ejecutarlo de forma local:\nresults = detection('Good_Smile_Company_offices_ladies.jpg')\n</code></pre> Debemos instalar la librer\u00eda <code>timm</code> (PyTorch Image Models) para Python. Es una extensa colecci\u00f3n de modelos de visi\u00f3n por computadora de \u00faltima generaci\u00f3n (SOTA, por sus siglas en ingl\u00e9s). Est\u00e1 dise\u00f1ada para ser utilizada con el framework PyTorch y es muy apreciada en la comunidad de aprendizaje profundo por su flexibilidad y la gran cantidad de modelos preentrenados que ofrece. <pre><code>pip install timm\n</code></pre> El resultado impreso se ver\u00eda as\u00ed: <pre><code>[{'score': 0.9179903864860535,\n  'label': 'person',\n  'box': {'xmin': 549, 'ymin': 145, 'xmax': 564, 'ymax': 165}},\n {'score': 0.9960624575614929,\n  'label': 'tv',\n  'box': {'xmin': 317, 'ymin': 212, 'xmax': 416, 'ymax': 299}},\n {'score': 0.9425505995750427,\n  'label': 'chair',\n  'box': {'xmin': 508, 'ymin': 306, 'xmax': 661, 'ymax': 429}},\n {'score': 0.9753392338752747,\n  'label': 'person',\n  'box': {'xmin': 673, 'ymin': 135, 'xmax': 705, 'ymax': 174}},\n {'score': 0.962176501750946,\n  'label': 'person',\n  'box': {'xmin': 703, 'ymin': 115, 'xmax': 722, 'ymax': 140}},\n {'score': 0.9881888628005981,\n  'label': 'person',\n  'box': {'xmin': 454, 'ymin': 142, 'xmax': 497, 'ymax': 202}},\n {'score': 0.9871691465377808,\n  'label': 'keyboard',\n  'box': {'xmin': 344, 'ymin': 276, 'xmax': 445, 'ymax': 346}},\n {'score': 0.9371852874755859,\n  'label': 'tv',\n  'box': {'xmin': 309, 'ymin': 194, 'xmax': 374, 'ymax': 237}},\n {'score': 0.9975801706314087,\n  'label': 'person',\n  'box': {'xmin': 395, 'ymin': 152, 'xmax': 446, 'ymax': 216}},\n {'score': 0.9986708164215088,\n  'label': 'person',\n  'box': {'xmin': 237, 'ymin': 174, 'xmax': 308, 'ymax': 264}},\n {'score': 0.9173707365989685,\n  'label': 'person',\n  'box': {'xmin': 720, 'ymin': 112, 'xmax': 737, 'ymax': 131}},\n {'score': 0.9895991086959839,\n  'label': 'potted plant',\n  'box': {'xmin': 124, 'ymin': 211, 'xmax': 230, 'ymax': 330}},\n {'score': 0.9996592998504639,\n  'label': 'person',\n  'box': {'xmin': 369, 'ymin': 226, 'xmax': 535, 'ymax': 427}},\n {'score': 0.9821581840515137,\n  'label': 'tv',\n  'box': {'xmin': 491, 'ymin': 181, 'xmax': 530, 'ymax': 223}},\n {'score': 0.9970135688781738,\n  'label': 'person',\n  'box': {'xmin': 516, 'ymin': 177, 'xmax': 628, 'ymax': 318}}]\n</code></pre> El resultado es una lista de diccionarios para cada objeto detectado. Para dibujar la etiqueta y el cuadro delimitador de cada objeto, utilizaremos el siguiente fragmento de c\u00f3digo: </p> <p><pre><code>import random\nfrom PIL import Image, ImageDraw\nimport requests\nimport torch\n\ndraw = ImageDraw.Draw(image)\n\nfor object in results:\n    box = [i for i in object['box'].values()]\n    print(\n        f\"Detected {object['label']} with confidence \"\n        f\"{(object['score'] * 100):.2f}% at {box}\"\n    )\n\n    r = random.randint(0, 255)\n    g = random.randint(0, 255)\n    b = random.randint(0, 255)\n    color = (r, g, b)\n\n    draw.rectangle(box,\n                   outline=color,\n                   width=2)\n\n    draw.text((box[0], box[1]-10),\n              object['label'],\n              fill='white')\n\ndisplay(image)\n</code></pre> La imagen ser\u00eda id\u00e9ntica a la que se muestra anteriormente pero con los cuadrados correspondientes. Con el objeto pipeline, tambi\u00e9n podemos obtener una lista de etiquetas directamente mediante el atributo <code>model.config.id2label</code>:  <pre><code>detection.model.config.id2label\n</code></pre></p>"},{"location":"hf/Tasks_vc/#actividad-guiada","title":"Actividad guiada","text":"<p>Desarrollar con Gradio un template similar es este:   Resultado final: </p> <p>Define:</p> <ul> <li>Una funci\u00f3n llamada predict</li> <li>Interface Gradio que env\u00ede una imagen y muestre la imagen con los objetos detectados</li> </ul> <p>C\u00f3digo final en Gradio: <pre><code>import gradio as gr\nfrom PIL import Image, ImageDraw\nfrom transformers import pipeline\nimport random \n\ndef predict(image):    \n\n    detection = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n\n    results = detection(image)       \n\n    draw = ImageDraw.Draw(image) \n\n    for object in results: \n        box = [i for i in object['box'].values()] \n        print( \n            f\"Detected {object['label']} with confidence \"   \n            f\"{(object['score'] * 100):.2f}% at {box}\"   \n        ) \n\n        r = random.randint(0, 255) \n        g = random.randint(0, 255) \n        b = random.randint(0, 255) \n        color = (r, g, b) \n\n        #Dibuja un cuadro delimitador alrededor del objeto.\n        draw.rectangle(box,  \n                    outline=color,  \n                    width=2)  \n        #Muestra la etiqueta del objeto\n        draw.text((box[0], box[1]-10),  \n                object['label'],  \n                fill='white')  \n\n    return image\n\n\ndemo = gr.Interface(\n    predict, \n    inputs=gr.Image(type=\"pil\"), \n    outputs=\"image\")\n\ndemo.launch()\n</code></pre></p>"},{"location":"hf/Tasks_vc/#actividad-2-comparativa-practica-de-deteccion-de-objetos-con-hugging-face-y-ultralytics-yolo11","title":"\ud83d\udcdd Actividad 2: Comparativa pr\u00e1ctica de Detecci\u00f3n de Objetos con Hugging Face y Ultralytics YOLO11","text":""},{"location":"hf/Tasks_vc/#contexto","title":"Contexto","text":"<p>Hemos trabajado en clase con modelos de detecci\u00f3n de objetos, usando ejemplos como <code>facebook/detr-resnet-50</code>  en Hugging Face (ver ejemplo y recursos de clase). En esta actividad, ir\u00e1s un paso m\u00e1s all\u00e1 probando la herramienta Ultralytics YOLO11, consultando su documentaci\u00f3n oficial de integraci\u00f3n con Gradio.</p>"},{"location":"hf/Tasks_vc/#objetivos_1","title":"Objetivos","text":"<ul> <li>Investigar y comprender el funcionamiento de la familia YOLO (en especial YOLO11).</li> <li>Probar distintos c\u00f3digos y ejemplos reales usando YOLO11 y Gradio.</li> <li>Comparar los resultados con los de <code>facebook/detr-resnet-50</code> en velocidad, facilidad de uso y precisi\u00f3n.</li> <li>Reflexionar sobre ventajas e inconvenientes de cada enfoque en distintos escenarios reales.</li> </ul>"},{"location":"hf/Tasks_vc/#1-lectura-e-investigacion-inicial","title":"1. Lectura e investigaci\u00f3n inicial","text":"<ul> <li>Lee la documentaci\u00f3n de Ultralytics YOLO11 y familiar\u00edzate con su API y flujo de trabajo.</li> <li>Consulta y ejecuta el ejemplo de integraci\u00f3n con Gradio: docs oficiales.</li> </ul>"},{"location":"hf/Tasks_vc/#2-implementacion-y-pruebas","title":"2. Implementaci\u00f3n y pruebas","text":"<ul> <li>Ejecuta la demo b\u00e1sica de YOLO11+Gradio incluida en la documentaci\u00f3n.</li> <li>Realiza anotaciones sobre el input, formato de resultados y velocidad tras varias ejecuciones con im\u00e1genes reales o ejemplos propios.</li> </ul>"},{"location":"hf/Tasks_vc/#3-comparativa-objetiva-con-hugging-face","title":"3. Comparativa objetiva con Hugging Face","text":"<ul> <li>Utiliza el modelo <code>facebook/detr-resnet-50</code> desde Hugging Face (ya visto en clase) para detectar objetos en al menos dos im\u00e1genes iguales a las usadas en YOLO11.</li> <li>Rellena la tabla comparativa:</li> </ul> Imagen Modelo Objetos detectados Tiempo de inferencia Falsos positivos/negativos Facilidad de integraci\u00f3n Observaciones (insertar nombre) YOLO11 (insertar nombre) detr-resnet-50 (HF) <ul> <li>Comenta los resultados en t\u00e9rminos de:<ul> <li>Exactitud y n\u00famero/calidad de predicciones.</li> <li>Consumo de recursos y tiempo de ejecuci\u00f3n (compara si es posible en CPU y GPU).</li> <li>Facilidad de uso/grado de documentaci\u00f3n o n\u00famero de l\u00edneas de c\u00f3digo para uso en Gradio.</li> </ul> </li> </ul>"},{"location":"hf/Tasks_vc/#entrega","title":"Entrega","text":"<ul> <li>Un archivo <code>.py</code> con el c\u00f3digo empleado y comentarios.</li> <li>Las im\u00e1genes o capturas de pantalla de las pruebas realizadas.</li> <li>Rellena y agrega la tabla comparativa.</li> </ul>"},{"location":"hf/Tasks_vc/#3-segmentacion-de-imagenes-image-segmentation","title":"3. Segmentaci\u00f3n de im\u00e1genes (Image segmentation)","text":"<p>La segmentaci\u00f3n de im\u00e1genes es una t\u00e9cnica de visi\u00f3n por computador que divide una imagen en segmentos o regiones, cada una correspondiente a un objeto de inter\u00e9s. Con la segmentaci\u00f3n de im\u00e1genes, podemos analizar una imagen y extraer informaci\u00f3n valiosa de ella. </p> <p></p> <p>Algunos de sus usos son: </p> <ul> <li>Im\u00e1genes m\u00e9dicas: se utilizan para identificar y segmentar tumores en resonancias magn\u00e9ticas o tomograf\u00edas computarizadas </li> <li>Detecci\u00f3n y reconocimiento de objetos: al igual que la detecci\u00f3n de objetos que hemos visto anteriormente, tambi\u00e9n podemos utilizar la segmentaci\u00f3n de im\u00e1genes para identificar y localizar objetos en una imagen </li> <li>Procesamiento de documentos: se utiliza para segmentar regiones de texto en documentos escaneados </li> <li>Biometr\u00eda: se utiliza para identificar y localizar rostros en im\u00e1genes o fotogramas de v\u00eddeo </li> </ul> <p>Hugging Face incluye varios modelos de segmentaci\u00f3n de im\u00e1genes que podemos usar. Uno de ellos es el modelo SegFormer \"SegFormer model fine-tuned on ADE20k\", optimizado con ADE20k.</p> <p>La siguiente imagen muestra el modelo SegFormer fine-tuned (optimizado) por el modelo ADE20k en la web de Hugging Face: </p> <p>Para probar el modelo de segmentaci\u00f3n, usaremos una imagen del Taj Mahal. La arrastraremos y la soltaremos en la secci\u00f3n de \"Hosted inference API\" alojada en la p\u00e1gina de Hugging Face:</p> <p>Imagen del Taj Mahal  Fuente: https://mng.bz/5vzD</p> <p>Resultado de la segmentaci\u00f3n de im\u00e1genes utilizando una imagen del Taj Mahal: </p> <p>Como podemos ver en el resultado, el modelo puede detectar diferentes objetos en la imagen (edificios, cielos, \u00e1rboles, etc\u00e9tera) y resaltar los diversos segmentos en dicha imagen. De hecho, podemos pasar el rat\u00f3n sobre las diversas etiquetas segmentadas y la imagen resaltar\u00e1 dicha etiqueta seleccionada. </p>"},{"location":"hf/Tasks_vc/#uso-del-modelo-con-pipeline","title":"Uso del modelo con pipeline","text":"<p>Como es habitual, usaremos el modelo mediante programaci\u00f3n. Primero, cargamos el modelo y luego verificamos cu\u00e1ntos objetos puede detectar el modelo. La forma m\u00e1s f\u00e1cil de usar el modelo es usar un pipeline  de la librer\u00eda <code>transformers</code>:  <pre><code>from transformers import pipeline \n\nsegmentation = pipeline(\"image-segmentation\",  \n               model=\"nvidia/segformer-b0-finetuned-ade-512-512\") \n\nprint(segmentation.model.config.id2label)\n</code></pre> Estos son los primeros y \u00faltimos cinco objetos que puede detectar (el modelo puede detectar un total de 150 objetos):  <pre><code>{0: 'wall', \n 1: 'building', \n 2: 'sky', \n 3: 'floor', \n 4: 'tree', \n ... \n 145: 'shower', \n 146: 'radiator', \n 147: 'glass', \n 148: 'clock', \n 149: 'flag'} \n</code></pre> Para este ejemplo, usaremos una imagen donde vemos a un hombre y a un avi\u00f3n que vuela por encima, para as\u00ed descubrir los distintos segmentos de dicha imagen: </p> <p> </p> <p>Fuente: https://unsplash.com/photos/EC_GhFRGTAY</p> <p>Para detectar los distintos segmentos de la imagen, pasamos la direcci\u00f3n URL de una imagen al objeto pipeline:  <pre><code>from transformers import pipeline \nfrom PIL import Image\nimport requests\n\nsegmentation = pipeline(\"image-segmentation\",  \n               model=\"nvidia/segformer-b0-finetuned-ade-512-512\") \n\nurl = 'https://bit.ly/46iDeJQ'\nresults = segmentation(url)\nprint(results)\n</code></pre> La salida de la variable results es una lista de diccionarios que contiene detalles de cada uno de los segmentos detectados en la imagen:  <pre><code>[{'score': None,\n  'label': 'wall',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'building',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'sky',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'person',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'airplane',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;}]\n</code></pre> En particular, el elemento mask contiene la m\u00e1scara del segmento detectado. Para ver cada una de las m\u00e1scaras detectadas, recorremos la variable results: </p> <p><pre><code>for result in results:\n    print(result['label'])\n    result['mask'].show()\n</code></pre> Por pantalla visualizamos las etiquetas encontradas: <pre><code>wall\nbuilding\nsky\nperson\nairplane\n</code></pre> La figura siguiente muestra las m\u00e1scaras detectadas para person (persona) y airplane (avi\u00f3n): </p> <p>La parte blanca de la m\u00e1scara representa la parte de la imagen que contiene el segmento de inter\u00e9s. Podemos aplicar la m\u00e1scara sobre la imagen original mediante el siguiente fragmento de c\u00f3digo: </p> <p><pre><code>image = Image.open(requests.get(url, stream=True).raw) \n\nfor result in results: \n    base_image = image.copy() \n    mask_image = result['mask'] \n\n    # Aplica la m\u00e1scara sobre la imagen original\n    base_image.paste(mask_image, mask=mask_image) \n    #Imprime la etiqueta del segmento\n    print(result['label']) \n    result['mask'].show()\n</code></pre> La figura siguiente muestra las m\u00e1scaras de person (persona) y airplane (avi\u00f3n) aplicadas sobre la imagen original: </p> <p>Cuando aplicamos la m\u00e1scara sobre la imagen, observaremos que el segmento de inter\u00e9s est\u00e1 en blanco. Ser\u00eda m\u00e1s natural invertir esto, es decir, el segmento de inter\u00e9s deber\u00eda mostrarse mientras que el resto deber\u00eda estar en blanco. Para hacer esto, podemos invertir la m\u00e1scara usando la funci\u00f3n <code>invert()</code> de la clase <code>ImageOps</code> en el paquete <code>PIL</code>. Los siguientes cambios invierten la m\u00e1scara y, a continuaci\u00f3n, la aplican sobre la imagen original: </p> <p><pre><code>from PIL import ImageOps \n\nfor result in results: \n    base_image = image.copy() \n    mask_image = result['mask'] \n\n    mask_image = ImageOps.invert(mask_image)  #Invierte la m\u00e1scara \n    base_image.paste(mask_image, mask=mask_image)  #Aplica la m\u00e1scara sobre la imagen original \n    print(result['label'])  #Imprime la etiqueta del segmento\n    display(base_image) \n</code></pre> La figura siguiente muestra las m\u00e1scaras invertidas para person (persona) y airplane (avi\u00f3n)aplicadas en la imagen original. </p> <p></p>"},{"location":"hf/Tasks_vc/#actividad-3-gradio-para-segmentacion-de-imagenes","title":"\ud83d\udcdd Actividad 3: Gradio para segmentaci\u00f3n de im\u00e1genes","text":"<p>Crea un prototipo mediante Gradio haciendo uso de la clase Interface que te permita probar el modelo de segmentaci\u00f3n bas\u00e1ndote en el siguiente prototipo:  </p> <p>Pasamos una foto y en el campo Label especificamos el objeto a buscar, por ejemplo, person:  </p> <p>Resultados:  Detecci\u00f3n de person </p> <p>Detecci\u00f3n de airplane  Pasos:  </p> <ul> <li>Definir la interfaz de Gradio con los componentes de entrada y de salida similares al prototipo de la imagen </li> <li>Crea una funci\u00f3n \u201csegmentation\u201d que reciba los par\u00e1metros de entrada correspondientes. Dentro de dicha funci\u00f3n </li> <li>Cuando el modelo devuelva el resultado, iterar\u00e1 a trav\u00e9s del resultado y buscar\u00e1 la etiqueta especificada por el usuario (en el par\u00e1metro label). </li> <li>A continuaci\u00f3n, la funci\u00f3n invierte la m\u00e1scara correspondiente, la aplica a la imagen y la devuelve autom\u00e1ticamente.  <p>NOTA: Dentro de la funci\u00f3n deber\u00edas de imprimir los labels que te devuelve el modelo para saber   </p> </li> </ul> <p>Realiza algunas pruebas con im\u00e1genes diferentes y adjunta en este documento los resultados. </p> <p>Entrega el fichero py y las im\u00e1genes que hayas utilizado. </p>"},{"location":"hf/Tasks_vc_profundidad/","title":"POR HACER","text":""},{"location":"hf/Tasks_vc_profundidad/#4-estimacion-de-profundidad-depth-estimation","title":"4. Estimaci\u00f3n de Profundidad (Depth Estimation)","text":"<ul> <li>Definici\u00f3n: Predice la distancia de cada p\u00edxel respecto a la c\u00e1mara usando solo una imagen.</li> <li>Aplicaciones: Rob\u00f3tica, realidad aumentada, veh\u00edculos aut\u00f3nomos, etc.</li> <li>Modelos populares: DPT, MiDaS</li> </ul> <pre><code># Utiliza el pipeline:\n\nfrom transformers import pipeline\n\ndepth = pipeline(\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n\nresult = depth(\"ruta_o_url_imagen\")\n</code></pre>"}]}