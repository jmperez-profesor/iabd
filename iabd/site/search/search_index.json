{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apuntes y ejercicios pr\u00e1cticas del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data","text":"<p>Apuntes, pr\u00e1cticas y ejercicios del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data en el IES Severo Ochoa (Elche).</p> <p>Los ejercicios y conocimientos contenidos en las pr\u00e1cticas y/o apuntes de  todos los m\u00f3dulos tienen exclusivamente prop\u00f3sito formativo, por lo que  nunca se deber\u00e1n utilizar con fines maliciosos o delictivos.</p> <p>Ning\u00fan alumno o alumna de este curso, ni profesor o profesora como  docentes, ser\u00e1n responsables de los da\u00f1os directos o indirectos que  pudieran derivarse del uso inadecuado de las herramientas y mecanismos  expuestos.</p>"},{"location":"Datasets/Actividad1/","title":"Actividad 1 - Datasets de Hugging Face","text":""},{"location":"Datasets/Actividad1/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"Datasets/Actividad1/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p>"},{"location":"Datasets/Actividad1/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"Datasets/Actividad1/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\ndataset = load_dataset(\"PlanTL-GOB-ES/squad-es\")\nprint(dataset)\n\n# Nivel 2: Dividir en train/test\ntrain_dataset = dataset[\"train\"]\nsplit_dataset = train_dataset.train_test_split(test_size=0.2)\ntrain_split = split_dataset[\"train\"]\ntest_split = split_dataset[\"test\"]\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\ndef add_num_paragraphs(example):\n    return {\"num_paragraphs\": len(example[\"paragraphs\"])}\ntrain_split = train_split.map(add_num_paragraphs)\n\n# Nivel 4: Filtrar y persistir\nfiltered_train = train_split.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nfiltered_train = filtered_train.remove_columns([\"num_paragraphs\"])\nfiltered_train.to_parquet(\"filtered_train.parquet\")\n\n# Nivel 5: Publicar en Hugging Face\n# filtered_train.push_to_hub(\"usuario/nombre-dataset\")\n</code></pre>"},{"location":"Datasets/Actividad1/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"Datasets/Actividad1_solution/","title":"1. Descargar los datos de SquadES desde fuente remota","text":"<p>El primer paso es cargar los archivos de entrenamiento y validaci\u00f3n desde URLs remotas. Utilizaremos la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Hugging Face Datasets, indicando que queremos cargar archivos JSON alojados en GitHub. Los datos remotos se corresponden con <code>train-v2.0-es.json</code> (entrenamiento) y <code>dev-v2.0-es.json</code> (validaci\u00f3n).</p> <p><pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\ndata_files = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\"\n}\nsquad_es = load_dataset(\"json\", data_files=data_files, field=\"data\")  # field=\"data\" porque los datos est\u00e1n bajo esa clave\nprint(squad_es)\n</code></pre> Esto produce un objeto DatasetDict con splits train y val, donde cada elemento tiene las claves title y paragraphs.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#2-dividir-los-datos-de-entrenamiento-en-entrenamiento-y-prueba","title":"2. Dividir los datos de entrenamiento en entrenamiento y prueba","text":"<p>Para realizar una partici\u00f3n del <code>split</code> de entrenamiento en dos partes (por ejemplo, 90% entrenamiento y 10% prueba), usamos el m\u00e9todo <code>train_test_split()</code>:</p> <p><pre><code>squad_train_full = squad_es[\"train\"]\nsplit_dataset = squad_train_full.train_test_split(test_size=0.1, seed=42)\nsquad_train = split_dataset[\"train\"]\nsquad_test = split_dataset[\"test\"]\nprint(squad_train)\nprint(squad_test)\n</code></pre> Ahora disponemos de <code>squad_train</code> (entrenamiento 90%) y <code>squad_test</code> (prueba 10%).\u200b</p>"},{"location":"Datasets/Actividad1_solution/#3-anadir-una-columna-con-el-numero-de-parrafos","title":"3. A\u00f1adir una columna con el n\u00famero de p\u00e1rrafos","text":"<p>Podemos emplear el m\u00e9todo <code>map()</code> para agregar una columna llamada, por ejemplo, <code>num_paragraphs</code>, contando los elementos en la clave <code>paragraphs</code> de cada ejemplo.</p> <p><pre><code>squad_train = squad_train.map(lambda x: {\"num_paragraphs\": len(x[\"paragraphs\"])})\nprint(squad_train.column_names)  # Debe incluir 'num_paragraphs'\nprint(squad_train[0][\"num_paragraphs\"])\n</code></pre> De este modo, cada registro en el <code>split</code> de entrenamiento tiene la columna con el n\u00famero de p\u00e1rrafos.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#4-filtrar-los-ejemplos-con-mas-de-10-parrafos","title":"4. Filtrar los ejemplos con m\u00e1s de 10 p\u00e1rrafos","text":"<p>Usaremos el m\u00e9todo <code>filter()</code>, pasando una funci\u00f3n lambda que conserve solo aquellos ejemplos cuya columna <code>num_paragraphs</code> sea mayor que 10:</p> <p><pre><code>squad_train_large = squad_train.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nprint(squad_train_large)\n</code></pre> As\u00ed, el dataset de entrenamiento contiene \u00fanicamente los registros relevantes para el criterio pedido.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#5-eliminar-la-columna-num_paragraphs","title":"5. Eliminar la columna num_paragraphs","text":"<p>Para dejar el dataset limpio, eliminamos la columna extra:</p> <p><pre><code>squad_train_final = squad_train_large.remove_columns(\"num_paragraphs\")\nprint(squad_train_final.column_names)\n</code></pre> Esto deja \u00fanicamente las columnas originales: <code>title</code> y <code>paragraphs</code>.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#6-persistir-el-dataset-en-formato-parquet","title":"6. Persistir el dataset en formato Parquet","text":"<p>El m\u00e9todo to_parquet() permite guardar el dataset resultante en disco en formato Parquet, que es eficiente y compatible para grandes vol\u00famenes de datos.</p> <p><pre><code>squad_train_final.to_parquet(\"squad_train_filtered.parquet\")\n</code></pre> Esto crea el archivo Parquet con los ejemplos filtrados.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#7-publicar-el-dataset-en-hugging-face","title":"7. Publicar el dataset en Hugging Face","text":"<p>Antes de publicar necesitas autenticarte con tu cuenta (aseg\u00farate de tener instalado huggingface_hub y un token de escritura):</p> <p><pre><code>from huggingface_hub import login\nlogin()  # Te pedir\u00e1 el token\n</code></pre> A continuaci\u00f3n, puedes usar el m\u00e9todo <code>push_to_hub</code> del dataset. Opcionalmente, crea primero un <code>DatasetDict</code> si quieres incluir tambi\u00e9n el <code>split</code> de validaci\u00f3n o test:</p> <pre><code>from datasets import DatasetDict\n\nfinal_dataset = DatasetDict({\n    \"train\": squad_train_final,\n    \"test\": squad_test\n})\n</code></pre> <p>Sube el dataset (reemplaza /squad_es_filtrado por tu nombre de usuario/repositorio en Hugging Face) <code>final_dataset.push_to_hub(\"&lt;tu_usuario&gt;/squad_es_filtrado\")</code>"},{"location":"Datasets/Actividad1_solution/#opcionalmente-anade-un-ejemplo-en-la-documentacion-editando-la-dataset-card-en-la-propia-web-de-hugging-face-tal-como-recomienda-la-sesionattached_file1","title":"Opcionalmente, a\u00f1ade un ejemplo en la documentaci\u00f3n editando la \"Dataset Card\" en la propia web de Hugging Face, tal como recomienda la sesi\u00f3n[attached_file:1].","text":"<p>Notas finales - Durante el proceso, imprime ejemplos y utiliza peque\u00f1os prints para comprobar cada paso. - La edici\u00f3n de la tarjeta del dataset (\"Dataset Card\") se realiza desde la web de Hugging Face: ah\u00ed puedes a\u00f1adir un ejemplo, uso previsto y detalles del proceso seguido, favoreciendo la comprensi\u00f3n de terceros usuarios.</p>"},{"location":"Datasets/Actividad1_solution/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"Datasets/Datasets/","title":"\ud83d\udcd8 Hugging Face Datasets: Apuntes + Reto Gamificado","text":""},{"location":"Datasets/Datasets/#1-introduccion","title":"1\ufe0f\u20e3 Introducci\u00f3n","text":"<p>El paquete <code>datasets</code> de Hugging Face es una potente herramienta para acceder, compartir y procesar conjuntos de datos (datasets) de IA para una amplia gama de tareas, que incluyen:</p> <ul> <li>Procesamiento del Lenguaje Natural (PLN)</li> <li>Visi\u00f3n por computador</li> <li>Procesamiento de audio</li> </ul> <p>Est\u00e1 dise\u00f1ado para manejar grandes vol\u00famenes de datos de manera eficiente mediante el uso de mapeo de memoria y el formato Apache Arrow, lo que permite trabajar con datos que superan la RAM disponible.</p> <p>Arrow Apache Arrow define un formato de memoria columnar independiente del lenguaje para datos planos y anidados, organizado para operaciones anal\u00edticas eficientes en hardware moderno como CPU y GPU. El formato de memoria Arrow tambi\u00e9n admite lecturas sin copia para un acceso a datos ultrarr\u00e1pido sin sobrecarga de serializaci\u00f3n.</p> <p>El proyecto del formato Apache Arrow comenz\u00f3 en febrero de 2016, centr\u00e1ndose en cargas de trabajo de an\u00e1lisis columnar en memoria. A diferencia de formatos de archivo como Parquet o CSV, que especifican c\u00f3mo se organizan los datos en el disco, Arrow se centra en c\u00f3mo se organizan los datos en la memoria. </p> <p>Los creadores buscan consolidar Arrow como un formato est\u00e1ndar en memoria para el an\u00e1lisis de cargas de trabajo. Estos fundamentos atraen a numerosos colaboradores de proyectos como Pandas, Spark, Cassandra, Apache Calcite, Dremio e Ibis.</p>"},{"location":"Datasets/Datasets/#caracteristicas-clave","title":"\ud83d\udd11 Caracter\u00edsticas Clave","text":"<ul> <li>Vasto Repositorio (Hub): Gran cantidad de datasets p\u00fablicos y privados.</li> <li>F\u00e1cil Acceso: Carga en una sola l\u00ednea de c\u00f3digo con <code>load_dataset</code>.</li> <li>Procesamiento Eficiente: M\u00e9todos como <code>map()</code> paralelizados.</li> <li>Escalabilidad: Objetos <code>Dataset</code> y <code>IterableDataset</code>.</li> <li>Gesti\u00f3n de Datos: Crear y subir datasets propios al Hub de Hugging    Face.</li> </ul> <p>Los datasets de Hugging Face sirven para:</p>"},{"location":"Datasets/Datasets/#1-acceder-a-datos-listos-para-ia","title":"\u2705 1. Acceder a datos listos para IA","text":"<p>Hugging Face ofrece un repositorio enorme de conjuntos de datos p\u00fablicos y privados para tareas como:</p> <ul> <li>Procesamiento del Lenguaje Natural (PLN)</li> <li>Visi\u00f3n por computadora</li> <li>Audio y multimodalidad</li> </ul>"},{"location":"Datasets/Datasets/#2-facilitar-el-preprocesamiento","title":"\u2705 2. Facilitar el preprocesamiento","text":"<p>Permite aplicar transformaciones como:</p> <ul> <li>Tokenizaci\u00f3n de texto</li> <li>Filtrado y remuestreo</li> <li>Conversi\u00f3n a formatos como Pandas, NumPy, PyTorch y TensorFlow</li> </ul>"},{"location":"Datasets/Datasets/#3-escalabilidad-y-eficiencia","title":"\u2705 3. Escalabilidad y eficiencia","text":"<p>Usa Apache Arrow y mapeo de memoria, lo que permite trabajar con datasets que superan la RAM disponible. Soporta dos tipos:</p> <ul> <li>Dataset (acceso aleatorio r\u00e1pido)</li> <li>IterableDataset (para streaming de datos grandes)</li> </ul>"},{"location":"Datasets/Datasets/#4-compartir-y-colaborar","title":"\u2705 4. Compartir y colaborar","text":"<p>Podemos crear y subir nuestros propios datasets al Hugging Face Hub, con documentaci\u00f3n y ejemplos. Esto fomenta la reproducibilidad y el trabajo en equipo.</p>"},{"location":"Datasets/Datasets/#5-integracion-directa-con-modelos","title":"\u2705 5. Integraci\u00f3n directa con modelos","text":"<p>Los datasets se integran f\u00e1cilmente con transformers y otros frameworks para entrenamiento y evaluaci\u00f3n.</p>"},{"location":"Datasets/Datasets/#instalacion","title":"\u2699\ufe0f Instalaci\u00f3n","text":"<pre><code>pip install datasets\npip install datasets[audio]\npip install datasets[vision]\n</code></pre>"},{"location":"Datasets/Datasets/#ejemplo-cargar-un-dataset-local","title":"\ud83e\udde9 Ejemplo: Cargar un dataset local","text":"<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Salida esperada: <pre><code>DatasetDict({\n    train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 })\n})\n</code></pre></p>"},{"location":"Datasets/Datasets/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"Datasets/Datasets/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p> <ol> <li>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y solo empleando Python:<ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con los datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset solo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> </ol>"},{"location":"Datasets/Datasets/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"Datasets/Datasets/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\n\n\n# Nivel 2: Dividir en train/test\n\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\n\n\n# Nivel 4: Filtrar y persistir\n\n# Nivel 5: Publicar en Hugging Face\n</code></pre> <p>Ya hemos visto que podemos utilizar diferentes datasets existentes en la plataforma para re-entrenar nuestros modelos.</p> <p>En esta sesi\u00f3n vamos a estudiar c\u00f3mo crear un dataset a partir de nuestros datos, limpiar un dataset, a reducirlo para que quepa en RAM y como tras crear nuestro dataset, subirlo al Hub.</p>"},{"location":"Datasets/Datasets/#cargando-datos","title":"Cargando datos","text":"<p>Para cargar datos, haremos uso de la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Datasets. Si fuera necesaria instalarla, mediante <code>pip</code> har\u00edamos:</p> <pre><code>pip install datasets\n</code></pre> <p>Podemos cargar archivos con diferentes formatos indicando el formato y los datos:</p> <pre><code># CSV &amp; TSV\nload_dataset(\"csv\", data_files=\"mis_datos.csv\")\n# Texto\nload_dataset(\"text\", data_files=\"mis_datos.txt\")\n# JSON &amp; JSONL\nload_dataset(\"json\", data_files=\"mis_datos.jsonl\")\n</code></pre> <p>Para este apartado nos vamos a centrar en los datos de SQuAD (Stanford Question Answering Dataset), compuesto de un conjunto de art\u00edculos de Wikipedia, con respuestas a diferentes preguntas sobre los art\u00edculos. En nuestro caso, nos vamos a centrar en una versi\u00f3n en castellano que podemos ver en https://huggingface.co/datasets/squad_es o descargarlo de desde https://github.com/ccasimiro88/TranslateAlignRetrieve/tree/master/SQuAD-es-v2.0.</p> <p>Si descargamos los archivos y los colocamos en la misma carpeta que el siguiente script, podemos realizar una carga local de los datos:</p> <p>squad-es-local.py <pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre></p> <p>Tras ejecutarlo, nos muestra que al cargar el dataset ha creado un split para train, con la cantidad de filas y las columnas contenidas dentro de un <code>DatasetDict</code>:</p> <pre><code>Generating train split: 442 examples [00:00, 1211.02 examples/s]\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n})\n</code></pre> <p>Si queremos obtener m\u00e1s informaci\u00f3n, podemos mostrar las caracter\u00edsticas:</p> <pre><code>print(squad_dataset[\"train\"].features)\n# {'title': Value(dtype='string', id=None),\n#  'paragraphs': [\n#     {'context': Value(dtype='string', id=None),\n#      'qas': [\n#         {'answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'id': Value(dtype='string', id=None),\n#          'is_impossible': Value(dtype='bool', id=None),\n#          'plausible_answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'question': Value(dtype='string', id=None)}]\n#     }\n#  ]\n# }\n</code></pre> <p>Y al tratarse de un diccionario, podemos navegar mediante los corchetes para, por ejemplo, recuperar parte de los datos:</p> <pre><code>print(squad_dataset[\"train\"][0][\"title\"])\n# Beyonc\u00e9 Knowles\nprint(squad_dataset[\"train\"][0][\"paragraphs\"][0])\n# {'context': 'Beyonc\u00e9 Giselle Knowles-Carter (nacida el 4 de septiembre de 1981)..',\n # 'qas': [\n    # {'answers': [{'answer_start': 246, 'text': 'a finales de 1990'}],\n    #  'id': '56be85543aeaaa14008c9063', 'is_impossible': False, 'plausible_answers': None, 'question': '\u00bfCu\u00e1ndo Beyonce comenz\u00f3 a ser popular?'},\n    # {'answers': ...\n</code></pre> <p>Cuando cargamos un dataset, realmente queremos cargar los datos de entrenamiento y los de validaci\u00f3n. Para ello, le pasaremos un diccionario con los diferentes archivos para <code>train</code> y para <code>val</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n</code></pre> <p>Obteniendo:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    val: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 35\n    })\n})\n</code></pre> <p>Si queremos cargar \u00fanicamente una de las partes, le pasaremos el par\u00e1metro <code>split</code> con el valor deseado:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\", split=\"train\")\nprint(squad_dataset_train)\n</code></pre> <p>Si el dataset est\u00e1 en un archivo comprimido en gzip, zip o tar, la librer\u00eda descomprimir\u00e1 los datos autom\u00e1ticamente:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json.zip\", \"val\": \"dev-v2.0-es.json.zip\"}\nsquad_dataset_trainval = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset_trainval)\n</code></pre>"},{"location":"Datasets/Datasets/#carga-remota","title":"Carga remota","text":"<p>Si el dataset est\u00e1 almacenado en una URL remota, podemos cargarlos directamente:</p> <p>squad-es-remoto.py <pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\n\nficheros_datos = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\",\n}\nsquad_remote_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n\nprint(squad_remote_dataset)\n</code></pre></p>"},{"location":"Datasets/Datasets/#dividiendo-el-dataset","title":"Dividiendo el dataset","text":"<p>Si en vez de cargar por separado los datos de entrenamiento y los de test, tenemos un \u00fanico conjunto de datos, podemos dividirlo mediante el m\u00e9todo <code>Dataset.train_test_split()</code> indicando el tama\u00f1o, por ejemplo, de los datos de test:</p> <p>squad-es-split.py <pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\", split=\"train\")\nsquad_split_dataset = squad_dataset.train_test_split(test_size=0.1)\nprint(squad_split_dataset)\n</code></pre></p>"},{"location":"Datasets/Datasets/#tamano-de-la-division-de-prueba-test_size-numpyrandomgenerator-opcional","title":"Tama\u00f1o de la divisi\u00f3n de prueba: <code>test_size (numpy.random.Generator, opcional)</code>","text":"<ul> <li>Si es <code>float</code>, debe estar entre 0.0 y 1.0 y representar la proporci\u00f3n del conjunto de datos que se incluir\u00e1 en la divisi\u00f3n de prueba. Si es int, representa el n\u00famero absoluto de muestras de prueba. </li> <li>Si es <code>None</code>, el valor se establece en el complemento del tama\u00f1o de entrenamiento.    Si <code>train_size</code> tambi\u00e9n es <code>None</code>, se establecer\u00e1 en 0.25.</li> </ul> <p>De manera que pasamos de un dataset de 442 filas a uno dividido en 397 para entrenamiento y 45 para test:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 397\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 45\n    })\n})\n</code></pre>"},{"location":"Datasets/Datasets/#cargando-en-pandas","title":"Cargando en Pandas","text":"<p>Si estamos trabajando con Pandas y queremos cargar un archivo que est\u00e1 en un dataset de Hugging Face, podemos utilizar la librer\u00eda de <code>huggingface_hub</code> para crear, eliminar, actualizar y recuperar informaci\u00f3n de los repositorios.</p> <p>Por ejemplo, para cargar un dataset CSV con Pandas podr\u00edamos hacer:</p> <p>cargando_en_pandas.py <pre><code>from huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"aitor-medrano/iabd\"\nFICHERO = \"cp_train.csv\"\n\ndataset = pd.read_csv(\n    hf_hub_download(repo_id=REPO_ID, filename=FICHERO, repo_type=\"dataset\")\n)\nprint(dataset)\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 3214 entries, 0 to 3213\n# Data columns (total 3 columns):\n#  #   Column   Non-Null Count  Dtype  \n# ---  ------   --------------  -----  \n#  0   formula  3214 non-null   object \n#  1   T        3214 non-null   float64\n#  2   Cp       3214 non-null   float64\n# dtypes: float64(2), object(1)\n# memory usage: 75.5+ KB\n</code></pre></p>"},{"location":"Datasets/Datasets/#cargando-en-streaming","title":"Cargando en streaming","text":"<p>Si el dataset ocupa varios gigas, con modelos como BERT o GPT, puede que no quepan en memoria RAM. Para ello, la librer\u00eda datasets gestiona los datos como ficheros mapeados en memoria realizando un streaming de los datos.</p> Streaming de un dataset - https://huggingface.co/docs/datasets/stream <p>Para este apartado, vamos a utilizar parte de un dataset con c\u00f3digo encontrado en GitHub de los diferentes lenguajes de programaci\u00f3n, conocido como BigCode.</p> <p>Gated dataset</p> <p>El dataset de BigCode est\u00e1 configurado para pedir las credenciales de los usuarios que los utilizan. Esto se conoce como un Gated dataset. Es por ello que tenemos que hacer login en Hugging Face antes de poder descargarlo.</p> <p>En nuestro caso, nos vamos a centrar en archivos <code>DockerFile</code> para que el dataset sea asumible (alrededor de 1Gb). As\u00ed pues, cargamos los datos:</p> <p>cargando_en_streaming.py <pre><code>from datasets import load_dataset\n\nbigcode_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\")\nprint(bigcode_dataset)\n# Dataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     num_rows: 4155925\n# })\n</code></pre></p> <p>Y el contenido de un documento:</p> <pre><code>{\n   \"blob_id\":\"a29dd2b33082d2b98541c66ba3620ae054991503\",\n   \"directory_id\":\"71568d223947f51cb007a8564e5f808e0987779e\",\n   \"path\":\"/src/Dapr/GameServer.Host/Dockerfile\",\n   \"content_id\":\"072b3138be512a71cf4e8bbbdb657497e808d2f6\",\n   \"detected_licenses\":[\n      \"MIT\",\n      \"LicenseRef-scancode-unknown-license-reference\"\n   ],\n   \"license_type\":\"permissive\",\n   \"repo_name\":\"devblack/OpenMU\",\n   \"snapshot_id\":\"8cdc521178da47c9c7d2daa502dc71688835fd0a\",\n   \"revision_id\":\"1064b0dca1a491bc28f325e42ca6b97d406d6558\",\n   \"branch_name\":\"refs/heads/master\",\n   \"visit_date\":Timestamp(\"2023-04-07 10:56:30.043316\"),\n   \"revision_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"committer_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"github_id\":None,\n   \"star_events_count\":0,\n   \"fork_events_count\":0,\n   \"gha_license_id\":None,\n   \"gha_event_created_at\":None,\n   \"gha_created_at\":None,\n   \"gha_language\":None,\n   \"src_encoding\":\"UTF-8\",\n   \"language\":\"Dockerfile\",\n   \"is_vendor\":false,\n   \"is_generated\":false,\n   \"length_bytes\":709,\n   \"extension\":\"\"\n}\n</code></pre> <p>Si ahora cargamos el dataset mediante streaming, pas\u00e1ndole el par\u00e1metros <code>streaming=True</code>, en vez de recuperar un Dataset, obtendremos un <code>IterableDataset</code>:</p> <pre><code>from datasets import load_dataset\n\nstreaming_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\", streaming=True)\nprint(streaming_dataset)\n# Resolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 976/976 [00:00&lt;00:00, 1064.36it/s]\n# IterableDataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     n_shards: 1\n# })\n</code></pre> <p>Como sugiere el nombre, para acceder a los elementos del <code>IterableDataset</code> necesitamos iterar sobre \u00e9l:</p> <pre><code>print(next(iter(streaming_dataset)))\n</code></pre> <p>Los elementos de un dataset en streaming  se pueden procesar al vuelo mediante la funci\u00f3n <code>map</code> o <code>filter</code>. Por ejemplo, para pasar a may\u00fasculas el campo <code>license_type</code> har\u00edamos:</p> <pre><code>def mayusLicencias(registro):\n    registro[\"license_type\"] = registro[\"license_type\"].upper()\n    return registro\n\nmapped_dataset = streaming_dataset.map(mayusLicencias)\n\nprint(next(iter(streaming_dataset)))\n# ... 'detected_licenses': ['MIT', ...], 'license_type': 'PERMISSIVE', 'repo_name': 'devblack/OpenMU' ...\n</code></pre> <p>Al vuelo</p> <p>Debemos tener en cuenta que la transformaci\u00f3n se realizar\u00e1 al recorrer el dataset, no al invocar a los m\u00e9todos <code>map</code> o <code>filter</code>.</p> <p>Otras opciones son utilizar las operaciones <code>take()</code>, <code>shuffle()</code> o <code>skip()</code> dentro del <code>IterableDataset</code> para trabajar con muestras peque\u00f1as de los datos cargados:</p> <pre><code>muestra = streaming_dataset.take(100)\nmuestra_aleatoria = streaming_dataset.shuffle(seed=42, buffer_size=1000).take(500)\nrango = streaming_dataset.skip(1000).take(100)\n</code></pre> <p>M\u00e1s informaci\u00f3n en https://huggingface.co/docs/datasets/stream y https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable.</p>"},{"location":"Datasets/Datasets/#limpiando","title":"Limpiando","text":"<p>De forma similar a Pandas o Spark, podemos manipular el contenido de los objetos <code>Dataset</code>.</p> <p>El primer paso deber\u00eda ser barajar los datos para desordenarlos y coger una muestra. Para ello, emplearemos <code>Dataset.shuffle()</code>:</p> <pre><code>squad_train = squad_dataset_trainval[\"train\"]\nprint(squad_train[0][\"title\"]) # Beyonc\u00e9 Knowles\n\nsquad_train_shuffled = squad_train.shuffle(seed=333)\nprint(squad_train_shuffled[0][\"title\"]) # Carnaval\n</code></pre>"},{"location":"Datasets/Datasets/#por-que-se-usa-seed-semilla-fija-de-333-en-el-metodo-suffle","title":"\u00bfPor qu\u00e9 se usa seed (semilla) fija de 333 en el m\u00e9todo suffle()?","text":"<p><pre><code>squad_train_shuffled = squad_train.shuffle(seed=333)\n</code></pre> Se usa una semilla fija (seed) en el m\u00e9todo shuffle() para garantizar reproducibilidad.</p> <p>\u2705 \u00bfQu\u00e9 significa esto?</p> <ul> <li>Cuando barajas datos, el orden resultante depende de un generador aleatorio.</li> <li>Si no se fija una semilla, cada ejecuci\u00f3n produce un orden distinto.</li> <li>Al establecer <code>seed=333</code>:<ul> <li>El generador aleatorio se inicializa siempre igual.</li> <li>El orden barajado ser\u00e1 id\u00e9ntico en cada ejecuci\u00f3n, lo que permite reproducir experimentos.</li> </ul> </li> </ul> <p>\u2705 \u00bfPor qu\u00e9 es importante en Machine Learning?</p> <ul> <li>Consistencia: Si compartes c\u00f3digo con otros, obtendr\u00e1n el mismo resultado.</li> <li>Depuraci\u00f3n: Puedes repetir pruebas sin que el orden cambie.</li> <li>Comparaci\u00f3n justa: Cuando eval\u00faas modelos, necesitas que los datos sean los mismos en cada experimento.</li> </ul>"},{"location":"Datasets/Datasets/#seleccionando-filas","title":"Seleccionando filas","text":"<p>Para recuperar filas, usaremos el m\u00e9todo <code>Dataset.select()</code> pas\u00e1ndole un iterador con las posiciones a seleccionar:</p> <pre><code>tres_filas = squad_train_shuffled.select([5,10,15])\nseis_filas = squad_train_shuffled.select(range(6))\n</code></pre> <p>Si queremos seleccionar las filas por alg\u00fan criterio espec\u00edfico, debemos emplear <code>Dataset.filter()</code> y funciones lambda:</p> <pre><code>empieza_por_b_filas = squad_train_shuffled.filter(lambda x: x[\"title\"].startswith(\"B\"))\nfor i in range(empieza_por_b_filas.num_rows):\n    print(empieza_por_b_filas[i][\"title\"])\n# Beidou _ Navigation _ Satellite _ System (en ingl\u00e9s)\n# Biodiversidad\n# Boston ...\n</code></pre>"},{"location":"Datasets/Datasets/#trabajando-con-columnas","title":"Trabajando con columnas","text":"<p>Al trabajar con columnas, podemos a\u00f1adir una nueva mediante <code>Dataset.add_column()</code>, cambiarles el nombre con <code>DatasetDict.rename_column()</code> o eliminar una columna con <code>DatasetDict.remove_column()</code>:</p> <pre><code>nueva_columna = [\"nueva\"] * squad_train_shuffled.num_rows\nsquad_train_shuffled = squad_train_shuffled.add_column(name=\"inicial\",column=nueva_columna)\n# Dataset({\n#     features: ['title', 'paragraphs', 'inicial'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.rename_column(\n    original_column_name=\"inicial\", new_column_name=\"modificada\"\n)\n# Dataset({\n#     features: ['title', 'paragraphs', 'modificada'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.remove_columns([\"modificada\"])\n# Dataset({\n#     features: ['title', 'paragraphs'],\n#     num_rows: 442\n# })\n</code></pre>"},{"location":"Datasets/Datasets/#trabajando-con-map","title":"Trabajando con map","text":"<p>Y la joya de la corona es la funci\u00f3n <code>Dataset.map()</code> para aplicar una transformaci\u00f3n a medida. Por ejemplo, si queremos modificar todos los t\u00edtulos y pasarlos a min\u00fascula haremos:</p> <p>squad-map.py <pre><code>from datasets import load_dataset\n\nficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nsquad_dataset = squad_dataset[\"train\"]\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\nsquad_minus = squad_dataset.map(titulo_minus)\n\n# Mostramos 5 elementos\nprint(squad_minus.shuffle(seed=987)[\"title\"][:5])\n# ['marshall', 'ascensor', 'kanye', 'bbc televisi\u00f3n', 'florida']\n</code></pre></p> <p>Mediante la funci\u00f3n <code>map</code> tambi\u00e9n podemos crear nuevas columnas a partir de los valores de otras. Por ejemplo, a\u00f1adimos una nueva caracter\u00edstica con la cantidad de p\u00e1rrafos y quitamos toda la informaci\u00f3n existente previamente de los p\u00e1rrafos:</p> <pre><code># A\u00f1adimos nueva columna\nsquad_col_num_parrafos = squad_dataset.map(lambda x: {\"num_paragraphs\":len(x[\"paragraphs\"])})\n# Borramos la antigua\nsquad_col_num_parrafos = squad_col_num_parrafos.remove_columns(\"paragraphs\")\nprint(squad_col_num_parrafos.shuffle(seed=987)[:5])\n# {'title': ['Marshall', 'Ascensor', 'Kanye', 'BBC Televisi\u00f3n', 'Florida'], 'num_paragraphs': [51, 52, 79, 31, 35]}\n</code></pre> <p>\u00bfY si queremos ordenar los p\u00e1rrafos por la cantidad de p\u00e1rrafos? Para ello, podemos utilizar <code>Dataset.sort()</code>:</p> <pre><code>squad_num_parrafos_ordenados = squad_col_num_parrafos.sort(\"num_paragraphs\")\n\nprint(squad_num_parrafos_ordenados[:3]) # tres primeros\n# {'title': ['Tono _ (m\u00fasica)', 'Uva', 'Letra _ case'], 'num_paragraphs': [10, 10, 12]}\n\nprint(squad_num_parrafos_ordenados[-3:]) # tres \u00faltimos\n# {'title': ['American Idol (en ingl\u00e9s).', 'Nueva York', 'Budismo'], 'num_paragraphs': [127, 148, 149]}\n</code></pre>"},{"location":"Datasets/Datasets/#trabajando-con-batches","title":"Trabajando con batches","text":"<p>El m\u00e9todo <code>Dataset.map()</code> admite el argumento <code>batched</code> que, si se establece en <code>True</code>, hace que env\u00ede un lote de datos a la funci\u00f3n <code>map</code> a la vez (el tama\u00f1o del lote es configurable mediante el par\u00e1metro <code>batch_size</code>, pero por defecto es de 1000). Al indicar <code>batched=True</code>, la funci\u00f3n <code>map</code> recibe un diccionario con los campos del conjunto de datos, pero cada valor es ahora una lista de valores, y no un \u00fanico valor, y del mismo modo, devuelve un diccionario con los campos que queremos actualizar o a\u00f1adir a nuestro conjunto de datos, y una lista de valores.</p> <p>Por ello, debemos modificar la funci\u00f3n que realiza el <code>map</code> para que trabaje con un diccionario de listas.</p> <p>Para el siguiente ejemplo, vamos a coger el dataset de SQuAD_es desde Hugging Face que contiene m\u00e1s datos, y vamos a comparar el tiempo de ejecuci\u00f3n, recodificando la funci\u00f3n para trabajar con un diccionario que contiene una lista por cada campo:</p> <p>squad-map-batch.py <pre><code>from datasets import load_dataset\nimport time\n\n# Cargamos el dataset desde HF\nsquad_dataset = load_dataset(\"squad_es\", \"v2.0.0\", split=\"train\")  # (1)!\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus) # (2)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 37926.07 examples/s]   \nfin = time.time()\nprint(fin - inicio) # 3.008699893951416\n\ndef titulo_minus_batch(batch):\n    return {\"title\":[titulo.lower() for titulo in batch[\"title\"]]}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus_batch, batched=True) # (3)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 535210.67 examples/s]\nfin = time.time()\nprint(fin - inicio) # 0.19133710861206055\n</code></pre></p> <ol> <li>Cargamos los datos desde el dataset de Hugging Face</li> <li>El mapper normal realiza casi 40.000 registros por segundo</li> <li>Al hacerlo con batches realiza m\u00e1s de 500.000 por segundo</li> </ol>"},{"location":"Datasets/Datasets/#uso-de-pandas","title":"Uso de Pandas","text":"<p>Para facilitar la conversi\u00f3n entre diferentes formados, podemos usar el m\u00e9todo <code>Dataset.set_format()</code> para modificar el formato de salida (por ejemplo, a <code>pandas</code>, <code>numpy</code>, <code>torch</code>, <code>tensorflow</code>, <code>jax</code>, ...), sin modificar el formato original del dataset (el cual es Apache Arrow - <code>arrow</code>).</p> <p>As\u00ed, si por ejemplo queremos pasar los datos a Pandas para trabajar con ellos, transformarlos o visualizarlos, podemos hacer:</p> <pre><code>print(squad_dataset[0])\nsquad_dataset.set_format(\"pandas\")\ndf = squad_dataset[:]\nprint(df.shape) # (130313, 5)\n\n# Otra forma\ndf2 = squad_dataset.to_pandas()\nprint(df2.info())\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 130313 entries, 0 to 130312\n# Data columns (total 5 columns):\n#  #   Column    Non-Null Count   Dtype \n# ---  ------    --------------   ----- \n#  0   id        130313 non-null  object\n#  1   title     130313 non-null  object\n#  2   context   130313 non-null  object\n#  3   question  130313 non-null  object\n#  4   answers   130313 non-null  object\n# dtypes: object(5)\n# memory usage: 5.0+ MB\n\n# Y realizar consultas sobre los datos, por ejemplo, la 5 preguntas con m\u00e1s respuestas\nprint(df2.groupby(\"title\")[\"answers\"].count().nlargest(5))\n# title\n# Reina                         883\n# Nueva York                    817\n# American Idol (en ingl\u00e9s).    790\n# Beyonc\u00e9 Knowles               753\n# Universidad                   738\n</code></pre> <p>Y para volver a tener un objeto <code>Dataset</code>, podemos utilizar el m\u00e9todo <code>Dataset.from_pandas()</code> o resetear el dataset que ten\u00edamos mediante <code>Dataset.reset_format()</code>:</p> <pre><code>squad_transformed = Dataset.from_pandas(df)\nsquad_original = squad_dataset.reset_format()\n</code></pre>"},{"location":"Datasets/Datasets/#creando-un-dataset-desde-cero","title":"Creando un dataset desde cero","text":"<p>Si en vez de cargar un dataset, tenemos que crear uno, el primer paso ser\u00e1 obtener los datos.</p> <p>Ya sea mediante peticiones a URL externas con la librer\u00eda <code>request</code>, accediendo a bases de datos relacionales, por ejemplo, mediante SQLAlchemy o a MongoDB mediante <code>PyMongo</code>, lo m\u00e1s normal es crear una estructura de datos mediante una lista o un diccionario y luego crear el dataset a partir del mismo.</p> <p>As\u00ed pues, si en vez de cargar un dataset queremos crear uno desde cero, podemos hacer uso de <code>Dataset.from_dict()</code> o <code>Dataset.from_list()</code> dependiendo de donde tengamos los datos. Por ejemplo, vamos a conectarnos a MongoDB y creamos una lista con todos los datos. A partir de la lista, generamos un Dataset:</p> <p>dataset-mongodb.py <pre><code>from datasets import Dataset\nfrom pymongo import MongoClient\n\ncliente = MongoClient('mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net')\n\n# Vamos a trabajar con la colecci\u00f3n grades de sample_training\niabd_db = cliente.sample_training\ngrades_coll = iabd_db.grades\n\n# Creamos un dataset vac\u00edo mediante una lista\ndocumentos = []\n\n# Recuperamos todas las calificaciones\ncursor = grades_coll.find({})\nfor document in cursor:\n  del document[\"_id\"]  # Quitamos el ObjectId\n  documentos.append(document)\n\nmongo_dataset = Dataset.from_list(documentos)\nprint(mongo_dataset)\n# Dataset({\n#     features: ['student_id', 'scores', 'class_id'],\n#     num_rows: 100000\n# })\n</code></pre></p> <p>O si queremos almacenar los datos de forma temporal, por ejemplo, podemos hacer uso de Pandas:</p> <pre><code>df = pd.DataFrame.from_records(documentos)\ndf.to_json(\"grades-docs.jsonl\", orient=\"records\", lines=True)\njsonl_dataset = load_dataset(\"json\", data_files=\"grades-docs.jsonl\", split=\"train\")\n</code></pre>"},{"location":"Datasets/Datasets/#persistiendo","title":"Persistiendo","text":"<p>Cuando cargamos un dataset se cachean los datos para evitar tener que descargar los datos de forma remota y crear las tablas de PyArrow. Podemos averiguar donde se almacenan los datos mediante la propiedad <code>cache_files</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset.cache_files)\n# {'train': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-train.arrow'}],\n#  'test': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-test.arrow'}]}\n</code></pre> <p>Si necesitamos persistir los datos de nuevo con las transformaciones realizadas, podemos hacerlo en diferentes formatos:</p> <ul> <li>Arrow: <code>Dataset.save_to_disk()</code></li> <li>CSV: <code>Dataset.to_csv()</code></li> <li>Pandas: <code>Dataset.to_pandas()</code></li> <li>JSON: <code>Dataset.to_json()</code></li> <li>Parquet: <code>Dataset.to_parquet()</code></li> </ul> <p>Si por ejemplo persistimos el dataset en Arrow:</p> <pre><code>squad_dataset.save_to_disk(\"squad_train_test\")\n</code></pre> <p>Se crear\u00e1 la siguiente estructura en las carpetas, donde para cada split se ha creado su propia carpeta, as\u00ed como un par de archivos con metadatos:</p> <pre><code>squad_train_test\n\u251c\u2500\u2500 dataset_dict.json\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 data-00000-of-00001.arrow\n\u2502   \u251c\u2500\u2500 dataset_info.json\n\u2502   \u2514\u2500\u2500 state.json\n\u2514\u2500\u2500 train\n    \u251c\u2500\u2500 data-00000-of-00001.arrow\n    \u251c\u2500\u2500 dataset_info.json\n    \u2514\u2500\u2500 state.json\n</code></pre> <p>Una vez que hemos persistido el dataset, lo podemos volver a cargar mediante <code>load_from_disk</code>:</p> <pre><code>from datasets import load_from_disk\nsquad_disk = load_from_disk(\"squad_train_test\")\n</code></pre> <p>Si queremos persistirlos en formato CSV o JSON, por cada uno de los splits se crea un archivo separado. Para ello, podemos iterar sobre las claves mediante <code>DatasetDict.items()</code>:</p> <pre><code>for split, dataset in squad_dataset.items():\n    dataset.to_json(f\"squad_{split}.jsonl\")\n</code></pre>"},{"location":"Datasets/Datasets/#publicando","title":"Publicando","text":"<p>Pues una vez hemos creado nuestro Dataset y lo hemos persistido en disco, el \u00fanico paso que nos queda es publicar en Hugging Face. Para ello, en vez de subir los archivos a mano como hicimos en la sesi\u00f3n inicial desde el interfaz web o hacer uso de Git y copiar los archivos, desde Python es mucho m\u00e1s c\u00f3modo.</p> <p>Previamente debemos realizar login para autenticarnos, tal como vimos en la secci\u00f3n de Login de la sesi\u00f3n anterior y usar nuestro token de acceso (en este caso, necesitamos que nuestro token sea de escritura):</p> <pre><code>hf auth login\n</code></pre> <p>Login desde un cuaderno Jupyter / Colab</p> <p>Si trabajamos con un cuaderno de Jupyter o Colab, podemos utilizar la librer\u00eda de huggingface_hub y al ejecutarlo, nos aparecer\u00e1 una ventana donde introducir el token de acceso:</p> <pre><code>from huggingface_hub import login\nlogin()\n</code></pre> <p> Login desde un notebook </p> <p>Tambi\u00e9n podemos hacer uso de la variable de entorno <code>HF_TOKEN</code> para almacenar el token de acceso, ya sea con un Space almacen\u00e1ndolo en un secret de Spaces o en claves privadas de Google Colab.</p> <p>Una vez autenticado, es tan sencillo como usar el m\u00e9todo <code>push_to_hub()</code> (si queremos publicar el dataset como privado, le a\u00f1adimos el par\u00e1metro <code>private=True</code>):</p> <pre><code>midataset.push_to_hub(\"aitor-medrano/midatasetpushed\")\n# midataset.push_to_hub(\"aitor-medrano/midatasetpushed\", private=True)\n</code></pre> <p>Si tuvi\u00e9ramos diferentes splits podr\u00edamos hacer:</p> <pre><code>train_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"train\")\nval_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"validation\")\n</code></pre> <p>Creando el dataset card</p> <p>Para crear el dataset card, podemos emplear la aplicaci\u00f3n https://huggingface.co/spaces/huggingface/datasets-tagging que facilita rellenar los campos e incluye recomendaciones de otros datasets.</p> <p>Tambi\u00e9n se recomienda leer la Dataset Card Creation Guide</p>"},{"location":"Datasets/Datasets/#datasets-multimedia","title":"Datasets multimedia","text":"<p>Si nuestro dataset va a contener elementos multimedia con im\u00e1genes o audios, podemos subir los archivos en crudo, lo cual es lo m\u00e1s practico en la mayor\u00eda de los casos. Si los dataset de im\u00e1genes o audios son muy grandes, se recomienda el formato WebDataset, aunque lo m\u00e1s normal, es almacenar el dataset en formato Parquet.</p> <p>Dicho esto, vamos a ver como trabajar con datasets que contienen elementos multimedia.</p>"},{"location":"Datasets/Datasets/#audios","title":"Audios","text":"<p>Para trabajar con audios, necesitaremos instalar las librer\u00edas de audio:</p> <pre><code>pip install datasets[audio]\n</code></pre> <p>Una vez instalado, vamos a cargar un dataset que contiene audios, como es PolyAI/minds14:</p> <p>audio_minds.py <pre><code>from datasets import load_dataset\n\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\nprint(minds)\n# Dataset({\n#     features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n#     num_rows: 486\n# })\n</code></pre></p> <p>El dataset contiene 486 archivos de audio, cada uno de los cuales va acompa\u00f1ado de una transcripci\u00f3n, una traducci\u00f3n al ingl\u00e9s y una etiqueta que indica la intenci\u00f3n de la consulta de la persona. La columna de audio contiene los datos de audio sin procesar. Veamos uno de los ejemplos:</p> <pre><code>{'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n 'audio': {'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n    'array': array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            -0.00024414, -0.00024414]),\n    'sampling_rate': 8000\n },\n 'transcription': 'hola buenas a ver tengo un problema con vuestra aplicaci\u00f3n resulta que quiero hacer una transferencia bancaria a una cuenta conocida',\n 'english_transcription': 'hello good to see I have a problem with your application it turns out that I want to make a bank transfer to a known account',\n 'intent_class': 2,\n 'lang_id': 5}\n</code></pre> <p>Si nos centramos en el campo <code>audio</code>, vemos como contiene diferentes valores, los cuales forman parte de un objeto <code>Audio</code>:</p> <ul> <li><code>path</code>: ruta al archivo de audio (en este ejemplo <code>*.wav</code>).</li> <li><code>array</code>: los datos del audio decodificados, representados por un array de NumPy</li> <li><code>sampling_rate</code>: la frecuencia de muestreo (8,000 Hz).</li> </ul> <pre><code>ejemplo = minds.shuffle()[0]\n\nid2label = minds.features[\"intent_class\"].int2str\nlabel = id2label(ejemplo[\"intent_class\"])\n</code></pre> <p>Y si queremos escuchar el audio, usando Gradio podemos crear un componente:</p> <p>audio_app.py</p> <pre><code>import gradio as gr\nfrom datasets import load_dataset\n\n# Cargar el dataset\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\n\n# Obtener un audio aleatorio del dataset\naudio = minds.shuffle()[0]\n\n#obtenemos el audio decodificado (que devuelve un objeto AudioDecoder)\naudio_decode = audio[\"audio\"]\n\n# Etiqueta de intenci\u00f3n\nid2label = minds.features[\"intent_class\"].int2str\nprint(id2label)\n#Usando la funci\u00f3n anterior, toma el identificador de intenciones del ejemplo \n# y lo convierte en su nombre textual \n# para saber qu\u00e9 clase de intento representa ese dato.\nintent_id = audio[\"intent_class\"]\nlabel = id2label(intent_id)\n\n#print(label.names[])\n\ndef load_audio():\n    return (audio_decode[\"sampling_rate\"], audio_decode[\"array\"])\n\n# Crear la interfaz de Gradio\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Reproductor de Audio - Dataset PolyAI/minds14\")\n\n    audio_component = gr.Audio(\n        value=load_audio(),\n        label=label,\n        interactive=False\n    )\n\ndemo.launch(share=True)\n</code></pre> <p>Y reproducir el audio seleccionado:</p> Reproduciendo un audio desde Gradio <p>Finalmente, si queremos mostrar un gr\u00e1fico con el espectograma del audio, y haciendo uso de la librer\u00eda Librosa:</p> <pre><code>import librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\n\narray = ejemplo[\"audio\"][\"array\"]\nsampling_rate = ejemplo[\"audio\"][\"sampling_rate\"]\n\nplt.figure().set_figwidth(12)\nlibrosa.display.waveshow(array, sr=sampling_rate)\n</code></pre> <p>Obteniendo:</p> Representaci\u00f3n gr\u00e1fica de un audio"},{"location":"Datasets/Datasets/#creando-un-dataset-desde-python","title":"Creando un dataset desde Python","text":"<p>Si queremos crear un dataset a partir de audios propios, necesitamos crear un diccionario o una lista con la ruta a nuestros audios, y a continuaci\u00f3n, llamar al m\u00e9todo <code>cast_column</code> para transformar la columna a tipo <code>Audio</code> con las caracter\u00edsticas que hemos visto antes.</p> <p>As\u00ed pues, si tenemos los datos en un diccionario podemos hacer:</p> <pre><code>audio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", \"path/to/audio_2\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\n</code></pre>"},{"location":"Datasets/Datasets/#audiofolder","title":"Audiofolder","text":"<p>Otra posibilidad es crear el dataset a partir de una carpeta con todos los audios, y un archivo <code>metadata.csv</code> con los datos:</p> <pre><code>folder/train/metadata.csv\nfolder/train/first_audio_file.mp3\nfolder/train/second_audio_file.mp3\nfolder/train/third_audio_file.mp3\n</code></pre> <p>Y un archivo de metadatos con:</p> <p>metadata.csv <pre><code>file_name,transcription\nfirst_audio_file.mp3,este es el texto del primer audio\nsecond_audio_file.mp3,en el segundo audio cuento un chiste\nthird_audio_file.mp3,y en este audio agradezco al p\u00fablico sus abucheos\n</code></pre></p> <p>Y a continuaci\u00f3n, indicamos como primer par\u00e1metro <code>autofolder</code> y con <code>data_dir</code> indicamos la carpeta que contiene los audios:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n</code></pre>"},{"location":"Datasets/Datasets/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> <li>Documentaci\u00f3n oficial de dataset</li> <li>El cap\u00edtulo La librer\u00eda Dataset del curso NLP de Hugging Face.</li> </ul>"},{"location":"Datasets/Datasets/#actividades","title":"Actividades","text":"<ol> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y s\u00f3lo empleando Python:</p> <ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con lo datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset s\u00f3lo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 0.5p) Carga en streaming el dataset de imagenet-1k, y muestra la etiqueta de los primeros 5 elementos.</p> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 1p) Tenemos un dataset con informaci\u00f3n sobre vuelos retrasados en Espa\u00f1a y otro con informaci\u00f3n general sobre los aeropuertos. Se pide:</p> <ol> <li>Une ambos datasets para que la informaci\u00f3n de los vuelos contengan el nombre y la ciudad de los aeropuertos.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato Arrow.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / opcional ) Sobre la base de datos de MongoDB de <code>sample_training</code> disponible en los datos de muestra de MongoAtlas:</p> <ol> <li>Carga los de estados de <code>states.js</code>.</li> <li>Realiza una join entre las colecciones <code>zips</code> y <code>states</code>.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato JSON.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> </ol>"},{"location":"Datasets/datasets_aitor/","title":"Cargando datasets en HuggingFace (apuntes de Aitor Medrano)","text":"<p>Ya hemos visto que podemos utilizar diferentes datasets existentes en la plataforma para re-entrenar nuestros modelos.</p> <p>En esta sesi\u00f3n vamos a estudiar c\u00f3mo crear un dataset a partir de nuestros datos, limpiar un dataset, a reducirlo para que quepa en RAM y como tras crear nuestro dataset, subirlo al Hub.</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#cargando-datos","title":"Cargando datos","text":"<p>Para cargar datos, haremos uso de la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Datasets. Si fuera necesaria instalarla, mediante <code>pip</code> har\u00edamos:</p> <pre><code>pip install datasets\n</code></pre> <p>Podemos cargar archivos con diferentes formatos indicando el formato y los datos:</p> <pre><code># CSV &amp; TSV\nload_dataset(\"csv\", data_files=\"mis_datos.csv\")\n# Texto\nload_dataset(\"text\", data_files=\"mis_datos.txt\")\n# JSON &amp; JSONL\nload_dataset(\"json\", data_files=\"mis_datos.jsonl\")\n</code></pre> <p>Para este apartado nos vamos a centrar en los datos de SQuAD (Stanford Question Answering Dataset), compuesto de un conjunto de art\u00edculos de Wikipedia, con respuestas a diferentes preguntas sobre los art\u00edculos. En nuestro caso, nos vamos a centrar en una versi\u00f3n en castellano que podemos ver en https://huggingface.co/datasets/squad_es o descargarlo de desde https://github.com/ccasimiro88/TranslateAlignRetrieve/tree/master/SQuAD-es-v2.0.</p> <p>Si descargamos los archivos y los colocamos en la misma carpeta que el siguiente script, podemos realizar una carga local de los datos:</p> squad-es-local.py<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Tras ejecutarlo, nos muestra que al cargar el dataset ha creado un split para train, con la cantidad de filas y las columnas contenidas dentro de un <code>DatasetDict</code>:</p> <pre><code>Generating train split: 442 examples [00:00, 1211.02 examples/s]\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n})\n</code></pre> <p>Si queremos obtener m\u00e1s informaci\u00f3n, podemos mostrar las caracter\u00edsticas:</p> <pre><code>print(squad_dataset[\"train\"].features)\n# {'title': Value(dtype='string', id=None),\n#  'paragraphs': [\n#     {'context': Value(dtype='string', id=None),\n#      'qas': [\n#         {'answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'id': Value(dtype='string', id=None),\n#          'is_impossible': Value(dtype='bool', id=None),\n#          'plausible_answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'question': Value(dtype='string', id=None)}]\n#     }\n#  ]\n# }\n</code></pre> <p>Y al tratarse de un diccionario, podemos navegar mediante los corchetes para, por ejemplo, recuperar parte de los datos:</p> <pre><code>print(squad_dataset[\"train\"][0][\"title\"])\n# Beyonc\u00e9 Knowles\nprint(squad_dataset[\"train\"][0][\"paragraphs\"][0])\n# {'context': 'Beyonc\u00e9 Giselle Knowles-Carter (nacida el 4 de septiembre de 1981)..',\n # 'qas': [\n    # {'answers': [{'answer_start': 246, 'text': 'a finales de 1990'}],\n    #  'id': '56be85543aeaaa14008c9063', 'is_impossible': False, 'plausible_answers': None, 'question': '\u00bfCu\u00e1ndo Beyonce comenz\u00f3 a ser popular?'},\n    # {'answers': ...\n</code></pre> <p>Cuando cargamos un dataset, realmente queremos cargar los datos de entrenamiento y los de validaci\u00f3n. Para ello, le pasaremos un diccionario con los diferentes archivos para <code>train</code> y para <code>val</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n</code></pre> <p>Obteniendo:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    val: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 35\n    })\n})\n</code></pre> <p>Si queremos cargar \u00fanicamente una de las partes, le pasaremos el par\u00e1metro <code>split</code> con el valor deseado:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\", split=\"train\")\nprint(squad_dataset_train)\n</code></pre> <p>Si el dataset est\u00e1 en un archivo comprimido en gzip, zip o tar, la librer\u00eda descomprimir\u00e1 los datos autom\u00e1ticamente:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json.zip\", \"val\": \"dev-v2.0-es.json.zip\"}\nsquad_dataset_trainval = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset_trainval)\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#carga-remota","title":"Carga remota","text":"<p>Si el dataset est\u00e1 almacenado en una URL remota, podemos cargarlos directamente:</p> squad-es-remoto.py<pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\n\nficheros_datos = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\",\n}\nsquad_remote_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n\nprint(squad_remote_dataset)\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#dividiendo-el-dataset","title":"Dividiendo el dataset","text":"<p>Si en vez de cargar por separado los datos de entrenamiento y los de test, tenemos un \u00fanico conjunto de datos, podemos dividirlo mediante el m\u00e9todo <code>Dataset.train_test_split()</code> indicando el tama\u00f1o, por ejemplo, de los datos de test:</p> squad-es-split.py<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\", split=\"train\")\nsquad_split_dataset = squad_dataset.train_test_split(test_size=0.1)\nprint(squad_split_dataset)\n</code></pre> <p>De manera que pasamos de un dataset de 442 filas a uno dividido en 397 para entrenamiento y 45 para test:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 397\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 45\n    })\n})\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#cargando-en-pandas","title":"Cargando en Pandas","text":"<p>Si estamos trabajando con Pandas y queremos cargar un archivo que est\u00e1 en un dataset de Hugging Face, podemos utilizar la librer\u00eda de <code>huggingface_hub</code> para crear, eliminar, actualizar y recuperar informaci\u00f3n de los repositorios.</p> <p>Por ejemplo, para cargar un dataset CSV con Pandas podr\u00edamos hacer:</p> <pre><code>from huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"aitor-medrano/iabd\"\nFICHERO = \"cp_train.csv\"\n\ndataset = pd.read_csv(\n    hf_hub_download(repo_id=REPO_ID, filename=FICHERO, repo_type=\"dataset\")\n)\nprint(dataset)\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 3214 entries, 0 to 3213\n# Data columns (total 3 columns):\n#  #   Column   Non-Null Count  Dtype  \n# ---  ------   --------------  -----  \n#  0   formula  3214 non-null   object \n#  1   T        3214 non-null   float64\n#  2   Cp       3214 non-null   float64\n# dtypes: float64(2), object(1)\n# memory usage: 75.5+ KB\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#cargando-en-streaming","title":"Cargando en streaming","text":"<p>Si el dataset ocupa varios gigas, con modelos como BERT o GPT, puede que no quepan en memoria RAM. Para ello, la librer\u00eda datasets gestiona los datos como ficheros mapeados en memoria realizando un streaming de los datos.</p> Streaming de un dataset - https://huggingface.co/docs/datasets/stream <p>Para este apartado, vamos a utilizar parte de un dataset con c\u00f3digo encontrado en GitHub de los diferentes lenguajes de programaci\u00f3n, conocido como BigCode.</p> <p>Gated dataset</p> <p>El dataset de BigCode est\u00e1 configurado para pedir las credenciales de los usuarios que los utilizan. Esto se conoce como un Gated dataset. Es por ello que tenemos que hacer login en Hugging Face antes de poder descargarlo.</p> <p>En nuestro caso, nos vamos a centrar en archivos <code>DockerFile</code> para que el dataset sea asumible (alrededor de 1Gb). As\u00ed pues, cargamos los datos:</p> <pre><code>from datasets import load_dataset\n\nbigcode_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\")\nprint(bigcode_dataset)\n# Dataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     num_rows: 4155925\n# })\n</code></pre> <p>Y el contenido de un documento:</p> <pre><code>{\n   \"blob_id\":\"a29dd2b33082d2b98541c66ba3620ae054991503\",\n   \"directory_id\":\"71568d223947f51cb007a8564e5f808e0987779e\",\n   \"path\":\"/src/Dapr/GameServer.Host/Dockerfile\",\n   \"content_id\":\"072b3138be512a71cf4e8bbbdb657497e808d2f6\",\n   \"detected_licenses\":[\n      \"MIT\",\n      \"LicenseRef-scancode-unknown-license-reference\"\n   ],\n   \"license_type\":\"permissive\",\n   \"repo_name\":\"devblack/OpenMU\",\n   \"snapshot_id\":\"8cdc521178da47c9c7d2daa502dc71688835fd0a\",\n   \"revision_id\":\"1064b0dca1a491bc28f325e42ca6b97d406d6558\",\n   \"branch_name\":\"refs/heads/master\",\n   \"visit_date\":Timestamp(\"2023-04-07 10:56:30.043316\"),\n   \"revision_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"committer_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"github_id\":None,\n   \"star_events_count\":0,\n   \"fork_events_count\":0,\n   \"gha_license_id\":None,\n   \"gha_event_created_at\":None,\n   \"gha_created_at\":None,\n   \"gha_language\":None,\n   \"src_encoding\":\"UTF-8\",\n   \"language\":\"Dockerfile\",\n   \"is_vendor\":false,\n   \"is_generated\":false,\n   \"length_bytes\":709,\n   \"extension\":\"\"\n}\n</code></pre> <p>Si ahora cargamos el dataset mediante streaming, pas\u00e1ndole el par\u00e1metros <code>streaming=True</code>, en vez de recuperar un Dataset, obtendremos un <code>IterableDataset</code>:</p> <pre><code>from datasets import load_dataset\n\nstreaming_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\", streaming=True)\nprint(streaming_dataset)\n# Resolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 976/976 [00:00&lt;00:00, 1064.36it/s]\n# IterableDataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     n_shards: 1\n# })\n</code></pre> <p>Como sugiere el nombre, para acceder a los elementos del <code>IterableDataset</code> necesitamos iterar sobre \u00e9l:</p> <pre><code>print(next(iter(streaming_dataset)))\n</code></pre> <p>Los elementos de un dataset en streaming  se pueden procesar al vuelo mediante la funci\u00f3n <code>map</code> o <code>filter</code>. Por ejemplo, para pasar a may\u00fasculas el campo <code>license_type</code> har\u00edamos:</p> <pre><code>def mayusLicencias(registro):\n    registro[\"license_type\"] = registro[\"license_type\"].upper()\n    return registro\n\nmapped_dataset = streaming_dataset.map(mayusLicencias)\n\nprint(next(iter(streaming_dataset)))\n# ... 'detected_licenses': ['MIT', ...], 'license_type': 'PERMISSIVE', 'repo_name': 'devblack/OpenMU' ...\n</code></pre> <p>Al vuelo</p> <p>Debemos tener en cuenta que la transformaci\u00f3n se realizar\u00e1 al recorrer el dataset, no al invocar a los m\u00e9todos <code>map</code> o <code>filter</code>.</p> <p>Otras opciones son utilizar las operaciones <code>take()</code>, <code>shuffle()</code> o <code>skip()</code> dentro del <code>IterableDataset</code> para trabajar con muestras peque\u00f1as de los datos cargados:</p> <pre><code>muestra = streaming_dataset.take(100)\nmuestra_aleatoria = streaming_dataset.shuffle(seed=42, buffer_size=1000).take(500)\nrango = streaming_dataset.skip(1000).take(100)\n</code></pre> <p>M\u00e1s informaci\u00f3n en https://huggingface.co/docs/datasets/stream y https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable.</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#limpiando","title":"Limpiando","text":"<p>De forma similar a Pandas o Spark, podemos manipular el contenido de los objetos <code>Dataset</code>.</p> <p>El primer paso deber\u00eda ser barajar los datos para desordenarlos y coger una muestra. Para ello, emplearemos <code>Dataset.shuffle()</code>:</p> <pre><code>squad_train = squad_dataset_trainval[\"train\"]\nprint(squad_train[0][\"title\"]) # Beyonc\u00e9 Knowles\n\nsquad_train_shuffled = squad_train.shuffle(seed=333)\nprint(squad_train_shuffled[0][\"title\"]) # Carnaval\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#seleccionando-filas","title":"Seleccionando filas","text":"<p>Para recuperar filas, usaremos el m\u00e9todo <code>Dataset.select()</code> pas\u00e1ndole un iterador con las posiciones a seleccionar:</p> <pre><code>tres_filas = squad_train_shuffled.select([5,10,15])\nseis_filas = squad_train_shuffled.select(range(6))\n</code></pre> <p>Si queremos seleccionar las filas por alg\u00fan criterio espec\u00edfico, debemos emplear <code>Dataset.filter()</code> y funciones lambda:</p> <pre><code>empieza_por_b_filas = squad_train_shuffled.filter(lambda x: x[\"title\"].startswith(\"B\"))\nfor i in range(empieza_por_b_filas.num_rows):\n    print(empieza_por_b_filas[i][\"title\"])\n# Beidou _ Navigation _ Satellite _ System (en ingl\u00e9s)\n# Biodiversidad\n# Boston ...\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#trabajando-con-columnas","title":"Trabajando con columnas","text":"<p>Al trabajar con columnas, podemos a\u00f1adir una nueva mediante <code>Dataset.add_column()</code>, cambiarles el nombre con <code>DatasetDict.rename_column()</code> o eliminar una columna con <code>DatasetDict.remove_column()</code>:</p> <pre><code>nueva_columna = [\"nueva\"] * squad_train_shuffled.num_rows\nsquad_train_shuffled = squad_train_shuffled.add_column(name=\"inicial\",column=nueva_columna)\n# Dataset({\n#     features: ['title', 'paragraphs', 'inicial'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.rename_column(\n    original_column_name=\"inicial\", new_column_name=\"modificada\"\n)\n# Dataset({\n#     features: ['title', 'paragraphs', 'modificada'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.remove_columns([\"modificada\"])\n# Dataset({\n#     features: ['title', 'paragraphs'],\n#     num_rows: 442\n# })\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#trabajando-con-map","title":"Trabajando con map","text":"<p>Y la joya de la corona es la funci\u00f3n <code>Dataset.map()</code> para aplicar una transformaci\u00f3n a medida. Por ejemplo, si queremos modificar todos los t\u00edtulos y pasarlos a min\u00fascula haremos:</p> squad-map.py<pre><code>from datasets import load_dataset\n\nficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nsquad_dataset = squad_dataset[\"train\"]\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\nsquad_minus = squad_dataset.map(titulo_minus)\n\n# Mostramos 5 elementos\nprint(squad_minus.shuffle(seed=987)[\"title\"][:5])\n# ['marshall', 'ascensor', 'kanye', 'bbc televisi\u00f3n', 'florida']\n</code></pre> <p>Mediante la funci\u00f3n <code>map</code> tambi\u00e9n podemos crear nuevas columnas a partir de los valores de otras. Por ejemplo, a\u00f1adimos una nueva caracter\u00edstica con la cantidad de p\u00e1rrafos y quitamos toda la informaci\u00f3n existente previamente de los p\u00e1rrafos:</p> <pre><code># A\u00f1adimos nueva columna\nsquad_col_num_parrafos = squad_dataset.map(lambda x: {\"num_paragraphs\":len(x[\"paragraphs\"])})\n# Borramos la antigua\nsquad_col_num_parrafos = squad_col_num_parrafos.remove_columns(\"paragraphs\")\nprint(squad_col_num_parrafos.shuffle(seed=987)[:5])\n# {'title': ['Marshall', 'Ascensor', 'Kanye', 'BBC Televisi\u00f3n', 'Florida'], 'num_paragraphs': [51, 52, 79, 31, 35]}\n</code></pre> <p>\u00bfY si queremos ordenar los p\u00e1rrafos por la cantidad de p\u00e1rrafos? Para ello, podemos utilizar <code>Dataset.sort()</code>:</p> <pre><code>squad_num_parrafos_ordenados = squad_col_num_parrafos.sort(\"num_paragraphs\")\n\nprint(squad_num_parrafos_ordenados[:3]) # tres primeros\n# {'title': ['Tono _ (m\u00fasica)', 'Uva', 'Letra _ case'], 'num_paragraphs': [10, 10, 12]}\n\nprint(squad_num_parrafos_ordenados[-3:]) # tres \u00faltimos\n# {'title': ['American Idol (en ingl\u00e9s).', 'Nueva York', 'Budismo'], 'num_paragraphs': [127, 148, 149]}\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#trabajando-con-batches","title":"Trabajando con batches","text":"<p>El m\u00e9todo <code>Dataset.map()</code> admite el argumento <code>batched</code> que, si se establece en <code>True</code>, hace que env\u00ede un lote de datos a la funci\u00f3n <code>map</code> a la vez (el tama\u00f1o del lote es configurable mediante el par\u00e1metro <code>batch_size</code>, pero por defecto es de 1000). Al indicar <code>batched=True</code>, la funci\u00f3n <code>map</code> recibe un diccionario con los campos del conjunto de datos, pero cada valor es ahora una lista de valores, y no un \u00fanico valor, y del mismo modo, devuelve un diccionario con los campos que queremos actualizar o a\u00f1adir a nuestro conjunto de datos, y una lista de valores.</p> <p>Por ello, debemos modificar la funci\u00f3n que realiza el <code>map</code> para que trabaje con un diccionario de listas.</p> <p>Para el siguiente ejemplo, vamos a coger el dataset de SQuAD_es desde Hugging Face que contiene m\u00e1s datos, y vamos a comparar el tiempo de ejecuci\u00f3n, recodificando la funci\u00f3n para trabajar con un diccionario que contiene una lista por cada campo:</p> squad-map-batch.py<pre><code>from datasets import load_dataset\nimport time\n\n# Cargamos el dataset desde HF\nsquad_dataset = load_dataset(\"squad_es\", \"v2.0.0\", split=\"train\")  # (1)!\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus) # (2)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 37926.07 examples/s]   \nfin = time.time()\nprint(fin - inicio) # 3.008699893951416\n\ndef titulo_minus_batch(batch):\n    return {\"title\":[titulo.lower() for titulo in batch[\"title\"]]}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus_batch, batched=True) # (3)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 535210.67 examples/s]\nfin = time.time()\nprint(fin - inicio) # 0.19133710861206055\n</code></pre> <ol> <li>Cargamos los datos desde el dataset de Hugging Face</li> <li>El mapper normal realiza casi 40.000 registros por segundo</li> <li>Al hacerlo con batches realiza m\u00e1s de 500.000 por segundo</li> </ol>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#uso-de-pandas","title":"Uso de Pandas","text":"<p>Para facilitar la conversi\u00f3n entre diferentes formados, podemos usar el m\u00e9todo <code>Dataset.set_format()</code> para modificar el formato de salida (por ejemplo, a <code>pandas</code>, <code>numpy</code>, <code>torch</code>, <code>tensorflow</code>, <code>jax</code>, ...), sin modificar el formato original del dataset (el cual es Apache Arrow - <code>arrow</code>).</p> <p>As\u00ed, si por ejemplo queremos pasar los datos a Pandas para trabajar con ellos, transformarlos o visualizarlos, podemos hacer:</p> <pre><code>print(squad_dataset[0])\nsquad_dataset.set_format(\"pandas\")\ndf = squad_dataset[:]\nprint(df.shape) # (130313, 5)\n\n# Otra forma\ndf2 = squad_dataset.to_pandas()\nprint(df2.info())\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 130313 entries, 0 to 130312\n# Data columns (total 5 columns):\n#  #   Column    Non-Null Count   Dtype \n# ---  ------    --------------   ----- \n#  0   id        130313 non-null  object\n#  1   title     130313 non-null  object\n#  2   context   130313 non-null  object\n#  3   question  130313 non-null  object\n#  4   answers   130313 non-null  object\n# dtypes: object(5)\n# memory usage: 5.0+ MB\n\n# Y realizar consultas sobre los datos, por ejemplo, la 5 preguntas con m\u00e1s respuestas\nprint(df2.groupby(\"title\")[\"answers\"].count().nlargest(5))\n# title\n# Reina                         883\n# Nueva York                    817\n# American Idol (en ingl\u00e9s).    790\n# Beyonc\u00e9 Knowles               753\n# Universidad                   738\n</code></pre> <p>Y para volver a tener un objeto <code>Dataset</code>, podemos utilizar el m\u00e9todo <code>Dataset.from_pandas()</code> o resetear el dataset que ten\u00edamos mediante <code>Dataset.reset_format()</code>:</p> <pre><code>squad_transformed = Dataset.from_pandas(df)\nsquad_original = squad_dataset.reset_format()\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#creando-un-dataset-desde-cero","title":"Creando un dataset desde cero","text":"<p>Si en vez de cargar un dataset, tenemos que crear uno, el primer paso ser\u00e1 obtener los datos.</p> <p>Ya sea mediante peticiones a URL externas con la librer\u00eda <code>request</code>, accediendo a bases de datos relacionales, por ejemplo, mediante SQLAlchemy o a MongoDB mediante <code>PyMongo</code>, lo m\u00e1s normal es crear una estructura de datos mediante una lista o un diccionario y luego crear el dataset a partir del mismo.</p> <p>As\u00ed pues, si en vez de cargar un dataset queremos crear uno desde cero, podemos hacer uso de <code>Dataset.from_dict()</code> o <code>Dataset.from_list()</code> dependiendo de donde tengamos los datos. Por ejemplo, vamos a conectarnos a MongoDB y creamos una lista con todos los datos. A partir de la lista, generamos un Dataset:</p> dataset-mongodb.py<pre><code>from datasets import Dataset\nfrom pymongo import MongoClient\n\ncliente = MongoClient('mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net')\n\n# Vamos a trabajar con la colecci\u00f3n grades de sample_training\niabd_db = cliente.sample_training\ngrades_coll = iabd_db.grades\n\n# Creamos un dataset vac\u00edo mediante una lista\ndocumentos = []\n\n# Recuperamos todas las calificaciones\ncursor = grades_coll.find({})\nfor document in cursor:\n  del document[\"_id\"]  # Quitamos el ObjectId\n  documentos.append(document)\n\nmongo_dataset = Dataset.from_list(documentos)\nprint(mongo_dataset)\n# Dataset({\n#     features: ['student_id', 'scores', 'class_id'],\n#     num_rows: 100000\n# })\n</code></pre> <p>O si queremos almacenar los datos de forma temporal, por ejemplo, podemos hacer uso de Pandas:</p> <pre><code>df = pd.DataFrame.from_records(documentos)\ndf.to_json(\"grades-docs.jsonl\", orient=\"records\", lines=True)\njsonl_dataset = load_dataset(\"json\", data_files=\"grades-docs.jsonl\", split=\"train\")\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#persistiendo","title":"Persistiendo","text":"<p>Cuando cargamos un dataset se cachean los datos para evitar tener que descargar los datos de forma remota y crear las tablas de PyArrow. Podemos averiguar donde se almacenan los datos mediante la propiedad <code>cache_files</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset.cache_files)\n# {'train': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-train.arrow'}],\n#  'test': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-test.arrow'}]}\n</code></pre> <p>Si necesitamos persistir los datos de nuevo con las transformaciones realizadas, podemos hacerlo en diferentes formatos:</p> <ul> <li>Arrow: <code>Dataset.save_to_disk()</code></li> <li>CSV: <code>Dataset.to_csv()</code></li> <li>Pandas: <code>Dataset.to_pandas()</code></li> <li>JSON: <code>Dataset.to_json()</code></li> <li>Parquet: <code>Dataset.to_parquet()</code></li> </ul> <p>Si por ejemplo persistimos el dataset en Arrow:</p> <pre><code>squad_dataset.save_to_disk(\"squad_train_test\")\n</code></pre> <p>Se crear\u00e1 la siguiente estructura en las carpetas, donde para cada split se ha creado su propia carpeta, as\u00ed como un par de archivos con metadatos:</p> <pre><code>squad_train_test\n\u251c\u2500\u2500 dataset_dict.json\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 data-00000-of-00001.arrow\n\u2502   \u251c\u2500\u2500 dataset_info.json\n\u2502   \u2514\u2500\u2500 state.json\n\u2514\u2500\u2500 train\n    \u251c\u2500\u2500 data-00000-of-00001.arrow\n    \u251c\u2500\u2500 dataset_info.json\n    \u2514\u2500\u2500 state.json\n</code></pre> <p>Una vez que hemos persistido el dataset, lo podemos volver a cargar mediante <code>load_from_disk</code>:</p> <pre><code>from datasets import load_from_disk\nsquad_disk = load_from_disk(\"squad_train_test\")\n</code></pre> <p>Si queremos persistirlos en formato CSV o JSON, por cada uno de los splits se crea un archivo separado. Para ello, podemos iterar sobre las claves mediante <code>DatasetDict.items()</code>:</p> <pre><code>for split, dataset in squad_dataset.items():\n    dataset.to_json(f\"squad_{split}.jsonl\")\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#publicando","title":"Publicando","text":"<p>Pues una vez hemos creado nuestro Dataset y lo hemos persistido en disco, el \u00fanico paso que nos queda es publicar en Hugging Face. Para ello, en vez de subir los archivos a mano como hicimos en la sesi\u00f3n inicial desde el interfaz web o hacer uso de Git y copiar los archivos, desde Python es mucho m\u00e1s c\u00f3modo.</p> <p>Previamente debemos realizar login para autenticarnos, tal como vimos en la secci\u00f3n de Login de la sesi\u00f3n anterior y usar nuestro token de acceso (en este caso, necesitamos que nuestro token sea de escritura):</p> <pre><code>hf auth login\n</code></pre> <p>Login desde un cuaderno Jupyter / Colab</p> <p>Si trabajamos con un cuaderno de Jupyter o Colab, podemos utilizar la librer\u00eda de huggingface_hub y al ejecutarlo, nos aparecer\u00e1 una ventana donde introducir el token de acceso:</p> <pre><code>from huggingface_hub import login\nlogin()\n</code></pre> <p> Login desde un notebook </p> <p>Tambi\u00e9n podemos hacer uso de la variable de entorno <code>HF_TOKEN</code> para almacenar el token de acceso, ya sea con un Space almacen\u00e1ndolo en un secret de Spaces o en claves privadas de Google Colab.</p> <p>Una vez autenticado, es tan sencillo como usar el m\u00e9todo <code>push_to_hub()</code> (si queremos publicar el dataset como privado, le a\u00f1adimos el par\u00e1metro <code>private=True</code>):</p> <pre><code>midataset.push_to_hub(\"aitor-medrano/midatasetpushed\")\n# midataset.push_to_hub(\"aitor-medrano/midatasetpushed\", private=True)\n</code></pre> <p>Si tuvi\u00e9ramos diferentes splits podr\u00edamos hacer:</p> <pre><code>train_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"train\")\nval_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"validation\")\n</code></pre> <p>Creando el dataset card</p> <p>Para crear el dataset card, podemos emplear la aplicaci\u00f3n https://huggingface.co/spaces/huggingface/datasets-tagging que facilita rellenar los campos e incluye recomendaciones de otros datasets.</p> <p>Tambi\u00e9n se recomienda leer la Dataset Card Creation Guide</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#datasets-multimedia","title":"Datasets multimedia","text":"<p>Si nuestro dataset va a contener elementos multimedia con im\u00e1genes o audios, podemos subir los archivos en crudo, lo cual es lo m\u00e1s practico en la mayor\u00eda de los casos. Si los dataset de im\u00e1genes o audios son muy grandes, se recomienda el formato WebDataset, aunque lo m\u00e1s normal, es almacenar el dataset en formato Parquet.</p> <p>Dicho esto, vamos a ver como trabajar con datasets que contienen elementos multimedia.</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#audios","title":"Audios","text":"<p>Para trabajar con audios, necesitaremos instalar las librer\u00edas de audio:</p> <pre><code>pip install datasets[audio]\n</code></pre> <p>Una vez instalado, vamos a cargar un dataset que contiene audios, como es PolyAI/minds14:</p> <pre><code>from datasets import load_dataset\n\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\nprint(minds)\n# Dataset({\n#     features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n#     num_rows: 486\n# })\n</code></pre> <p>El dataset contiene 486 archivos de audio, cada uno de los cuales va acompa\u00f1ado de una transcripci\u00f3n, una traducci\u00f3n al ingl\u00e9s y una etiqueta que indica la intenci\u00f3n de la consulta de la persona. La columna de audio contiene los datos de audio sin procesar. Veamos uno de los ejemplos:</p> <pre><code>{'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n 'audio': {'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n    'array': array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            -0.00024414, -0.00024414]),\n    'sampling_rate': 8000\n },\n 'transcription': 'hola buenas a ver tengo un problema con vuestra aplicaci\u00f3n resulta que quiero hacer una transferencia bancaria a una cuenta conocida',\n 'english_transcription': 'hello good to see I have a problem with your application it turns out that I want to make a bank transfer to a known account',\n 'intent_class': 2,\n 'lang_id': 5}\n</code></pre> <p>Si nos centramos en el campo <code>audio</code>, vemos como contiene diferentes valores, los cuales forman parte de un objeto <code>Audio</code>:</p> <ul> <li><code>path</code>: ruta al archivo de audio (en este ejemplo <code>*.wav</code>).</li> <li><code>array</code>: los datos del audio decodificados, representados por un array de NumPy</li> <li><code>sampling_rate</code>: la frecuencia de muestreo (8,000 Hz).</li> </ul> <pre><code>ejemplo = minds.shuffle()[0]\n\nid2label = minds.features[\"intent_class\"].int2str\nlabel = id2label(ejemplo[\"intent_class\"])\n</code></pre> <p>Y si queremos escuchar el audio, usando Gradio podemos crear un componente:</p> <pre><code> with gr.Blocks() as demo:\n    with gr.Column():\n        gr.Audio(audio[\"path\"], label=label)\ndemo.launch(share=True)\n</code></pre> <p>Y reproducir el audio seleccionado:</p> Reproduciendo un audio desde Gradio <p>Finalmente, si queremos mostrar un gr\u00e1fico con el espectograma del audio, y haciendo uso de la librer\u00eda Librosa:</p> <pre><code>import librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\n\narray = ejemplo[\"audio\"][\"array\"]\nsampling_rate = ejemplo[\"audio\"][\"sampling_rate\"]\n\nplt.figure().set_figwidth(12)\nlibrosa.display.waveshow(array, sr=sampling_rate)\n</code></pre> <p>Obteniendo:</p> Representaci\u00f3n gr\u00e1fica de un audio","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#creando-un-dataset-desde-python","title":"Creando un dataset desde Python","text":"<p>Si queremos crear un dataset a partir de audios propios, necesitamos crear un diccionario o una lista con la ruta a nuestros audios, y a continuaci\u00f3n, llamar al m\u00e9todo <code>cast_column</code> para transformar la columna a tipo <code>Audio</code> con las caracter\u00edsticas que hemos visto antes.</p> <p>As\u00ed pues, si tenemos los datos en un diccionario podemos hacer:</p> <pre><code>audio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", \"path/to/audio_2\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#audiofolder","title":"Audiofolder","text":"<p>Otra posibilidad es crear el dataset a partir de una carpeta con todos los audios, y un archivo <code>metadata.csv</code> con los datos:</p> <pre><code>folder/train/metadata.csv\nfolder/train/first_audio_file.mp3\nfolder/train/second_audio_file.mp3\nfolder/train/third_audio_file.mp3\n</code></pre> <p>Y un archivo de metadatos con:</p> metadata.csv<pre><code>file_name,transcription\nfirst_audio_file.mp3,este es el texto del primer audio\nsecond_audio_file.mp3,en el segundo audio cuento un chiste\nthird_audio_file.mp3,y en este audio agradezco al p\u00fablico sus abucheos\n</code></pre> <p>Y a continuaci\u00f3n, indicamos como primer par\u00e1metro <code>autofolder</code> y con <code>data_dir</code> indicamos la carpeta que contiene los audios:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#referencias","title":"Referencias","text":"<ul> <li>Documentaci\u00f3n oficial de dataset</li> <li>El cap\u00edtulo La librer\u00eda Dataset del curso NLP de Hugging Face.</li> </ul>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#actividades","title":"Actividades","text":"<ol> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y s\u00f3lo empleando Python:</p> <ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con lo datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset s\u00f3lo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 0.5p) Carga en streaming el dataset de imagenet-1k, y muestra la etiqueta de los primeros 5 elementos.</p> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 1p) Tenemos un dataset con informaci\u00f3n sobre vuelos retrasados en Espa\u00f1a y otro con informaci\u00f3n general sobre los aeropuertos. Se pide:</p> <ol> <li>Une ambos datasets para que la informaci\u00f3n de los vuelos contengan el nombre y la ciudad de los aeropuertos.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato Arrow.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / opcional ) Sobre la base de datos de MongoDB de <code>sample_training</code> disponible en los datos de muestra de MongoAtlas:</p> <ol> <li>Carga los de estados de <code>states.js</code>.</li> <li>Realiza una join entre las colecciones <code>zips</code> y <code>states</code>.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato JSON.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> </ol>","tags":["HuggingFace"]},{"location":"bedrock/sesion1/","title":"\ud83d\udcd8 Sesi\u00f3n 1: Introducci\u00f3n a Amazon Bedrock","text":""},{"location":"bedrock/sesion1/#objetivos","title":"Objetivos:","text":"<ul> <li>Comprender qu\u00e9 es Amazon Bedrock, sus funcionalidades principales y su prop\u00f3sito.</li> <li>Explorar los conceptos de \u201cmodelos fundacionales\u201d (FMs), \u201cRAG\u201d (generaci\u00f3n aumentada por recuperaci\u00f3n), \u201cfine-tuning\u201d y \u201cguards / guardrails\u201d para una IA responsable.</li> <li>Formular hip\u00f3tesis sobre c\u00f3mo una empresa (hotel, restaurante, ...) o centros educativos, ayuntamientos, etc\u00e9tera podr\u00edan beneficiarse de la IA generativa.</li> <li>Dise\u00f1ar, en equipo, una propuesta de aplicaci\u00f3n concreta usando Amazon Bedrock adaptada a un caso real.</li> </ul>"},{"location":"bedrock/sesion1/#1-introduccion-a-amazon-bedrock","title":"1. Introducci\u00f3n a Amazon Bedrock","text":"<p>Amazon Bedrock es un servicio de AWS que permite a los desarrolladores construir aplicaciones generativas utilizando modelos fundacionales (FMs) sin necesidad de gestionar infraestructura. Ofrece acceso a modelos como Claude (Anthropic), Titan (Amazon) y Stable Diffusion (Stability AI).</p>"},{"location":"bedrock/sesion1/#capacidades-destacadas","title":"Capacidades destacadas","text":"<ul> <li>RAG (Retrieval-Augmented Generation): podemos conectar Bedrock a nuestras propias fuentes de datos (documentos, bases de conocimiento) de modo que las respuestas del modelo est\u00e9n informadas por datos reales de nuestra empresa. Esto ayuda a responder consultas concretas basadas en informaci\u00f3n actualizada.  Amazon Web Services, Inc. </li> <li>Fine-tuning / personalizaci\u00f3n privada: es posible adaptar un modelo para tareas espec\u00edficas o para un dominio concreto (por ejemplo, hoteler\u00eda, turismo, restaurante, etc.), usando nuestros propios datos, sin que esos datos entren a formar parte del modelo base. </li> <li>Seguridad, privacidad e IA responsable: Bedrock integra funcionalidades de protecci\u00f3n, guardrails, control de contenidos y privacidad de datos, para reducir riesgos \u2014 por ejemplo, filtrado de contenido inapropiado, protecci\u00f3n de datos, auditor\u00eda\u2026 </li> <li>Flexibilidad de modelos: podemos elegir entre muchos FMs de distintos proveedores seg\u00fan el uso: algunos ser\u00e1n mejores para generaci\u00f3n de texto creativa; otros para respuestas precisas; otros para integraci\u00f3n con datos. </li> </ul>"},{"location":"bedrock/sesion1/#que-es-retrieval-augmented-generation-rag-o-generacion-aumentada-por-recuperacion","title":"\u00bfQu\u00e9 es \"Retrieval Augmented Generation\" (RAG) o Generaci\u00f3n Aumentada por Recuperaci\u00f3n ?","text":"<p>La Retrieval Augmented Generation (RAG) o Generaci\u00f3n Aumentada por Recuperaci\u00f3n en espa\u00f1ol, es una t\u00e9cnica de IA que combina dos componentes: un sistema de recuperaci\u00f3n de informaci\u00f3n (bases de datos vectoriales) y un LLM (modelo de lenguaje grande).  El objetivo es mejorar la precisi\u00f3n y la actualidad de las respuestas del LLM al permitirle acceder y utilizar informaci\u00f3n de fuentes de datos externas y espec\u00edficas antes de generar su respuesta, sin necesidad de reentrenamiento. </p> <ul> <li>Retrieval (Recuperaci\u00f3n): Esta etapa consiste en indexar, recuperar los segmentos de texto creados (embeddings) que son relevantes en funci\u00f3n de la similutd sem\u00e1ntica.</li> <li>Augmentation (aumentar): Incrementar con informaci\u00f3n adicional los conocimientos del LLM.</li> <li>Generation: Generar o elaborar respuestas a partir de los conocimientos del LLM. </li> </ul>"},{"location":"bedrock/sesion1/#como-funciona","title":"C\u00f3mo funciona","text":"<ol> <li>Recuperaci\u00f3n: Cuando se hace una consulta, un sistema de recuperaci\u00f3n busca y selecciona los fragmentos de informaci\u00f3n m\u00e1s relevantes de una base de conocimiento externa (que puede incluir documentos privados, bases de datos o fuentes de noticias).</li> <li>Generaci\u00f3n: El modelo de lenguaje grande (LLM) toma la consulta original junto con la informaci\u00f3n recuperada para generar una respuesta m\u00e1s precisa, actualizada y contextualizada.</li> <li>Ejemplo: Si un usuario pregunta sobre un producto espec\u00edfico, el sistema RAG puede buscar en la base de datos de la empresa informaci\u00f3n sobre ese producto y luego usarla para que el LLM genere una respuesta detallada y precisa. </li> </ol>"},{"location":"bedrock/sesion1/#beneficios-de-rag","title":"Beneficios de RAG","text":"<ul> <li>Acceso a datos actualizados: Permite a los LLM acceder a informaci\u00f3n m\u00e1s reciente que la que ten\u00edan durante su entrenamiento inicial.</li> <li>Reducci\u00f3n de alucinaciones : Disminuye la probabilidad de que el modelo \"invente\" informaci\u00f3n, ya que se basa en datos concretos.</li> <li>Adaptaci\u00f3n a dominios espec\u00edficos: Facilita la creaci\u00f3n de chatbots o aplicaciones que pueden responder preguntas sobre temas muy espec\u00edficos o propietarios, como el conocimiento interno de una empresa.</li> <li>Referencia de fuentes: Puede citar las fuentes de informaci\u00f3n utilizadas, lo que aumenta la transparencia y la confianza en las respuestas.</li> <li>Eficiencia: Es una forma m\u00e1s r\u00e1pida y econ\u00f3mica de actualizar la informaci\u00f3n de un LLM en comparaci\u00f3n con el reentrenamiento completo del modelo. </li> </ul> <p>M\u00e1s informaci\u00f3n en este v\u00eddeo: https://www.youtube.com/watch?v=-NqZehslaNk</p>"},{"location":"bedrock/sesion1/#ventajas","title":"Ventajas","text":"<ul> <li>Sin necesidad de entrenar modelos desde cero.</li> <li>Integraci\u00f3n nativa con servicios AWS.</li> <li>Escalabilidad y seguridad empresarial.</li> </ul>"},{"location":"bedrock/sesion1/#casos-de-uso","title":"Casos de uso","text":"<ul> <li>Chatbots inteligentes.</li> <li>Generaci\u00f3n de contenido.</li> <li>Res\u00famenes autom\u00e1ticos.</li> <li>Recuperaci\u00f3n aumentada (RAG).</li> </ul>"},{"location":"bedrock/sesion1/#ejemplos-clave-de-uso-de-bedrock","title":"\ud83d\udca1 Ejemplos Clave de Uso de Bedrock","text":"\u00c1rea de Uso Descripci\u00f3n Modelo Fundacional T\u00edpico Generaci\u00f3n de contenido Crear art\u00edculos, descripciones de productos o scripts autom\u00e1ticamente. Amazon Titan Text, Anthropic Claude B\u00fasqueda y resumen Crear chatbots que responden preguntas bas\u00e1ndose en documentos internos (patrones RAG). Amazon Titan Embeddings, Meta Llama 2 Automatizaci\u00f3n de agentes Construir agentes de IA que pueden realizar tareas complejas de varios pasos (ej. procesar reclamaciones). Agents for Amazon Bedrock (usando modelos como Claude) Generaci\u00f3n de c\u00f3digo Asistencia para desarrolladores que genera fragmentos de c\u00f3digo, traduce lenguajes o explica funciones. Anthropic Claude Code"},{"location":"bedrock/sesion1/#ejemplo-ilustrativo","title":"Ejemplo ilustrativo","text":"<p>Imagina que gestinamos un hotel y queremos ofrecer a nuestros clientes un \u201casistente inteligente\u201d (chatbot) que:</p> <ul> <li>Responda preguntas frecuentes: horarios, servicios, recomendaciones locales.</li> <li>Sugiera experiencias seg\u00fan perfil del cliente (familia, pareja, negocios, con mascotas).</li> <li>Responda en varios idiomas.</li> </ul> <p>Con Amazon Bedrock podr\u00edamos:</p> <ul> <li>Crear una base de conocimiento con informaci\u00f3n propia del hotel: descripciones de servicios, normas, tarifas, actividades, recomendaciones locales.</li> <li>Usar RAG (Retrieval-Augmented Generation) para que el modelo se base en esa informaci\u00f3n interna cuando responda.</li> <li>Si queremos precisi\u00f3n en el estilo de las respuestas (por ejemplo, tono amable, cercano, profesional), har\u00edamos un fine-tuning con ejemplos de interacciones t\u00edpicas.</li> <li>Publicar ese asistente como chatbot web, Bot de Telegram, WhatsApp o similar sin tener que disponer de servidores propios: Bedrock lo gestiona.</li> </ul>"},{"location":"bedrock/sesion1/#2-parametros-de-inferencia-y-experimentacion","title":"2. Par\u00e1metros de Inferencia y Experimentaci\u00f3n","text":"<p>Podemos experimentar con las configuraciones del prompt para controlar el comportamiento del modelo.</p> Par\u00e1metro Descripci\u00f3n Impacto en el resultado Temperatura Controla la creatividad y la diversidad de las respuestas. Un valor superior genera respuestas m\u00e1s creativas y diversas. P Superior (Top P) Permite seleccionar palabras m\u00e1s probables. Permite variar entre respuestas m\u00e1s probables o menos probables. Longitud M\u00e1xima (MaxTokenCount) Define el tama\u00f1o m\u00e1ximo de la respuesta generada. Limita el coste y la extensi\u00f3n de la respuesta."},{"location":"bedrock/sesion1/#3-eleccion-estrategica-del-modelo","title":"3. Elecci\u00f3n estrat\u00e9gica del modelo","text":"<p>Amazon Bedrock ofrece flexibilidad para elegir el modelo que mejor se adapte a cada necesidad.</p> <ul> <li>Modelos de Amazon (Titan/Nova): Modelos propietarios que ofrecen inteligencia multimodal r\u00e1pida y rentable, incluyendo generaci\u00f3n de texto, im\u00e1genes, comprensi\u00f3n de documentos y c\u00f3digo. El modelo Nova Lite es multimodal y sensible a los costes, mientras que Nova Pro es competente para tareas m\u00e1s complejas. Los modelos Titan Text Express son recomendados para tareas de alto volumen y bajo coste como el resumen.</li> <li>Anthropic (Claude): Modelos que destacan en razonamiento complejo, generaci\u00f3n de c\u00f3digo y seguimiento de instrucciones, adecuados para industrias que exigen cumplimiento y confianza.</li> <li>Stability AI: Conocidos por sus modelos de generaci\u00f3n de im\u00e1genes, como Stable Diffusion 3.5 Large.</li> <li>DeepSeek: Modelos avanzados de razonamiento que resuelven problemas complejos paso a paso.</li> <li>Mistral AI: Modelos especializados para el razonamiento agentic y tareas multimodales.</li> </ul>"},{"location":"bedrock/sesion1/#ejemplo-basico-generar-texto-con-claude","title":"Ejemplo b\u00e1sico: Generar texto con Claude","text":"<p>C\u00f3digo en Python usando boto3:</p> <pre><code>import boto3\n\nclient = boto3.client('bedrock-runtime', region_name='us-east-1')\n\nprompt = \"Resume en 3 puntos las ventajas de Amazon Bedrock\"\nresponse = client.invoke_model(\n    modelId=\"anthropic.claude-v2\",\n    body={\"input\": prompt}\n)\n\nprint(response['body'])\n</code></pre>"},{"location":"bedrock/sesion1/#workshop","title":"WORKSHOP","text":""},{"location":"bedrock/sesion1/#1-hello-bedrock-primeros-pasos-20-minutos","title":"1. HELLO BEDROCK - PRIMEROS PASOS (20 minutos)","text":""},{"location":"bedrock/sesion1/#objetivos_1","title":"Objetivos:","text":"<ul> <li>Entender la interfaz b\u00e1sica de Amazon Bedrock</li> <li>Realizar primeras interacciones con modelos fundacionales</li> <li>Identificar limitaciones de los modelos sin base de conocimiento</li> </ul>"},{"location":"bedrock/sesion1/#actividades-practicas","title":"Actividades pr\u00e1cticas:","text":""},{"location":"bedrock/sesion1/#configuracion-inicial","title":"Configuraci\u00f3n inicial:","text":"<ul> <li>Gu\u00eda paso a paso para acceder a la consola AWS</li> <li>\"Vamos a seleccionar el modelo Nova Pro para nuestros primeros ejemplos\"</li> <li>Explicaci\u00f3n de la interfaz del playground de Bedrock</li> </ul>"},{"location":"bedrock/sesion1/#ejercicio-hello-bedrock","title":"Ejercicio \"Hello Bedrock\":","text":"<p>Ejemplos de prompts para demostraci\u00f3n:</p> <ol> <li>Prompt b\u00e1sico de presentaci\u00f3n: <pre><code>Pres\u00e9ntate y explica brevemente qu\u00e9 puedes hacer como asistente de IA.\n</code></pre></li> <li>Prompt de conocimiento general: <pre><code>Explica en 5 puntos clave qu\u00e9 es la Inteligencia Artificial Generativa y c\u00f3mo est\u00e1 cambiando el sector educativo.\n</code></pre></li> <li>Prompt con instrucciones espec\u00edficas: <pre><code>Act\u00faa como un experto en formaci\u00f3n profesional y crea un plan de estudios breve para un m\u00f3dulo de introducci\u00f3n a la IA. El plan debe incluir 3 unidades con sus respectivos objetivos y actividades principales.\n</code></pre></li> <li>Prompt para probar par\u00e1metros: <pre><code>Genera tres ideas creativas para utilizar la IA en el aula. S\u00e9 muy conciso. \n</code></pre> (Demostrar c\u00f3mo cambia la respuesta modificando la temperatura)</li> </ol> <p>Preguntas para la audiencia:</p> <ul> <li>\"\u00bfQu\u00e9 diferencias not\u00e1is entre un prompt simple y uno m\u00e1s estructurado?\"</li> <li>\"\u00bfC\u00f3mo cre\u00e9is que afecta la temperatura a la creatividad de las respuestas?\"</li> </ul>"},{"location":"bedrock/sesion1/#analisis-de-limitaciones","title":"An\u00e1lisis de limitaciones:","text":"<p>Ejemplos de prompts que muestran limitaciones:</p> <ol> <li>Conocimiento actualizado: <pre><code>\u00bfCu\u00e1les son las \u00faltimas normativas de la Generalitat Valenciana sobre formaci\u00f3n profesional publicadas este a\u00f1o?\n</code></pre></li> <li>Informaci\u00f3n espec\u00edfica local: <pre><code>Describe el proceso actual para solicitar una beca de formaci\u00f3n profesional en la Comunidad Valenciana, incluyendo plazos y requisitos espec\u00edficos.\n</code></pre></li> <li>Datos t\u00e9cnicos precisos: <pre><code>\u00bfCu\u00e1l es el presupuesto exacto asignado a formaci\u00f3n profesional por la Generalitat Valenciana para el a\u00f1o actual?\n</code></pre></li> </ol> <p>Discusi\u00f3n guiada:</p> <ul> <li>\"\u00bfPor qu\u00e9 cre\u00e9is que el modelo no puede responder con precisi\u00f3n a estas preguntas?\"</li> <li>\"\u00bfQu\u00e9 consecuencias podr\u00eda tener confiar en estas respuestas en un entorno profesional?\"</li> <li>Explicaci\u00f3n del concepto de \"conocimiento limitado al entrenamiento\" y \"fecha de corte\"</li> </ul>"},{"location":"bedrock/sesion1/#2-bases-de-conocimiento-kb","title":"2. BASES DE CONOCIMIENTO (KB)","text":""},{"location":"bedrock/sesion1/#objetivos_2","title":"Objetivos:","text":"<ul> <li>Comprender qu\u00e9 es una base de conocimiento y su importancia</li> <li>Identificar los componentes necesarios para crear una KB</li> <li>Aprender a preparar documentos para su ingesta</li> </ul> <p>Knowledge Basement (KB): Una base de conocimiento es un repositorio que almacena informaci\u00f3n estructurada y permite a los modelos de IA acceder a datos espec\u00edficos fuera de su entrenamiento original.</p>"},{"location":"bedrock/sesion1/#elementos-clave-explicados","title":"Elementos clave explicados:","text":"<ul> <li>Presentaci\u00f3n: https://aules.edu.gva.es/fp/mod/resource/view.php?id=9716490</li> <li>Fuentes de datos compatibles: \"Bedrock puede procesar PDFs, documentos de texto, HTML, y otros formatos\"</li> <li>Vector store y embeddings: \"Los embeddings son representaciones num\u00e9ricas del significado de un texto\" * Analog\u00eda visual: \"Imaginad una biblioteca donde cada libro est\u00e1 ubicado junto a otros con temas similares, no por orden alfab\u00e9tico\"</li> <li>Chunking: \"Dividimos los documentos en fragmentos manejables para el modelo\"* Pregunta: \"\u00bfPor qu\u00e9 cre\u00e9is que es necesario dividir los documentos en fragmentos m\u00e1s peque\u00f1os?\"</li> <li>Metadatos: \"Informaci\u00f3n adicional que nos ayuda a filtrar y organizar el conocimiento\"</li> </ul>"},{"location":"bedrock/sesion1/#actividad-practica","title":"Actividad pr\u00e1ctica:","text":"<p>Preparaci\u00f3n de documentos:</p> <ul> <li>An\u00e1lisis de documentos de muestra:<ul> <li>Mostrar ejemplos de documentos administrativos de la Generalitat</li> <li>\"Vamos a analizar este documento sobre procedimientos de contrataci\u00f3n p\u00fablica\"</li> </ul> </li> <li>Mejores pr\u00e1cticas para estructurar informaci\u00f3n:<ul> <li>\"Los documentos bien estructurados mejoran la precisi\u00f3n de las respuestas\"</li> <li>Ejemplos de buena versus mala estructuraci\u00f3n </li> <li>Importancia de los t\u00edtulos, subt\u00edtulos y formato consistente</li> </ul> </li> </ul>"},{"location":"bedrock/sesion1/#datosinformacion-estructurada","title":"Datos/informaci\u00f3n estructurada","text":"<p>La informaci\u00f3n estructurada es aquella que sigue un esquema fijo (tablas, campos, tipos bien definidos) y se puede consultar con lenguajes como SQL o JSON.\u200b Ejemplos t\u00edpicos ser\u00edan tablas de una base de datos relacional con columnas como \u201cid_cliente\u201d, \u201cfecha\u201d, \u201cimporte\u201d, o un data warehouse en Redshift con registros de ventas, inventario o m\u00e9tricas num\u00e9ricas.\u200b</p> <p>En Bedrock, las bases de conocimiento ya permiten lanzar consultas en lenguaje natural que se traducen autom\u00e1ticamente a consultas SQL sobre estos or\u00edgenes de datos estructurados, de forma que un usuario puede preguntar \u201cventas del \u00faltimo trimestre por regi\u00f3n\u201d sin escribir SQL.\u200b En JSON, esto se traduce en documentos con una estructura homog\u00e9nea, por ejemplo, registros de clientes, pedidos o logs con claves bien definidas como id, fecha, importe, producto, etc.</p> <p>En resumen, Bedrock puede convertir preguntas en consultas estructuradas (p. ej. SQL) y devolver respuestas precisas sobre esos registros.\u200b</p>"},{"location":"bedrock/sesion1/#datosinformacion-no-estructurada","title":"Datos/informaci\u00f3n no estructurada","text":"<p>La informaci\u00f3n no estructurada no encaja en tablas de filas y columnas, porque no sigue un formato fijo ni un modelo predefinido.\u200b Aqu\u00ed entran documentos largos (PDF, Word), correos, presentaciones, p\u00e1ginas web, contenido multimedia (audio, v\u00eddeo, im\u00e1genes) o mensajes en redes sociales, que suelen almacenarse en sistemas de ficheros, S3, gestores de contenidos, Confluence, SharePoint, etc.\u200b</p> <p>Las bases de conocimiento de Bedrock se conectan a este tipo de or\u00edgenes (por ejemplo, S3, Confluence, Salesforce o SharePoint) y aplican t\u00e9cnicas como RAG: indexan el contenido, lo vectorizan y luego recuperan los fragmentos relevantes para enriquecer las respuestas de los modelos generativos con ese contexto espec\u00edfico.\u200b</p> <ul> <li>Consideraciones ling\u00fc\u00edsticas:<ul> <li>\"Nuestra KB debe manejar documentos en castellano y valenciano\"</li> <li>Pregunta: \"\u00bfQu\u00e9 desaf\u00edos cre\u00e9is que plantea trabajar con documentos biling\u00fces?\"</li> </ul> </li> </ul>"},{"location":"bedrock/sesion1/#creacion-de-una-kb-basica","title":"Creaci\u00f3n de una KB b\u00e1sica:","text":"<ul> <li>Demostraci\u00f3n paso a paso:<ul> <li>Creaci\u00f3n de un data source</li> <li>Configuraci\u00f3n del vector store</li> <li>Selecci\u00f3n de opciones de chunking (tama\u00f1o, solapamiento)</li> <li>Proceso de ingesta con ejemplos visuales</li> </ul> </li> <li>Verificaci\u00f3n:<ul> <li>\"As\u00ed podemos comprobar que nuestra KB se ha creado correctamente\"</li> <li>Demostraci\u00f3n de b\u00fasqueda b\u00e1sica para verificar la ingesta</li> </ul> </li> </ul>"},{"location":"bedrock/sesion1/#3-rag-bedrock-teoria-y-practica","title":"3. RAG BEDROCK: TEOR\u00cdA Y PR\u00c1CTICA","text":""},{"location":"bedrock/sesion1/#objetivos_3","title":"Objetivos:","text":"<ul> <li>Entender el flujo RAG (Retrieval Augmented Generation)</li> <li>Visualizar el proceso de razonamiento del modelo</li> <li>Comprender la memoria conversacional</li> </ul>"},{"location":"bedrock/sesion1/#contenido-teorico","title":"Contenido te\u00f3rico:","text":""},{"location":"bedrock/sesion1/#arquitectura-rag-explicada","title":"Arquitectura RAG explicada:","text":"<ul> <li>Diagrama visual del flujo RAG  </li> <li>\"RAG es como tener una biblioteca y un escritor trabajando juntos\"</li> </ul>"},{"location":"bedrock/sesion1/#explicacion-paso-a-paso","title":"Explicaci\u00f3n paso a paso:","text":"<ol> <li>Recepci\u00f3n de la pregunta: \"El usuario pregunta sobre un tr\u00e1mite espec\u00edfico\"</li> <li>Generaci\u00f3n de embeddings: \"Convertimos la pregunta a su representaci\u00f3n vectorial\"</li> <li>B\u00fasqueda vectorial: \"Buscamos fragmentos similares en nuestra KB\"</li> <li>Ranking: \"Ordenamos los resultados por relevancia\"</li> <li>Inyecci\u00f3n en contexto: \"Proporcionamos al modelo la informaci\u00f3n recuperada\"</li> <li>Generaci\u00f3n: \"El modelo formula una respuesta basada en la informaci\u00f3n proporcionada\"</li> </ol>"},{"location":"bedrock/sesion1/#pregunta-para-la-audiencia","title":"Pregunta para la audiencia:","text":"<ul> <li>\"\u00bfEn qu\u00e9 se diferencia este enfoque de simplemente entrenar un modelo con toda la documentaci\u00f3n?\"</li> </ul>"},{"location":"bedrock/sesion1/#memoria-conversacional","title":"Memoria conversacional:","text":"<ul> <li>\"La memoria permite mantener el contexto de una conversaci\u00f3n\"</li> <li>Ejemplos de conversaciones con y sin memoria</li> </ul>"},{"location":"bedrock/sesion1/#actividad-practica_1","title":"Actividad pr\u00e1ctica:","text":""},{"location":"bedrock/sesion1/#implementacion-de-rag","title":"Implementaci\u00f3n de RAG:","text":"<ul> <li>Configuraci\u00f3n del flujo RAG:<ul> <li>Conexi\u00f3n de la KB creada con el modelo seleccionado</li> <li>Configuraci\u00f3n de par\u00e1metros de b\u00fasqueda (n\u00famero de resultados, umbral de similitud)</li> </ul> </li> <li>Ejemplos de prompts para RAG:</li> <li>Prompt b\u00e1sico para consulta: <pre><code>\u00bfCu\u00e1l es el procedimiento para solicitar una licencia de apertura de negocio seg\u00fan la normativa actual?\n</code></pre></li> <li>Prompt con seguimiento (memoria conversacional): <pre><code>\u00bfQu\u00e9 documentaci\u00f3n necesito presentar para ese tr\u00e1mite?\n</code></pre></li> <li> <p>Prompt con instrucciones espec\u00edficas: <pre><code>Explica paso a paso el proceso de solicitud de subvenciones para empresas. Estructura tu respuesta en forma de lista numerada y destaca los plazos importantes.\n</code></pre> Visualizaci\u00f3n del proceso:</p> </li> <li> <p>Mostrar los fragmentos recuperados</p> </li> <li>Explicar c\u00f3mo se construye el prompt aumentado</li> <li>Analizar la respuesta generada</li> </ul>"},{"location":"bedrock/sesion1/#experimentacion","title":"Experimentaci\u00f3n:","text":"<ul> <li>Comparativa:<ul> <li>\"Vamos a hacer la misma pregunta con y sin KB para ver la diferencia\"</li> <li>An\u00e1lisis de precisi\u00f3n, detalle y fuentes citadas</li> </ul> </li> <li> <p>Ajuste de par\u00e1metros:</p> <ul> <li>Modificar n\u00famero de resultados recuperados</li> <li>Cambiar temperatura del modelo</li> <li>Pregunta: \"\u00bfQu\u00e9 cambios observ\u00e1is al modificar estos par\u00e1metros?\"</li> </ul> </li> <li> <p>Crear un agente y realizar pruebas.</p> <ul> <li>Instrucciones: <pre><code>Eres un asistente educado y cort\u00e9s el cual vas a ayudar tanto a alumnos como profesores. Puedes hablar espa\u00f1ol, valenciano e ingl\u00e9s. Solo puedes hablar del contenido de tu base de conocimientos (knowledge basement).\n</code></pre></li> <li>Prompts de ejemplo: <pre><code>Dime las leyes que se mencionan el documento 6868\n</code></pre></li> </ul> </li> </ul>"},{"location":"bedrock/sesion1/#4-caso-practico-documentacion-administrativa","title":"4. CASO PR\u00c1CTICO: DOCUMENTACI\u00d3N ADMINISTRATIVA","text":""},{"location":"bedrock/sesion1/#objetivos_4","title":"Objetivos:","text":"<ul> <li>Aplicar lo aprendido a un caso real</li> <li>Construir un asistente para documentaci\u00f3n administrativa</li> </ul>"},{"location":"bedrock/sesion1/#actividad","title":"Actividad:","text":"<ul> <li>Presentaci\u00f3n del escenario:<ul> <li>\"Vamos a crear un asistente virtual que ayude a ciudadanos y funcionarios a navegar por los procedimientos administrativos de la Generalitat\"</li> </ul> </li> <li>Implementaci\u00f3n paso a paso:</li> <li>Selecci\u00f3n de documentos:</li> <li>Documentos de procedimientos administrativos</li> <li>Formularios oficiales</li> <li>FAQs existentes</li> <li>Creaci\u00f3n de la KB especializada:</li> <li>Demostraci\u00f3n de la ingesta de estos documentos</li> <li>Configuraci\u00f3n espec\u00edfica para documentaci\u00f3n administrativa</li> <li>Dise\u00f1o de prompts efectivos:</li> </ul> <p>Ejemplo de prompt/sistema para el asistente: <pre><code>Eres un asistente especializado en procedimientos administrativos de la Generalitat Valenciana.\nTu objetivo es proporcionar informaci\u00f3n precisa y actualizada sobre tr\u00e1mites, requisitos y plazos.\nDebes:\n1. Responder con claridad y precisi\u00f3n\n2. Citar la normativa o documento espec\u00edfico de donde extraes la informaci\u00f3n\n3. Indicar claramente cuando no tengas informaci\u00f3n suficiente\n4. Estructurar las respuestas complejas en pasos numerados\n5. Ser respetuoso y profesional en todo momento\n</code></pre> Ejemplos de consultas para probar: <pre><code>\u00bfCu\u00e1les son los requisitos para solicitar una ayuda al alquiler?\n</code></pre></p> <pre><code>Necesito saber el procedimiento para presentar una reclamaci\u00f3n por una multa de tr\u00e1fico.\n</code></pre> <pre><code>\u00bfCu\u00e1les son los plazos para la presentaci\u00f3n del impuesto de sucesiones?\n</code></pre> <ol> <li>Pruebas con escenarios reales:</li> <li>Invitar a los participantes a formular preguntas</li> <li>Analizar la calidad y precisi\u00f3n de las respuestas</li> <li>Pregunta: \"\u00bfQu\u00e9 mejoras podr\u00edamos implementar en este asistente?\"</li> </ol>"},{"location":"bedrock/sesion1/#5-aprendizaje-basado-en-retos","title":"5. Aprendizaje basado en retos","text":""},{"location":"bedrock/sesion1/#reto-1","title":"\u2705 Reto 1","text":"<p>Crear un prompt que genere un plan de marketing para un producto tecnol\u00f3gico.</p>"},{"location":"bedrock/sesion1/#reto-2","title":"\u2705 Reto 2","text":"<p>Implementar un flujo con Bedrock Agents para responder preguntas sobre documentos internos.</p>"},{"location":"bedrock/sesion1/#reto-3","title":"\u2705 Reto 3","text":"<p>Conectar Bedrock con Amazon S3 para usar datos propios en la generaci\u00f3n de respuestas.</p>"},{"location":"bedrock/sesion1/#6-proyecto-integrador-real","title":"6. Proyecto integrador real","text":""},{"location":"bedrock/sesion1/#caso","title":"Caso","text":"<p>Automatizar res\u00famenes de informes t\u00e9cnicos en una empresa manufacturera.</p>"},{"location":"bedrock/sesion1/#arquitectura","title":"Arquitectura","text":"<ul> <li>AWS Lambda + API Gateway + Amazon Bedrock.</li> </ul>"},{"location":"bedrock/sesion1/#flujo","title":"Flujo","text":"<ol> <li>T\u00e9cnico sube informe a S3.</li> <li>Lambda invoca Bedrock para generar resumen.</li> <li>API Gateway expone endpoint para consultar resumen.</li> </ol>"},{"location":"bedrock/sesion1/#codigo-lambda-python","title":"C\u00f3digo Lambda (Python)","text":"<pre><code>import json\nimport boto3\n\ndef lambda_handler(event, context):\n    client = boto3.client('bedrock-runtime')\n    report_text = event['body']\n\n    response = client.invoke_model(\n        modelId=\"anthropic.claude-v2\",\n        body={\"input\": f\"Resume el siguiente informe t\u00e9cnico: {report_text}\"}\n    )\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps({'summary': response['body']})\n    }\n</code></pre>"},{"location":"bedrock/sesion1/#resultado-esperado","title":"Resultado esperado","text":"<ul> <li>Endpoint <code>/summarize</code> devuelve resumen en segundos.</li> <li>Beneficio: reduce tiempo de an\u00e1lisis en un 70%.</li> </ul>"},{"location":"bedrock/sesion1/#7-recursos-adicionales","title":"7. Recursos adicionales","text":"<ul> <li>Documentaci\u00f3n oficial</li> <li>Gu\u00eda r\u00e1pida AWS</li> </ul>"},{"location":"hf/","title":"Apuntes y ejercicios pr\u00e1cticas del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data","text":"<p>Apuntes, pr\u00e1cticas y ejercicios del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data en el IES Severo Ochoa (Elche).</p> <p>Los ejercicios y conocimientos contenidos en las pr\u00e1cticas y/o apuntes de  todos los m\u00f3dulos tienen exclusivamente prop\u00f3sito formativo, por lo que  nunca se deber\u00e1n utilizar con fines maliciosos o delictivos.</p> <p>Ning\u00fan alumno o alumna de este curso, ni profesor o profesora como  docentes, ser\u00e1n responsables de los da\u00f1os directos o indirectos que  pudieran derivarse del uso inadecuado de las herramientas y mecanismos  expuestos.</p>"},{"location":"hf/Actividades/","title":"Actividades","text":""},{"location":"hf/Actividades/#actividades","title":"Actividades","text":"<ol> <li>Estimaci\u00f3n de profundidad Utiliza el pipeline:</li> </ol> <pre><code>from transformers import pipeline\n\ndepth = pipeline(\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n\nresult = depth(\"ruta_o_url_imagen\")\n</code></pre> <ol> <li> <p>Clasificaci\u00f3n de im\u00e1genes Usa el pipeline: <pre><code>from transformers import pipeline\nclassifier = pipeline(\"image-classification\")\nres = classifier(\"ruta_o_url_imagen\")\n\nprint(res)\n</code></pre></p> </li> <li> <p>Avanzado (Optativo): Integrar clasificaci\u00f3n y segmentaci\u00f3n </p> </li> </ol> <p>Ejecuta ambos pipelines y visualiza el resultado conjunto.</p>"},{"location":"hf/Datasets/","title":"\ud83d\udcd8 Hugging Face Datasets: Gu\u00eda + Reto Gamificado","text":""},{"location":"hf/Datasets/#1-introduccion","title":"1\ufe0f\u20e3 Introducci\u00f3n","text":"<p>El paquete <code>datasets</code> de Hugging Face es una potente herramienta para acceder, compartir y procesar conjuntos de datos (datasets) de IA para una amplia gama de tareas, que incluyen:</p> <ul> <li>Procesamiento del Lenguaje Natural (PLN)</li> <li>Visi\u00f3n por computadora</li> <li>Procesamiento de audio</li> </ul> <p>Est\u00e1 dise\u00f1ado para manejar grandes vol\u00famenes de datos de manera eficiente mediante el uso de mapeo de memoria y el formato Apache Arrow, lo que permite trabajar con datos que superan la RAM disponible.</p> <p>Arrow Apache Arrow define un formato de memoria columnar independiente del lenguaje para datos planos y anidados, organizado para operaciones anal\u00edticas eficientes en hardware moderno como CPU y GPU. El formato de memoria Arrow tambi\u00e9n admite lecturas sin copia para un acceso a datos ultrarr\u00e1pido sin sobrecarga de serializaci\u00f3n.</p> <p>El proyecto del formato Apache Arrow comenz\u00f3 en febrero de 2016, centr\u00e1ndose en cargas de trabajo de an\u00e1lisis columnar en memoria. A diferencia de formatos de archivo como Parquet o CSV, que especifican c\u00f3mo se organizan los datos en el disco, Arrow se centra en c\u00f3mo se organizan los datos en la memoria.</p> <p>Los creadores buscan consolidar Arrow como un formato est\u00e1ndar en memoria para el an\u00e1lisis de cargas de trabajo. Estos fundamentos atraen a numerosos colaboradores de proyectos como Pandas, Spark, Cassandra, Apache Calcite, Dremio e Ibis.</p>"},{"location":"hf/Datasets/#caracteristicas-clave","title":"\ud83d\udd11 Caracter\u00edsticas Clave","text":"<ul> <li>Vasto Repositorio (Hub): Gran cantidad de datasets p\u00fablicos y privados.</li> <li>F\u00e1cil Acceso: Carga en una sola l\u00ednea de c\u00f3digo.</li> <li>Procesamiento Eficiente: M\u00e9todos como <code>map()</code> paralelizados.</li> <li>Escalabilidad: Objetos <code>Dataset</code> y <code>IterableDataset</code>.</li> <li>Gesti\u00f3n de Datos: Crear y subir datasets propios al Hub.</li> </ul>"},{"location":"hf/Datasets/#instalacion","title":"\u2699\ufe0f Instalaci\u00f3n","text":"<pre><code>pip install datasets\npip install datasets[audio]\npip install datasets[vision]\n</code></pre>"},{"location":"hf/Datasets/#ejemplo-cargar-un-dataset-local","title":"\ud83e\udde9 Ejemplo: Cargar un dataset local","text":"<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Salida esperada: <pre><code>DatasetDict({\n    train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 })\n})\n</code></pre></p>"},{"location":"hf/Datasets/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"hf/Datasets/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p>"},{"location":"hf/Datasets/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"hf/Datasets/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\ndataset = load_dataset(\"PlanTL-GOB-ES/squad-es\")\nprint(dataset)\n\n# Nivel 2: Dividir en train/test\ntrain_dataset = dataset[\"train\"]\nsplit_dataset = train_dataset.train_test_split(test_size=0.2)\ntrain_split = split_dataset[\"train\"]\ntest_split = split_dataset[\"test\"]\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\ndef add_num_paragraphs(example):\n    return {\"num_paragraphs\": len(example[\"paragraphs\"])}\ntrain_split = train_split.map(add_num_paragraphs)\n\n# Nivel 4: Filtrar y persistir\nfiltered_train = train_split.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nfiltered_train = filtered_train.remove_columns([\"num_paragraphs\"])\nfiltered_train.to_parquet(\"filtered_train.parquet\")\n\n# Nivel 5: Publicar en Hugging Face\n# filtered_train.push_to_hub(\"usuario/nombre-dataset\")\n</code></pre>"},{"location":"hf/Datasets/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"hf/Ejemplo_gradio/","title":"Aplicaci\u00f3n web Gradio + Modelo previamente entrenado","text":"<p>Vamos a crear una aplicaci\u00f3n web con Gradio que use el modelo creado en una sesi\u00f3n anterior: \u200bomarques/autotrain-dogs-and-cats-1527055142</p> <p>Ejemplo de aplicaci\u00f3n Gradio con una imagen de entrada y un Label como componente de salida: </p> <p>Etiquetado de la imagen de entrada: </p>"},{"location":"hf/Ejemplo_gradio/#codigo-de-ejemplo","title":"C\u00f3digo de ejemplo","text":"<pre><code>import gradio as gr\u200b\n\u200b\ndef image_classifier(inp):\u200b\n\u2003 return {'cat': 0.3, 'dog': 0.7}\u200b\n\ndemo = gr.Interface(fn=image_classifier, inputs='image', outputs='label')\u200b\n\ndemo.launch()\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#anadimos-el-modelo","title":"A\u00f1adimos el modelo","text":"<pre><code>import gradio as gr\u200b\nfrom transformers import pipeline\u200b\n\u200b\ndef image_classifier(inp):\u200b\n   pipe = pipeline(\"image-classification\", model=\"omarques/autotrain-dogs-and-cats-1527055142\")\u200b\n   pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\")\u200b\n   return {'cat': 0.3, 'dog': 0.7}\u200b\n\u200b\ndemo = gr.Interface(fn=image_classifier, inputs='image', outputs='label')\u200b\n\n\u200bdemo.launch()\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#instalamos-las-librerias-transformers-y-torch","title":"Instalamos las librer\u00edas transformers y torch\u200b","text":"<p><pre><code>pip install transformers torch\u200b\n</code></pre> Volvemos a probar y comprobamos que funciona correctamente.</p> <p>Dentro del componente Image, por defecto Gradio pasa un objeto tipo <code>numpy.ndarray</code> (la imagen como matriz) a las funciones de Gradio, por lo que debemos especificar el tipo con <code>gr.Image(type=\"filepath\")</code> en la creaci\u00f3n de la interfaz de Gradio\u200b. \u200b</p>"},{"location":"hf/Ejemplo_gradio/#especificar-el-tipo-en-image","title":"Especificar el tipo en Image","text":"<pre><code>demo = gr.Interface(fn=image_classifier, inputs=gr.Image(type=\"filepath\"), outputs='label')\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#codigo-actualizado","title":"C\u00f3digo actualizado","text":"<pre><code>import gradio as gr\u200b\nfrom transformers import pipeline\u200b\n\ndef image_classifier(inp):\u200b\n   #/tmp/gradio/b7be1455904a47b7fb3d953514163c828cc46e093fe7ba8bdeb950039a8e870e/1.png\u200b\n   print(inp) \n   pipe = pipeline(\"image-classification\", model=\"omarques/autotrain-dogs-and-cats-1527055142\")\u200b\n   #[{'label': 'cat', 'score': 0.6151219010353088}, {'label': 'dog', 'score': 0.38487812876701355}]\u200b\n   print(pipe(inp)) \n   return {'cat': 0.3, 'dog': 0.7}\u200b\n\ndemo = gr.Interface(fn=image_classifier, inputs=gr.Image(type=\"filepath\"), outputs='label')\u200b\n\ndemo.launch()\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#de-lista-a-diccionario-para-el-output-label","title":"De lista a diccionario para el output label","text":"<p>Para convertir la lista <code>[{'label': 'cat', 'score': 0.6151219010353088}, {'label': 'dog', 'score': 0.38487812876701355}]\u200b</code> al formato requerido por el componente de salida Label <code>(dict[str, float])</code> de Gradio, debemos crear un diccionario donde las claves son los valores de <code>label</code> y los valores son los correspondientes a <code>score</code>\u200b</p> <p>Ejemplo sencillo:\u200b <pre><code>lista = [\u200b\n\u2003\u2003{'label': 'cat', 'score': 0.6151219010353088},\u200b\n\u2003 {'label': 'dog', 'score': 0.38487812876701355}\u200b\n]\u200b\nresultado = {d['label']: d['score'] for d in lista}\u200b\u200b\n# Resultado: {'cat': 0.8, 'dog': 0.2}\u200b\nprint(resultado)\u200b\n</code></pre></p>"},{"location":"hf/Ejemplo_gradio/#codigo-final","title":"C\u00f3digo final","text":"<p><pre><code>import gradio as gr\u200b\nfrom transformers import pipeline\u200b\n\u200b\ndef image_classifier(inp):\u200b\n   pipe = pipeline(\"image-classification\", model=\"omarques/autotrain-dogs-and-cats-1527055142\")\u200b\n   #[{'label': 'cat', 'score': 0.6151219010353088}, {'label': 'dog', 'score': 0.38487812876701355}]\u200b\n   etiquetas = pipe(inp)\n   #[{'cat': 0.6151219010353088}, {'dog': 0.38487812876701355}]\u200b\n   resultado = {d['label']: d['score'] for d in etiquetas} \n   return resultado\u200b\n\ndemo = gr.Interface(fn=image_classifier, inputs=gr.Image(type=\"filepath\"), outputs='label')\u200b\n\ndemo.launch()\u200b\n</code></pre> \u200b</p> <p>\u200b</p>"},{"location":"hf/Referencias/","title":"\ud83d\udcce Referencias:","text":"<ul> <li>Tasks de HuggingFace</li> <li>Apuntes de HuggingFace elaborados por Aitor Medrano</li> </ul>"},{"location":"hf/Tasks_vc/","title":"Tasks de Hugging face relacionadas con la Visi\u00f3n por computador","text":""},{"location":"hf/Tasks_vc/#objetivos","title":"Objetivos","text":"<ul> <li>Diferenciar qu\u00e9 es un \"task\" en Machine Learning seg\u00fan Hugging Face.</li> <li>Aprender los conceptos y ejemplos de estimaci\u00f3n de profundidad, clasificaci\u00f3n y segmentaci\u00f3n de im\u00e1genes.</li> <li>Probar ejemplos pr\u00e1cticos con pipelines de Hugging Face.</li> </ul> <p>Hugging Face es el portal para todas las tareas de aprendizaje autom\u00e1tico. Aqu\u00ed encontraremos todo lo necesario para empezar con una tarea: demostraciones, casos de uso, modelos, conjuntos de datos y mucho m\u00e1s.</p>"},{"location":"hf/Tasks_vc/#que-es-un-task","title":"\u00bfQu\u00e9 es un task?","text":"<p>Un task en Hugging Face define el tipo de problema que un modelo est\u00e1 dise\u00f1ado para resolver. Esta clasificaci\u00f3n facilita la b\u00fasqueda, prueba y reutilizaci\u00f3n de modelos seg\u00fan la tarea espec\u00edfica que se desea abordar. Tasks (tareas) en Hugging Face </p>"},{"location":"hf/Tasks_vc/#uso-de-hugging-face-para-tareas-de-vision-por-computadora","title":"Uso de Hugging Face para tareas de Visi\u00f3n por Computadora","text":"<p>Hugging Face tambi\u00e9n proporciona una amplia colecci\u00f3n de modelos preentrenados para tareas de visi\u00f3n artificial. Con todos estos modelos alojados previamente entrenados, podemos crear aplicaciones interesantes que detectan objetos en im\u00e1genes, la edad de una persona y m\u00e1s. En este tema, aprenderemos a realizar las primeras cuatro tareas utilizando modelos de Hugging Face. </p>"},{"location":"hf/Tasks_vc/#1-clasificacion-de-imagenes-image-classification","title":"1. Clasificaci\u00f3n de Im\u00e1genes (Image Classification)","text":"<p>La clasificaci\u00f3n de im\u00e1genes es una tarea de visi\u00f3n por computador que consiste en asignar una o varias etiquetas predefinidas a una imagen, seg\u00fan su contenido. </p>"},{"location":"hf/Tasks_vc/#ejemplos-de-aplicaciones","title":"Ejemplos de aplicaciones","text":"<ul> <li>Diagn\u00f3stico m\u00e9dico: clasificaci\u00f3n de radiograf\u00edas para detectar enfermedades.</li> <li>Reconocimiento de objetos</li> <li>Clasificaci\u00f3n de productos en e-commerce</li> <li>Moderaci\u00f3n de contenido visual</li> </ul>"},{"location":"hf/Tasks_vc/#modelos-disponibles-en-hugging-face","title":"Modelos disponibles en Hugging Face","text":"<p>Hugging Face ofrece m\u00faltiples modelos preentrenados para clasificaci\u00f3n de im\u00e1genes. Estos modelos han sido entrenados con grandes conjuntos de datos, como ImageNet, lo que les permite reconocer una amplia variedad de objetos y escenas. Algunos destacados:</p> Modelo Arquitectura Dataset de entrenamiento Enlace <code>google/vit-base-patch16-224</code> Vision Transformer (ViT) ImageNet \ud83d\udd17 Ver modelo <code>microsoft/resnet-50</code> ResNet-50 ImageNet \ud83d\udd17 Ver modelo <code>facebook/deit-base-patch16-224</code> DeiT ImageNet \ud83d\udd17 Ver modelo"},{"location":"hf/Tasks_vc/#quick-draw-de-google","title":"\"Quick, Draw!\" de Google","text":"<p>Quick, Draw! es un juego basado en aprendizaje autom\u00e1tico en el que una red neuronal intenta adivinar el objeto que el usuario est\u00e1 dibujando. Evidentemente, no siempre funciona; pero cuanto m\u00e1s tiempo pasemos jugando, m\u00e1s aprender\u00e1. Destacar que ya reconoce cientos de conceptos y esperan poder a\u00f1adir m\u00e1s en el futuro. El gran objetivo de esta aplicaci\u00f3n, es mostrar un ejemplo de c\u00f3mo se puede usar el aprendizaje autom\u00e1tico de forma divertida. </p> <p>Caracter\u00edsticas clave</p> <ul> <li> <p>Juego con IA: El juego es un experimento de aprendizaje autom\u00e1tico. El jugador dibuja y la red neuronal intenta adivinar el dibujo en tiempo real.</p> </li> <li> <p>Aprendizaje continuo: La IA aprende de cada dibujo, mejorando su capacidad para adivinar correctamente en el futuro. Esto ayuda a Google a recopilar uno de los conjuntos de datos de garabatos m\u00e1s grandes del mundo para la investigaci\u00f3n en aprendizaje autom\u00e1tico.</p> </li> <li> <p>Mec\u00e1nica simple: El juego es similar al Pictionary. Consiste en seis rondas, y en cada una se nos pide dibujar un objeto diferente en 20 segundos. Al final, podemos ver nuestros dibujos y los resultados.</p> </li> </ul> <p>Podemos acceder al juego en el sitio web oficial: Web oficial. </p> <p>Importancia de los datos - BigData</p> <p>Los datos recopilados en Quick, Draw! son fundamentales para el Big Data y el aprendizaje autom\u00e1tico, ya que constituyen el conjunto de datos de garabatos m\u00e1s grande del mundo, esencial para entrenar y mejorar modelos de IA.  Su importancia radica en varios puntos clave:</p> <ul> <li> <p>Entrenamiento de IA: Los millones de dibujos (actualmente m\u00e1s de 50 millones en 345 categor\u00edas) sirven como un vasto corpus de datos para entrenar redes neuronales, ense\u00f1\u00e1ndoles a reconocer e interpretar garabatos de formas muy diversas. La IA aprende a identificar patrones visuales, sin importar el estilo individual del dibujante.</p> </li> <li> <p>Diversidad y variabilidad: A diferencia de conjuntos de datos de im\u00e1genes tradicionales, los garabatos muestran una enorme variabilidad en c\u00f3mo las personas de diferentes culturas y con distintas habilidades dibujan un mismo objeto. Esta diversidad es crucial para crear modelos de IA m\u00e1s robustos y menos sesgados que puedan funcionar globalmente.</p> </li> <li> <p>Datos en tiempo real y secuenciales: Los dibujos se capturan como series temporales de posiciones del l\u00e1piz (vectores con marca de tiempo), no solo como im\u00e1genes est\u00e1ticas. Esto permite a los investigadores comprender no solo el resultado final, sino tambi\u00e9n el proceso de dibujo (qu\u00e9 trazo se hizo primero, en qu\u00e9 direcci\u00f3n), lo cual es valioso para desarrollar modelos de IA m\u00e1s avanzados, como el modelo Sketch-RNN (Recurrent Neural Network para Bocetos es un modelo generativo de aprendizaje autom\u00e1tico desarrollado por David Ha y Douglas Eck en Google Brain, que es capaz de crear, completar y manipular bocetos vectoriales de objetos comunes)</p> </li> <li> <p>Investigaci\u00f3n abierta: Google ha hecho p\u00fablico este conjunto de datos para que investigadores de todo el mundo puedan utilizarlo en sus propios proyectos y estudios de aprendizaje autom\u00e1tico, fomentando la innovaci\u00f3n en el campo.</p> </li> <li> <p>Ejemplo de gamificaci\u00f3n para la recolecci\u00f3n de datos: El juego es un excelente ejemplo de c\u00f3mo la gamificaci\u00f3n puede motivar a un gran n\u00famero de usuarios a generar datos valiosos de forma divertida y a gran escala, un desaf\u00edo com\u00fan en el Big Data.</p> </li> </ul> <p>Datos de entrenamiento</p> <p></p> <p>En esta p\u00e1gina podemos ver, en el momento en el que se redactaban estos apuntes, 126.372 dibujos de pelotas de baloncesto hechas por personales reales...en Internet. Incluso, podemos ver los trazos que han realizado estas personas hasta que el modelo ha sido capaz de adivinar el dibujo.  Destacar la importancia del Big Data, ya que, los datos de entrenamiento son muy importantes para cualquier modelo de aprendizaje. </p> <p>Datos de entrenamiento para la pelota de baloncesto</p> <p></p>"},{"location":"hf/Tasks_vc/#desarrollo-de-nuestro-propio-pictionary-con-gradio","title":"Desarrollo de nuestro propio Pictionary con Gradio","text":"<p>Vamos a desarrollar nuestra propia aplicaci\u00f3n Pictionary utilizando Gradio, basada en el siguiente v\u00eddeo: https://www.youtube.com/watch?v=LS9Y2wDVI0k</p> <p>Todos los ficheros se encuentran en el siguiente espacio de Hugging Face: https://huggingface.co/spaces/nateraw/quickdraw</p> <p>Lo primero que debemos es, descargar los ficheros siguientes: <code>class_names.txt</code>, <code>pytorch_model.bin</code> y <code>app.py</code></p> <p>Analizamos el c\u00f3digo elaborado por el usuario:</p> <pre><code>from pathlib import Path  \nimport torch             \n\nimport gradio as gr       \nfrom torch import nn      \n\n# Lee las etiquetas/clases del archivo de texto, una por l\u00ednea. \n# Cada l\u00ednea es una categor\u00eda que el modelo puede predecir.\nLABELS = Path('class_names.txt').read_text().splitlines()\n\n# Definimos la arquitectura de la red neuronal convolucional (CNN) ya entrenada:\nmodel = nn.Sequential(\n    # Primera capa: 1 canal de entrada, 32 filtros, tama\u00f1o de filtro 3x3\n    nn.Conv2d(1, 32, 3, padding='same'),  \n    # Funci\u00f3n de activaci\u00f3n no lineal ReLU (acelera y facilita el aprendizaje)\n    nn.ReLU(),                            \n    # Max Pooling: reduce la resoluci\u00f3n espacial de las caracter\u00edsticas\n    #  (comprime la imagen a la vez que mantiene zonas m\u00e1s \u201cactivas\u201d)\n    nn.MaxPool2d(2),                      \n    nn.Conv2d(32, 64, 3, padding='same'), # Segunda capa: 32\u219264 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),# Tercera capa: 64\u2192128 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    # Aplana los datos resultantes para prepararlos para las capas densas \n    # (total elementos = 128 canales * 3 * 3)\n    nn.Flatten(),                         \n    # Capa totalmente conectada: de 1152 (productos anteriores) a 256 neuronas\n    nn.Linear(1152, 256),                 \n    nn.ReLU(),\n    # Capa de salida: 1 neurona por clase del archivo de etiquetas\n    nn.Linear(256, len(LABELS)),          \n)\n# Carga los pesos entrenados previamente desde \n# el archivo binario (estado del modelo)\nstate_dict = torch.load('pytorch_model.bin', map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\n# Coloca el modelo en modo \"solo inferencia\" \n# (no entrenamiento): no calcula gradientes ni actualiza pesos\nmodel.eval() \n\n# Funci\u00f3n de predicci\u00f3n principal: toma una imagen (array) \n# y devuelve las top-5 categor\u00edas con su probabilidad\ndef predict(im):\n    # Convierte el array de la imagen en un tensor, escala los valores a rango [0,1] \n    # y a\u00f1ade dimensiones de batch y canal\n    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.\n\n    # Desactiva el c\u00e1lculo de gradientes (m\u00e1s r\u00e1pido, no entrena)\n    with torch.no_grad():            \n        # Hacemos pasar la imagen por el modelo (forward pass)\n        out = model(x)               \n\n    # Calcula las probabilidades (softmax)\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)  \n\n    # Obtiene las 5 clases m\u00e1s probables\n    values, indices = torch.topk(probabilities, 5)              \n\n    # Devuelve un diccionario {clase: probabilidad} para las 5 mejores\n    return {LABELS[i]: v.item() for i, v in zip(indices, values)}\n\n# Creamos la interfaz web con Gradio:\n#   - predict: funci\u00f3n a ejecutar al recibir la entrada.\n#   - inputs: 'sketchpad', una zona para que el usuario dibuje a mano alzada.\n#   - outputs: 'label', salida tipo clasificaci\u00f3n de etiquetas.\n#   - live=True: muestra predicciones en tiempo real mientras dibujas.\ninterface = gr.Interface(predict, inputs='sketchpad', outputs='label', live=True)\n\ninterface.launch(debug=True)\n</code></pre>"},{"location":"hf/Tasks_vc/#que-es-una-red-neuronal-convolucional-cnn","title":"\u00bfQu\u00e9 es una red neuronal convolucional (CNN)?","text":"<p>Una red neuronal convolucional (CNN, por sus siglas en ingl\u00e9s, Convolutional Neural Network) es un tipo de red neuronal artificial especialmente dise\u00f1ada para procesar datos que tienen una estructura en forma de cuadr\u00edcula, como im\u00e1genes, audio o v\u00eddeo.</p>"},{"location":"hf/Tasks_vc/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ul> <li> <p>Inspiraci\u00f3n biol\u00f3gica:: Las CNNs se inspiran en la corteza visual de los mam\u00edferos. Primero detectan reglas simples (l\u00edneas, bordes) y despu\u00e9s patrones m\u00e1s complejos (formas, objetos).</p> </li> <li> <p>Arquitectura en capas:   Una CNN est\u00e1 compuesta por diferentes capas conectadas:</p> <ul> <li>Capas convolucionales: Aplican filtros o \u201ckernels\u201d para extraer patrones y caracter\u00edsticas locales (bordes, texturas, esquinas).</li> <li>Capas de activaci\u00f3n (ReLU): Introducen no linealidad, permitiendo que la red aprenda funciones m\u00e1s complejas.</li> <li>Capas de agrupamiento (pooling): Reducen la resoluci\u00f3n espacial y la cantidad de computaci\u00f3n, logrando robustez ante desplazamientos.</li> <li>Capas totalmente conectadas: Integran toda la informaci\u00f3n para tomar decisiones y realizar la predicci\u00f3n final.</li> </ul> </li> <li> <p>Aprendizaje jer\u00e1rquico:   Las CNNs aprenden jerarqu\u00edas de caracter\u00edsticas: Las primeras capas detectan elementos simples, las siguientes combinan estos elementos y las \u00faltimas reconocen patrones complejos y abstractos.</p> </li> <li> <p>Campos receptivos y par\u00e1metros compartidos: Los filtros se aplican en toda la imagen usando los mismos par\u00e1metros, lo que permite detectar el mismo patr\u00f3n en distintas posiciones. As\u00ed, el n\u00famero de par\u00e1metros y el coste de memoria disminuyen en comparaci\u00f3n con una red completamente conectada.</p> </li> </ul>"},{"location":"hf/Tasks_vc/#aplicaciones-tipicas","title":"Aplicaciones t\u00edpicas","text":"<ul> <li>Reconocimiento y clasificaci\u00f3n de im\u00e1genes: Detecci\u00f3n de objetos, diagn\u00f3stico m\u00e9dico, moderaci\u00f3n de contenido, etc.</li> <li>Visi\u00f3n por computador: Conducci\u00f3n aut\u00f3noma, videovigilancia, an\u00e1lisis de tr\u00e1fico.</li> <li>Procesamiento de v\u00eddeo: Reconocimiento de acciones, seguimiento de objetos en secuencias de im\u00e1genes, an\u00e1lisis deportivo.</li> </ul>"},{"location":"hf/Tasks_vc/#ejemplo-didactico-sencillo","title":"Ejemplo did\u00e1ctico sencillo","text":"<p>Cuando pasas una imagen por una CNN:</p> <ul> <li>Las primeras capas detectan bordes y formas sencillas.</li> <li>Las siguientes detectan partes m\u00e1s grandes (ruedas, patas, ojos).</li> <li>Al final, la red puede identificar el objeto completo (ej. \u201cbicicleta\u201d, \u201cgato\u201d, \u201cpersona\u201d) en la imagen.</li> </ul> <p>Como hemos comprobado en el ejemplo, el c\u00f3digo desarrollado por el usuario no funciona actualmente, por lo que debemos realizar algunas mejoras para que el c\u00f3digo original funcione. A continuaci\u00f3n podemos visualizar la soluci\u00f3n final:</p> <pre><code>from pathlib import Path\nfrom PIL import Image\nfrom torch import nn\n\nimport torch\nimport gradio as gr\nimport numpy as np\n\n# Leemos las etiquetas de clases (categor\u00edas) desde un fichero de texto\nLABELS = Path('class_names.txt').read_text().splitlines()\n\n# Definimos la arquitectura de la red neuronal convolucional (CNN) ya entrenada:\nmodel = nn.Sequential(\n    # Primera capa: 1 canal de entrada, 32 filtros, tama\u00f1o de filtro 3x3\n    nn.Conv2d(1, 32, 3, padding='same'),  \n    # Funci\u00f3n de activaci\u00f3n no lineal ReLU (acelera y facilita el aprendizaje)\n    nn.ReLU(),                            \n    # Max Pooling: reduce la resoluci\u00f3n espacial de las caracter\u00edsticas \n    # (comprime la imagen a la vez que mantiene zonas m\u00e1s \u201cactivas\u201d)\n    nn.MaxPool2d(2),                      \n    nn.Conv2d(32, 64, 3, padding='same'), # Segunda capa: 32\u219264 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),# Tercera capa: 64\u2192128 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    # Aplana los datos resultantes para prepararlos para las capas\n    # densas (total elementos = 128 canales * 3 * 3)\n    nn.Flatten(),                         \n    # Capa totalmente conectada: de 1152 (productos anteriores) \n    # a 256 neuronas\n    nn.Linear(1152, 256),                 \n    nn.ReLU(),\n    # Capa de salida: 1 neurona por clase del archivo de etiquetas\n    nn.Linear(256, len(LABELS)),          \n)\n# Cargamos los pesos previamente entrenados del modelo\nstate_dict = torch.load('pytorch_model.bin', map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\n# Ponemos el modelo en modo inferencia (no entrenamiento)\nmodel.eval()  \n\n# Funci\u00f3n principal de predicci\u00f3n, procesar\u00e1 el dibujo \n# de Gradio y calcular\u00e1 su clase\ndef predict(img):   \n    # Si no hay dibujo o la clave 'composite' no existe o est\u00e1 vac\u00eda, avisamos:\n    if img is None or \"composite\" not in img or img[\"composite\"] is None:\n        return {\"Por favor, dibuja algo\": 1.0}\n    # Extraemos la imagen resultado del canvas, canal RGBA\n    # Array con forma (ej. [800, 800, 4]), tipo uint8\n    arr = img[\"composite\"]        \n    # Convertimos de RGBA a escala de grises (Quick Draw es gris)\n    arr_gray = arr[..., :3].mean(axis=2)\n    # Convertimos a uint8 por si PIL lo necesita\n    arr_gray_uint8 = arr_gray.astype(\"uint8\")\n    # Redimensionamos a 28x28 p\u00edxeles (tama\u00f1o de entrada del modelo)\n    arr_img = Image.fromarray(arr_gray_uint8)\n    arr_resized = np.array(arr_img.resize((28, 28), resample=Image.BILINEAR))\n    # Escalamos a rango [0,1]\n    arr_normalized = arr_resized / 255.0\n    # A\u00f1adimos dimensiones de batch y canal: (1, 1, 28, 28)\n    x = torch.tensor(arr_normalized, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n    # Ejecutamos inferencia sin calcular gradientes (m\u00e1s eficiente)\n    with torch.no_grad():\n        out = model(x)\n    # Calculamos probabilidades con softmax\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n    # Obtenemos las 5 clases m\u00e1s probables (top-5)\n    values, indices = torch.topk(probabilities, 5)\n    # Devolvemos un diccionario: categor\u00eda : probabilidad (~confianza)\n    return {LABELS[i]: v.item() for i, v in zip(indices, values)}\n\n# Creamos la interfaz Gradio:\n# - El input es un sketchpad (zona para dibujar)\n# - El output son etiquetas: las categor\u00edas predecidas\n# - live=True: actualiza la predicci\u00f3n en tiempo real al dibujar\ndemo = gr.Interface(\n    predict,     \n    inputs='sketchpad',\n    outputs='label', \n    live=True)\n\ndemo.launch(share=True)\n</code></pre> <p>La funci\u00f3n softmax de torch (PyTorch) es una operaci\u00f3n matem\u00e1tica que transforma un vector de valores reales \u2014normalmente llamados \"logits\"\u2014 en una distribuci\u00f3n de probabilidades sobre diferentes clases, donde todos los elementos resultantes est\u00e1n entre 0 y 1 y la suma es exactamente 1. Por ejemplo, si tu modelo clasifica im\u00e1genes en tres clases, la salida softmax ser\u00e1 un vector con tres valores que representan la probabilidad atribuida a cada clase.\u200b</p> <p>En PyTorch, podemos usar esta funci\u00f3n tanto como capa de activaci\u00f3n en la salida de nuestro modelo, como directamente llamando torch.nn.functional.softmax() sobre un tensor de logits. Es com\u00fan utilizar softmax en la inferencia para obtener probabilidades interpretables, mientras que durante el entrenamiento suele usarse CrossEntropyLoss, que incorpora la softmax de forma interna y m\u00e1s eficiente.\u200b</p> <p>Aplicaciones comunes:</p> <ul> <li>Clasificaci\u00f3n multiclase: transforma las salidas del modelo en probabilidades para cada categor\u00eda.\u200b</li> <li>Visualizaci\u00f3n de la confianza del modelo en cada posible resultado. En resumen, softmax convierte los resultados num\u00e9ricos en probabilidades \u00fatiles para tomar decisiones y analizar resultados en Deep learning.</li> </ul>"},{"location":"hf/Tasks_vc/#actividad-1-usar-un-space-de-hugging-face","title":"\ud83d\udcdd Actividad 1. Usar un Space de Hugging Face","text":"<p>Bas\u00e1ndote en lo aprendido a partir de los casos de uso de Hola Spaces y Hola Spaces 2.0 trabajadas en una sesi\u00f3n anterior, mediante Gradio en Hugging Face crea un nuevo espacio p\u00fablico en tu cuenta que permita probar la aplicaci\u00f3n del pictionary desarrollada de forma local en un Space de Hugging Face. </p> <p>Entrega la url del espacio y algunas capturas de pantalla usando la aplicaci\u00f3n. </p>"},{"location":"hf/Tasks_vc/#2-deteccion-de-objetos","title":"2. Detecci\u00f3n de objetos","text":"<p>La detecci\u00f3n de objetos predice la distancia de cada p\u00edxel respecto a la c\u00e1mara usando solo una imagen. Es una t\u00e9cnica fundamental en visi\u00f3n computacional que permite identificar y localizar instancias de objetos definidos dentro de im\u00e1genes. Es ampliamente utilizada en aplicaciones como conducci\u00f3n aut\u00f3noma, seguimiento de objetos en deportes, b\u00fasqueda de im\u00e1genes y conteo de objetos en diferentes escenarios. </p> <p>Hugging Face alberga varios modelos que han sido entrenados previamente para detectar objetos en im\u00e1genes. Podemos ver una lista de modelos en https://huggingface.co/models?pipeline_tag=object-detection&amp;sort=trending </p> <p>En la figura siguiente podemos visualizar un listado de la categor\u00eda Object Detection:</p> <p></p> <p>Ejemplo de uso del modelo facebook/detr-resnet-50 para la detecci\u00f3n de objetos:</p> <p></p> <p>Podemos probar el modelo directamente utilizando la API de inferencia alojada en Hugging Face. Para ello, usaremos una imagen de una oficina con algunas mujeres: </p> <p>Fuente: https://en.wikipedia.org/wiki/Office#/media/File:Good_Smile_Company_offices_ladies.jpg </p> <p>Al arrastrar y soltar la imagen en la secci\u00f3n \"Inference API\" alojada en la p\u00e1gina del modelo en Hugging Face, veremos la lista de objetos detectados, as\u00ed como sus probabilidades correspondientes: </p> <p>Al pasar el rat\u00f3n por encima del nombre de un objeto detectado, la imagen resalta el cuadro delimitador del objeto seleccionado.</p>"},{"location":"hf/Tasks_vc/#algunos-modelos-disponibles-en-hugging-face","title":"Algunos modelos disponibles en Hugging Face","text":"<p>Hugging Face ofrece modelos preentrenados que permiten realizar detecci\u00f3n de objetos sin necesidad de entrenamiento adicional.</p> Modelo Arquitectura Dataset Enlace <code>facebook/detr-resnet-50</code> DETR (DEtection TRansformer) COCO \ud83d\udd17 Ver modelo <code>hustvl/yolos-small</code> YOLOS (Vision Transformer) COCO \ud83d\udd17 Ver modelo"},{"location":"hf/Tasks_vc/#principales-aplicaciones","title":"Principales Aplicaciones","text":"<ul> <li>Conducci\u00f3n aut\u00f3noma: Los coches sin conductor usan la detecci\u00f3n de objetos para reconocer peatones, bicicletas, sem\u00e1foros y se\u00f1ales de tr\u00e1fico, ayudando a la toma de decisiones en tiempo real.</li> <li>Seguimiento en deportes: En partidos de f\u00fatbol o tenis se rastrea el bal\u00f3n o los jugadores para mejorar el arbitraje y el an\u00e1lisis estad\u00edstico.</li> <li>B\u00fasqueda de im\u00e1genes: Los tel\u00e9fonos inteligentes permiten buscar lugares u objetos directamente en internet mediante la detecci\u00f3n de entidades en fotos.</li> <li>Conteo de objetos: La detecci\u00f3n ayuda a contar existencias en almacenes, tiendas, o personas en eventos.</li> </ul>"},{"location":"hf/Tasks_vc/#metricas-de-evaluacion","title":"M\u00e9tricas de Evaluaci\u00f3n","text":"<ul> <li>Precisi\u00f3n media promedio (AP): \u00c1rea bajo la curva de precisi\u00f3n versus recall para cada clase.</li> <li>mAP (mean Average Precision): Promedio de AP en todas las clases.</li> <li>AP\u03b1: Precisi\u00f3n promedio seg\u00fan el umbral de IoU (por ejemplo, AP50 muestra AP cuando el IoU es &gt;0,5).</li> </ul>"},{"location":"hf/Tasks_vc/#ejemplo-de-uso-con-gradio","title":"Ejemplo de uso con Gradio","text":"<p>Vamos a crear una aplicaci\u00f3n web con Gradio que use un objeto pipeline del modelo <code>facebook/detr-resnet-50</code>.</p> <p>As\u00ed es como se carga:  <pre><code>from transformers import pipeline\n\ndetection = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n</code></pre> Una vez que hayamos creado el objeto tipo pipeline (detecci\u00f3n en este caso), podemos pasar directamente la imagen (en formato PIL) al pipeline y obtener el resultado: </p> <p><pre><code>results = detection(image)\nresults\n</code></pre> Debemos tener en cuenta que el objeto de tipo pipeline (detecci\u00f3n) tambi\u00e9n puede incluir una URL de una imagen, no solo un objeto de imagen tipo PIL. Es decir, tambi\u00e9n podemos llamar al objeto pipeline de la siguiente manera: </p> <p><pre><code>results = detection('http://bit.ly/46xv3sL')\n</code></pre> <pre><code># Si no funcionara, prueba a descargar el fichero y ejecutarlo de forma local:\nresults = detection('Good_Smile_Company_offices_ladies.jpg')\n</code></pre> Debemos instalar la librer\u00eda <code>timm</code> (PyTorch Image Models) para Python. Es una extensa colecci\u00f3n de modelos de visi\u00f3n por computadora de \u00faltima generaci\u00f3n (SOTA, por sus siglas en ingl\u00e9s). Est\u00e1 dise\u00f1ada para ser utilizada con el framework PyTorch y es muy apreciada en la comunidad de aprendizaje profundo por su flexibilidad y la gran cantidad de modelos preentrenados que ofrece. <pre><code>pip install timm\n</code></pre> El resultado impreso se ver\u00eda as\u00ed: <pre><code>[{'score': 0.9179903864860535,\n  'label': 'person',\n  'box': {'xmin': 549, 'ymin': 145, 'xmax': 564, 'ymax': 165}},\n {'score': 0.9960624575614929,\n  'label': 'tv',\n  'box': {'xmin': 317, 'ymin': 212, 'xmax': 416, 'ymax': 299}},\n {'score': 0.9425505995750427,\n  'label': 'chair',\n  'box': {'xmin': 508, 'ymin': 306, 'xmax': 661, 'ymax': 429}},\n {'score': 0.9753392338752747,\n  'label': 'person',\n  'box': {'xmin': 673, 'ymin': 135, 'xmax': 705, 'ymax': 174}},\n {'score': 0.962176501750946,\n  'label': 'person',\n  'box': {'xmin': 703, 'ymin': 115, 'xmax': 722, 'ymax': 140}},\n {'score': 0.9881888628005981,\n  'label': 'person',\n  'box': {'xmin': 454, 'ymin': 142, 'xmax': 497, 'ymax': 202}},\n {'score': 0.9871691465377808,\n  'label': 'keyboard',\n  'box': {'xmin': 344, 'ymin': 276, 'xmax': 445, 'ymax': 346}},\n {'score': 0.9371852874755859,\n  'label': 'tv',\n  'box': {'xmin': 309, 'ymin': 194, 'xmax': 374, 'ymax': 237}},\n {'score': 0.9975801706314087,\n  'label': 'person',\n  'box': {'xmin': 395, 'ymin': 152, 'xmax': 446, 'ymax': 216}},\n {'score': 0.9986708164215088,\n  'label': 'person',\n  'box': {'xmin': 237, 'ymin': 174, 'xmax': 308, 'ymax': 264}},\n {'score': 0.9173707365989685,\n  'label': 'person',\n  'box': {'xmin': 720, 'ymin': 112, 'xmax': 737, 'ymax': 131}},\n {'score': 0.9895991086959839,\n  'label': 'potted plant',\n  'box': {'xmin': 124, 'ymin': 211, 'xmax': 230, 'ymax': 330}},\n {'score': 0.9996592998504639,\n  'label': 'person',\n  'box': {'xmin': 369, 'ymin': 226, 'xmax': 535, 'ymax': 427}},\n {'score': 0.9821581840515137,\n  'label': 'tv',\n  'box': {'xmin': 491, 'ymin': 181, 'xmax': 530, 'ymax': 223}},\n {'score': 0.9970135688781738,\n  'label': 'person',\n  'box': {'xmin': 516, 'ymin': 177, 'xmax': 628, 'ymax': 318}}]\n</code></pre> El resultado es una lista de diccionarios para cada objeto detectado. Para dibujar la etiqueta y el cuadro delimitador de cada objeto, utilizaremos el siguiente fragmento de c\u00f3digo: </p> <p><pre><code>import random\nfrom PIL import Image, ImageDraw\nimport requests\nimport torch\n\ndraw = ImageDraw.Draw(image)\n\nfor object in results:\n    box = [i for i in object['box'].values()]\n    print(\n        f\"Detected {object['label']} with confidence \"\n        f\"{(object['score'] * 100):.2f}% at {box}\"\n    )\n\n    r = random.randint(0, 255)\n    g = random.randint(0, 255)\n    b = random.randint(0, 255)\n    color = (r, g, b)\n\n    draw.rectangle(box,\n                   outline=color,\n                   width=2)\n\n    draw.text((box[0], box[1]-10),\n              object['label'],\n              fill='white')\n\ndisplay(image)\n</code></pre> La imagen ser\u00eda id\u00e9ntica a la que se muestra anteriormente pero con los cuadrados correspondientes. Con el objeto pipeline, tambi\u00e9n podemos obtener una lista de etiquetas directamente mediante el atributo <code>model.config.id2label</code>:  <pre><code>detection.model.config.id2label\n</code></pre></p>"},{"location":"hf/Tasks_vc/#actividad-guiada","title":"Actividad guiada","text":"<p>Desarrollar con Gradio un template similar es este:   Resultado final: </p> <p>Define:</p> <ul> <li>Una funci\u00f3n llamada predict</li> <li>Interface Gradio que env\u00ede una imagen y muestre la imagen con los objetos detectados</li> </ul> <p>C\u00f3digo final en Gradio: <pre><code>import gradio as gr\nfrom PIL import Image, ImageDraw\nfrom transformers import pipeline\nimport random \n\ndef predict(image):    \n\n    detection = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n\n    results = detection(image)       \n\n    draw = ImageDraw.Draw(image) \n\n    for object in results: \n        box = [i for i in object['box'].values()] \n        print( \n            f\"Detected {object['label']} with confidence \"   \n            f\"{(object['score'] * 100):.2f}% at {box}\"   \n        ) \n\n        r = random.randint(0, 255) \n        g = random.randint(0, 255) \n        b = random.randint(0, 255) \n        color = (r, g, b) \n\n        #Dibuja un cuadro delimitador alrededor del objeto.\n        draw.rectangle(box,  \n                    outline=color,  \n                    width=2)  \n        #Muestra la etiqueta del objeto\n        draw.text((box[0], box[1]-10),  \n                object['label'],  \n                fill='white')  \n\n    return image\n\n\ndemo = gr.Interface(\n    predict, \n    inputs=gr.Image(type=\"pil\"), \n    outputs=\"image\")\n\ndemo.launch()\n</code></pre></p>"},{"location":"hf/Tasks_vc/#actividad-2-comparativa-practica-de-deteccion-de-objetos-con-hugging-face-y-ultralytics-yolo11","title":"\ud83d\udcdd Actividad 2: Comparativa pr\u00e1ctica de Detecci\u00f3n de Objetos con Hugging Face y Ultralytics YOLO11","text":""},{"location":"hf/Tasks_vc/#contexto","title":"Contexto","text":"<p>Hemos trabajado en clase con modelos de detecci\u00f3n de objetos, usando ejemplos como <code>facebook/detr-resnet-50</code>  en Hugging Face (ver ejemplo y recursos de clase). En esta actividad, ir\u00e1s un paso m\u00e1s all\u00e1 probando la herramienta Ultralytics YOLO11, consultando su documentaci\u00f3n oficial de integraci\u00f3n con Gradio.</p>"},{"location":"hf/Tasks_vc/#objetivos_1","title":"Objetivos","text":"<ul> <li>Investigar y comprender el funcionamiento de la familia YOLO (en especial YOLO11).</li> <li>Probar distintos c\u00f3digos y ejemplos reales usando YOLO11 y Gradio.</li> <li>Comparar los resultados con los de <code>facebook/detr-resnet-50</code> en velocidad, facilidad de uso y precisi\u00f3n.</li> <li>Reflexionar sobre ventajas e inconvenientes de cada enfoque en distintos escenarios reales.</li> </ul>"},{"location":"hf/Tasks_vc/#1-lectura-e-investigacion-inicial","title":"1. Lectura e investigaci\u00f3n inicial","text":"<ul> <li>Lee la documentaci\u00f3n de Ultralytics YOLO11 y familiar\u00edzate con su API y flujo de trabajo.</li> <li>Consulta y ejecuta el ejemplo de integraci\u00f3n con Gradio: docs oficiales.</li> </ul>"},{"location":"hf/Tasks_vc/#2-implementacion-y-pruebas","title":"2. Implementaci\u00f3n y pruebas","text":"<ul> <li>Ejecuta la demo b\u00e1sica de YOLO11+Gradio incluida en la documentaci\u00f3n.</li> <li>Realiza anotaciones sobre el input, formato de resultados y velocidad tras varias ejecuciones con im\u00e1genes reales o ejemplos propios.</li> </ul>"},{"location":"hf/Tasks_vc/#3-comparativa-objetiva-con-hugging-face","title":"3. Comparativa objetiva con Hugging Face","text":"<ul> <li>Utiliza el modelo <code>facebook/detr-resnet-50</code> desde Hugging Face (ya visto en clase) para detectar objetos en al menos dos im\u00e1genes iguales a las usadas en YOLO11.</li> <li>Rellena la tabla comparativa:</li> </ul> Imagen Modelo Objetos detectados Tiempo de inferencia Falsos positivos/negativos Facilidad de integraci\u00f3n Observaciones (insertar nombre) YOLO11 (insertar nombre) detr-resnet-50 (HF) <ul> <li>Comenta los resultados en t\u00e9rminos de:<ul> <li>Exactitud y n\u00famero/calidad de predicciones.</li> <li>Consumo de recursos y tiempo de ejecuci\u00f3n (compara si es posible en CPU y GPU).</li> <li>Facilidad de uso/grado de documentaci\u00f3n o n\u00famero de l\u00edneas de c\u00f3digo para uso en Gradio.</li> </ul> </li> </ul>"},{"location":"hf/Tasks_vc/#entrega","title":"Entrega","text":"<ul> <li>Un archivo <code>.py</code> con el c\u00f3digo empleado y comentarios.</li> <li>Las im\u00e1genes o capturas de pantalla de las pruebas realizadas.</li> <li>Rellena y agrega la tabla comparativa.</li> </ul>"},{"location":"hf/Tasks_vc/#3-segmentacion-de-imagenes-image-segmentation","title":"3. Segmentaci\u00f3n de im\u00e1genes (Image segmentation)","text":"<p>La segmentaci\u00f3n de im\u00e1genes es una t\u00e9cnica de visi\u00f3n por computador que divide una imagen en segmentos o regiones, cada una correspondiente a un objeto de inter\u00e9s. Con la segmentaci\u00f3n de im\u00e1genes, podemos analizar una imagen y extraer informaci\u00f3n valiosa de ella. </p> <p></p> <p>Algunos de sus usos son: </p> <ul> <li>Im\u00e1genes m\u00e9dicas: se utilizan para identificar y segmentar tumores en resonancias magn\u00e9ticas o tomograf\u00edas computarizadas </li> <li>Detecci\u00f3n y reconocimiento de objetos: al igual que la detecci\u00f3n de objetos que hemos visto anteriormente, tambi\u00e9n podemos utilizar la segmentaci\u00f3n de im\u00e1genes para identificar y localizar objetos en una imagen </li> <li>Procesamiento de documentos: se utiliza para segmentar regiones de texto en documentos escaneados </li> <li>Biometr\u00eda: se utiliza para identificar y localizar rostros en im\u00e1genes o fotogramas de v\u00eddeo </li> </ul> <p>Hugging Face incluye varios modelos de segmentaci\u00f3n de im\u00e1genes que podemos usar. Uno de ellos es el modelo SegFormer \"SegFormer model fine-tuned on ADE20k\", optimizado con ADE20k.</p> <p>La siguiente imagen muestra el modelo SegFormer fine-tuned (optimizado) por el modelo ADE20k en la web de Hugging Face: </p> <p>Para probar el modelo de segmentaci\u00f3n, usaremos una imagen del Taj Mahal. La arrastraremos y la soltaremos en la secci\u00f3n de \"Hosted inference API\" alojada en la p\u00e1gina de Hugging Face:</p> <p>Imagen del Taj Mahal  Fuente: https://mng.bz/5vzD</p> <p>Resultado de la segmentaci\u00f3n de im\u00e1genes utilizando una imagen del Taj Mahal: </p> <p>Como podemos ver en el resultado, el modelo puede detectar diferentes objetos en la imagen (edificios, cielos, \u00e1rboles, etc\u00e9tera) y resaltar los diversos segmentos en dicha imagen. De hecho, podemos pasar el rat\u00f3n sobre las diversas etiquetas segmentadas y la imagen resaltar\u00e1 dicha etiqueta seleccionada. </p>"},{"location":"hf/Tasks_vc/#uso-del-modelo-con-pipeline","title":"Uso del modelo con pipeline","text":"<p>Como es habitual, usaremos el modelo mediante programaci\u00f3n. Primero, cargamos el modelo y luego verificamos cu\u00e1ntos objetos puede detectar el modelo. La forma m\u00e1s f\u00e1cil de usar el modelo es usar un pipeline  de la librer\u00eda <code>transformers</code>:  <pre><code>from transformers import pipeline \n\nsegmentation = pipeline(\"image-segmentation\",  \n               model=\"nvidia/segformer-b0-finetuned-ade-512-512\") \n\nprint(segmentation.model.config.id2label)\n</code></pre> Estos son los primeros y \u00faltimos cinco objetos que puede detectar (el modelo puede detectar un total de 150 objetos):  <pre><code>{0: 'wall', \n 1: 'building', \n 2: 'sky', \n 3: 'floor', \n 4: 'tree', \n ... \n 145: 'shower', \n 146: 'radiator', \n 147: 'glass', \n 148: 'clock', \n 149: 'flag'} \n</code></pre> Para este ejemplo, usaremos una imagen donde vemos a un hombre y a un avi\u00f3n que vuela por encima, para as\u00ed descubrir los distintos segmentos de dicha imagen: </p> <p> </p> <p>Fuente: https://unsplash.com/photos/EC_GhFRGTAY</p> <p>Para detectar los distintos segmentos de la imagen, pasamos la direcci\u00f3n URL de una imagen al objeto pipeline. Antes, de nada, tendremos que instalar mediante PIP la librer\u00eda PIL para el procesamiento de im\u00e1genes: <pre><code>pip install Pillow\n</code></pre> Una vez instalada, probamos el siguiente c\u00f3digo: <pre><code>from transformers import pipeline \nfrom PIL import Image\nimport requests\n\nsegmentation = pipeline(\"image-segmentation\",  \n               model=\"nvidia/segformer-b0-finetuned-ade-512-512\") \n\nurl = 'https://bit.ly/46iDeJQ'\nresults = segmentation(url)\nprint(results)\n</code></pre> La salida de la variable results es una lista de diccionarios que contiene detalles de cada uno de los segmentos detectados en la imagen:  <pre><code>[{'score': None,\n  'label': 'wall',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'building',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'sky',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'person',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'airplane',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;}]\n</code></pre> En particular, el elemento mask contiene la m\u00e1scara del segmento detectado. Para ver cada una de las m\u00e1scaras detectadas, recorremos la variable results: </p> <p><pre><code>for result in results:\n    print(result['label'])\n    result['mask'].show()\n</code></pre> Por pantalla visualizamos las etiquetas encontradas: <pre><code>wall\nbuilding\nsky\nperson\nairplane\n</code></pre> La figura siguiente muestra las m\u00e1scaras detectadas para person (persona) y airplane (avi\u00f3n): </p> <p>La parte blanca de la m\u00e1scara representa la parte de la imagen que contiene el segmento de inter\u00e9s. Podemos aplicar la m\u00e1scara sobre la imagen original mediante el siguiente fragmento de c\u00f3digo: </p> <p><pre><code>image = Image.open(requests.get(url, stream=True).raw) \n\nfor result in results: \n    base_image = image.copy() \n    mask_image = result['mask'] \n\n    # Aplica la m\u00e1scara sobre la imagen original\n    base_image.paste(mask_image, mask=mask_image) \n    #Imprime la etiqueta del segmento\n    print(result['label']) \n    result['mask'].show()\n</code></pre> La figura siguiente muestra las m\u00e1scaras de person (persona) y airplane (avi\u00f3n) aplicadas sobre la imagen original: </p> <p>Cuando aplicamos la m\u00e1scara sobre la imagen, observaremos que el segmento de inter\u00e9s est\u00e1 en blanco. Ser\u00eda m\u00e1s natural invertir esto, es decir, el segmento de inter\u00e9s deber\u00eda mostrarse mientras que el resto deber\u00eda estar en blanco. Para hacerlo, podemos invertir la m\u00e1scara usando la funci\u00f3n <code>invert()</code> de la clase <code>ImageOps</code> en el paquete <code>PIL</code>. Los siguientes cambios invierten la m\u00e1scara y, a continuaci\u00f3n, la aplican sobre la imagen original: </p> <pre><code>from PIL import ImageOps \n</code></pre> <p><pre><code>for result in results: \n    base_image = image.copy() \n    mask_image = result['mask'] \n\n    mask_image = ImageOps.invert(mask_image)  #Invierte la m\u00e1scara \n    base_image.paste(mask_image, mask=mask_image)  #Aplica la m\u00e1scara sobre la imagen original \n    print(result['label'])  #Imprime la etiqueta del segmento\n    base_image.show()\n</code></pre> La figura siguiente muestra las m\u00e1scaras invertidas para person (persona) y airplane (avi\u00f3n)aplicadas en la imagen original. </p> <p></p>"},{"location":"hf/Tasks_vc/#actividad-3-gradio-para-segmentacion-de-imagenes","title":"\ud83d\udcdd Actividad 3: Gradio para segmentaci\u00f3n de im\u00e1genes","text":"<p>Crea un prototipo mediante Gradio haciendo uso de la clase Interface que te permita probar el modelo de segmentaci\u00f3n bas\u00e1ndote en el siguiente prototipo:  </p> <p>Pasamos una foto y en el campo Label especificamos el objeto a buscar, por ejemplo, person:  </p> <p>Resultados:  Detecci\u00f3n de person </p> <p>Detecci\u00f3n de airplane  Pasos:  </p> <ul> <li>Definir la interfaz de Gradio con los componentes de entrada y de salida similares al prototipo de la imagen </li> <li>Crea una funci\u00f3n \u201csegmentation\u201d que reciba los par\u00e1metros de entrada correspondientes. Dentro de dicha funci\u00f3n </li> <li>Cuando el modelo devuelva el resultado, iterar\u00e1 a trav\u00e9s del resultado y buscar\u00e1 la etiqueta especificada por el usuario (en el par\u00e1metro label). </li> <li>A continuaci\u00f3n, la funci\u00f3n invierte la m\u00e1scara correspondiente, la aplica a la imagen y la devuelve autom\u00e1ticamente.  <p>CONSEJO: Dentro de la funci\u00f3n deber\u00edas de imprimir los labels que te devuelve el modelo para saber que etiquetas ha detectado el modelo.</p> </li> </ul> <p>Realiza algunas pruebas con im\u00e1genes diferentes y adjunta en este documento los resultados. </p> <p>Entrega el fichero py y las im\u00e1genes que hayas utilizado. </p>"},{"location":"hf/Tasks_vc/#4-modelo-de-clasificacion-de-imagenes-sin-entrenamiento-previo-zero-shot-image-classification","title":"4. Modelo de clasificaci\u00f3n de im\u00e1genes sin entrenamiento previo (Zero-shot Image Classification)","text":"<p>La clasificaci\u00f3n de im\u00e1genes sin entrenamiento previo (Zero-shot) es una tarea de visi\u00f3n artificial que consiste en clasificar im\u00e1genes de una o de varias clases, sin ning\u00fan entrenamiento previo ni conocimiento de dichas clases.  </p> <p></p>"},{"location":"hf/Tasks_vc/#pero-que-es-eso-del-aprendizaje-zero-shot-zero-shot-learning","title":"Pero, \u00bfqu\u00e9 es eso del aprendizaje zero-shot (Zero-Shot learning)?","text":"<p>El aprendizaje zero-shot (ZSL) es un enfoque de machine learning en el que un modelo es capaz de reconocer o clasificar categor\u00edas para las que no ha visto ning\u00fan ejemplo etiquetado durante el entrenamiento. En lugar de aprender directamente a partir de ejemplos de esas clases, se apoya en conocimiento auxiliar (descripciones textuales, atributos, embeddings, etc.) y en lo que ya ha aprendido sobre otras clases relacionadas.\u200b</p>"},{"location":"hf/Tasks_vc/#idea-basica","title":"Idea b\u00e1sica","text":"<p>En el aprendizaje supervisado cl\u00e1sico (y en few-shot/one-shot) el modelo ve ejemplos etiquetados de todas las clases que luego tendr\u00e1 que reconocer. En zero-shot, el modelo nunca ve ejemplos etiquetados de las clases \u201cno vistas\u201d, pero puede inferirlas gracias a informaci\u00f3n sem\u00e1ntica sobre esas clases (por ejemplo, descripciones, atributos o relaciones con clases conocidas).\u200b</p> <p>Un ejemplo intuitivo: un ni\u00f1o puede aprender qu\u00e9 es un \u201cp\u00e1jaro\u201d leyendo una descripci\u00f3n (tiene plumas, pico, alas\u2026) y luego reconocer un p\u00e1jaro sin haber visto antes fotos etiquetadas como \u201cp\u00e1jaro\u201d.\u200b</p>"},{"location":"hf/Tasks_vc/#zero-shot-vs-few-shot-y-generalizado-zero-shot","title":"Zero-shot vs. few-shot y generalizado zero-shot","text":"<ul> <li>Few-shot / one-shot: el modelo recibe unos pocos ejemplos etiquetados (p. ej. 1, 5 o 10) de las nuevas clases para adaptarse.\u200b</li> <li>Zero-shot: no hay ejemplos etiquetados de las nuevas clases; solo informaci\u00f3n auxiliar.\u200b</li> </ul> <p>En muchos escenarios reales, el modelo debe clasificar datos que pueden pertenecer tanto a clases ya vistas como a clases nuevas: esto se llama aprendizaje generalizado zero-shot (GZSL). En GZSL hay un problema adicional: el modelo tiende a sesgarse hacia las clases vistas, as\u00ed que se necesitan t\u00e9cnicas espec\u00edficas para reducir ese sesgo.\u200b</p>"},{"location":"hf/Tasks_vc/#papel-de-la-informacion-auxiliar-y-las-incrustaciones-embeddings","title":"Papel de la informaci\u00f3n auxiliar y las incrustaciones (embeddings)","text":"<p>Como no hay ejemplos etiquetados de las clases nuevas, los m\u00e9todos ZSL suelen usar informaci\u00f3n auxiliar como:\u200b - Descripciones de texto de las clases (por ejemplo, \u201coso polar: parecido a un oso pardo pero con pelaje blanco\u201d).\u200b - Atributos sem\u00e1nticos (color, forma, \u201cinsecto volador amarillo a rayas\u201d, etc.).\u200b - Embeddings vectoriales (representaciones num\u00e9ricas) de palabras, im\u00e1genes u otras modalidades.\u200b</p> <p>Una estrategia com\u00fan es representar tanto las muestras como las clases como vectores en un espacio de incrustaci\u00f3n conjunta y luego clasificar midiendo la similitud (coseno, distancia eucl\u00eddea, etc.) entre la muestra y cada clase. Si la incrustaci\u00f3n de una muestra est\u00e1 m\u00e1s cerca de la incrustaci\u00f3n de la clase \u201cabeja\u201d que de otras, se clasifica como \u201cabeja\u201d, aunque nunca se hayan visto ejemplos etiquetados de abejas.\u200b La palabra rei est\u00e1 m\u00e1s cerca de reina que la de manzana. Pero tambi\u00e9n debemos pensar que mantequilla y tostada suelen estar \"cerca\" sem\u00e1nticamente ya que aparecen juntas en muchos textos.</p>"},{"location":"hf/Tasks_vc/#relacion-con-modelos-preentrenados-y-aprendizaje-por-transferencia","title":"Relaci\u00f3n con modelos preentrenados y aprendizaje por transferencia","text":"<p>ZSL suele apoyarse en aprendizaje por transferencia: se reutilizan modelos ya entrenados (por ejemplo, BERT para texto, ResNet o ViT para im\u00e1genes) para obtener buenas representaciones sin empezar desde cero. Estos modelos preentrenados generan embeddings \u00fatiles que luego se pueden combinar con informaci\u00f3n auxiliar para hacer clasificaci\u00f3n zero-shot.\u200b</p> <p>Los grandes modelos de lenguaje (LLM) son especialmente interesantes para ZSL porque, al estar entrenados en enormes corpus de texto, capturan el significado de las palabras y las etiquetas, y pueden razonar sobre clases que no han visto etiquetadas expl\u00edcitamente.\u200b</p>"},{"location":"hf/Tasks_vc/#variantes-y-metodos-generativos","title":"Variantes y m\u00e9todos generativos","text":"<p>Adem\u00e1s de los m\u00e9todos basados en incrustaciones, existen enfoques generativos que usan modelos como VAE, GAN o modelos de IA generativa para sintetizar ejemplos de las clases no vistas a partir de sus descripciones. Una vez generados y etiquetados esos ejemplos sint\u00e9ticos, el problema se convierte en un aprendizaje supervisado cl\u00e1sico.\u200b</p> <p>\u201cEjemplos sint\u00e9ticos\u201d son datos creados artificialmente por un modelo o algoritmo, en lugar de venir de casos reales observados.\u200b En el contexto de zero-shot, significa que el modelo genera \u201cfalsos ejemplos\u201d de una clase nueva (por ejemplo, textos que describen o simulan esa clase) usando su conocimiento previo y descripciones auxiliares; despu\u00e9s, esos ejemplos generados se tratan como si fueran datos de entrenamiento normales para entrenar o ajustar otro modelo</p> <p>En resumen, el aprendizaje zero-shot permite que un modelo generalice a categor\u00edas nuevas utilizando conocimiento sem\u00e1ntico y modelos preentrenados, reduciendo la necesidad de grandes conjuntos de datos etiquetados para cada nueva clase que se quiera reconocer.</p> <p>Volviendo a la visi\u00f3n por computador, la clasificaci\u00f3n de im\u00e1genes sin entrenamiento previo funciona transfiriendo el conocimiento aprendido durante el entrenamiento de un modelo para clasificar clases nuevas que no estaban presentes en los datos de entrenamiento. Por lo tanto, se trata de una variante del aprendizaje por transferencia. Por ejemplo, un modelo entrenado para diferenciar coches de aviones puede utilizarse para clasificar im\u00e1genes de barcos.</p>"},{"location":"hf/Tasks_vc/#aprendizaje-con-pocos-ejemplos","title":"Aprendizaje con pocos ejemplos","text":"<p>El aprendizaje con pocos ejemplos es un m\u00e9todo en el que los sistemas aprenden a reconocer nuevos objetos utilizando solo un peque\u00f1o n\u00famero de ejemplos. Por ejemplo, si le mostramos a un modelo algunas im\u00e1genes de un ping\u00fcino, un pel\u00edcano y un frailecillo (este peque\u00f1o grupo se llama el \"conjunto de soporte\"), aprende c\u00f3mo son estos p\u00e1jaros. </p> <p>M\u00e1s tarde, si le mostramos al modelo una nueva imagen, como un ping\u00fcino, compara esta nueva imagen con las de su conjunto de soporte y elige la coincidencia m\u00e1s cercana. Cuando es dif\u00edcil reunir una gran cantidad de datos, este m\u00e9todo es beneficioso porque el sistema a\u00fan puede aprender y adaptarse con solo unos pocos ejemplos.</p> <p></p>"},{"location":"hf/Tasks_vc/#aprendizaje-zero-shot","title":"Aprendizaje Zero-shot","text":"<p>Tal y como hemos comentado, el aprendizaje zero-shot es una forma de que las m\u00e1quinas reconozcan cosas que nunca han visto antes sin necesidad de ejemplos de ellas. Utiliza informaci\u00f3n sem\u00e1ntica, como descripciones, para ayudar a establecer conexiones.</p> <p>Por ejemplo, si una m\u00e1quina ha aprendido sobre animales como gatos, leones y caballos comprendiendo caracter\u00edsticas como \"peque\u00f1o y esponjoso\", \"gran felino salvaje\" o \"cara larga\", puede utilizar este conocimiento para identificar un nuevo animal, como un tigre. Incluso si nunca ha visto un tigre antes, puede utilizar una descripci\u00f3n como \"un animal parecido a un le\u00f3n con rayas oscuras\" para identificarlo correctamente. Esto facilita que las m\u00e1quinas aprendan y se adapten sin necesidad de muchos ejemplos.</p> <p></p>"},{"location":"hf/Tasks_vc/#aprendizaje-por-transferencia-transfer-learning","title":"Aprendizaje por transferencia (Transfer learning)","text":"<p>El aprendizaje por transferencia es un paradigma de aprendizaje en el que un modelo utiliza lo que ha aprendido de una tarea para ayudar a resolver una tarea nueva similar. Esta t\u00e9cnica es especialmente \u00fatil cuando se trata de tareas de visi\u00f3n artificial como la detecci\u00f3n de objetos, la clasificaci\u00f3n de im\u00e1genes y el reconocimiento de patrones. </p> <p>Por ejemplo, en visi\u00f3n artificial, un modelo preentrenado puede reconocer objetos generales, como animales, y luego ajustarse mediante el aprendizaje por transferencia para identificar objetos espec\u00edficos, como diferentes razas de perros. Al reutilizar el conocimiento de tareas anteriores, el aprendizaje por transferencia facilita el entrenamiento de modelos de visi\u00f3n artificial en conjuntos de datos m\u00e1s peque\u00f1os, lo que ahorra tiempo y esfuerzo.</p> <p></p>"},{"location":"hf/Tasks_vc/#aplicaciones-en-el-mundo-real-de-varios-paradigmas-de-aprendizaje","title":"Aplicaciones en el mundo real de varios paradigmas de aprendizaje","text":""},{"location":"hf/Tasks_vc/#diagnostico-de-enfermedades-raras-con-aprendizaje-de-pocos-ejemplos","title":"Diagn\u00f3stico de enfermedades raras con aprendizaje de pocos ejemplos","text":"<p>El aprendizaje con pocos ejemplos ha cambiado las reglas del juego para el sector de la salud, especialmente en im\u00e1genes m\u00e9dicas. Puede ayudar a los m\u00e9dicos a diagnosticar enfermedades raras utilizando solo unos pocos ejemplos o incluso descripciones, sin necesidad de grandes cantidades de datos. Esto es especialmente \u00fatil cuando los datos son limitados, lo que suele ser el caso porque la recopilaci\u00f3n de grandes conjuntos de datos para afecciones raras puede ser un desaf\u00edo.</p> <p>Por ejemplo, SHEPHERD utiliza el aprendizaje con pocos ejemplos y grafos de conocimiento biom\u00e9dico para diagnosticar trastornos gen\u00e9ticos raros. Mapea la informaci\u00f3n del paciente, como los s\u00edntomas y los resultados de las pruebas, en una red de genes y enfermedades conocidos. Esto ayuda a identificar la causa gen\u00e9tica probable y a encontrar casos similares, incluso cuando los datos son limitados. </p> <p></p>"},{"location":"hf/Tasks_vc/#mejora-de-la-deteccion-de-enfermedades-de-plantas-con-aprendizaje-zero-shot","title":"Mejora de la detecci\u00f3n de enfermedades de plantas con aprendizaje zero-shot","text":"<p>En agricultura, la identificaci\u00f3n r\u00e1pida de enfermedades en las plantas es esencial porque los retrasos en la detecci\u00f3n pueden provocar da\u00f1os generalizados en los cultivos, la reducci\u00f3n de los rendimientos y p\u00e9rdidas financieras significativas. Los m\u00e9todos tradicionales a menudo se basan en grandes conjuntos de datos y conocimientos especializados, que no siempre son accesibles, especialmente en \u00e1reas remotas o con recursos limitados. Aqu\u00ed es donde entran en juego los avances en IA, como el aprendizaje zero-shot.</p> <p>Supongamos que un agricultor cultiva tomates y patatas y observa s\u00edntomas como hojas amarillentas o manchas marrones. El aprendizaje cero puede ayudar a identificar enfermedades como el tiz\u00f3n tard\u00edo sin necesidad de grandes conjuntos de datos. Utilizando descripciones de los s\u00edntomas, el modelo puede classify enfermedades que no ha visto antes. Este m\u00e9todo es r\u00e1pido, escalable y permite a los agricultores detectar diversos problemas de las plantas. Les ayuda a controlar la salud de los cultivos de forma m\u00e1s eficaz, tomar medidas a tiempo y reducir las p\u00e9rdidas.</p> <p></p>"},{"location":"hf/Tasks_vc/#vehiculos-autonomos-y-aprendizaje-por-transferencia","title":"Veh\u00edculos aut\u00f3nomos y aprendizaje por transferencia","text":"<p>Los veh\u00edculos aut\u00f3nomos a menudo necesitan adaptarse a diferentes entornos para navegar de forma segura. El aprendizaje por transferencia les ayuda a utilizar el conocimiento previo para adaptarse r\u00e1pidamente a nuevas condiciones sin comenzar su entrenamiento desde cero. Combinadas con la visi\u00f3n artificial, que ayuda a los veh\u00edculos a interpretar la informaci\u00f3n visual, estas tecnolog\u00edas permiten una navegaci\u00f3n m\u00e1s fluida en diferentes terrenos y condiciones clim\u00e1ticas, lo que hace que la conducci\u00f3n aut\u00f3noma sea m\u00e1s eficiente y fiable.</p> <p>Un buen ejemplo de ello es un sistema de gesti\u00f3n de aparcamientos que utiliza YOLO11 Ultralytics para controlar las plazas de aparcamiento. YOLO11, un modelo de detecci\u00f3n de objetos preentrenado, puede ajustarse mediante aprendizaje por transferencia para identificar plazas de aparcamiento vac\u00edas y ocupadas en tiempo real. Al entrenar el modelo en un conjunto de datos m\u00e1s peque\u00f1o de im\u00e1genes de aparcamientos, aprende a detectar con precisi\u00f3n plazas abiertas, plazas llenas e incluso zonas reservadas.</p> <p> \u200d Integrado con otras tecnolog\u00edas, este sistema puede guiar a los conductores hasta la plaza disponible m\u00e1s cercana, ayudando a reducir el tiempo de b\u00fasqueda y la congesti\u00f3n del tr\u00e1fico. El aprendizaje por transferencia lo hace posible bas\u00e1ndose en las capacidades de detecci\u00f3n de objetos existentes de YOLO11, lo que le permite adaptarse a las necesidades espec\u00edficas de la gesti\u00f3n de aparcamientos sin empezar de cero. Este enfoque ahorra tiempo y recursos al tiempo que crea una soluci\u00f3n altamente eficiente y escalable que mejora las operaciones de aparcamiento y mejora la experiencia general del usuario.</p> <p>Los datos en este paradigma de aprendizaje consisten en:</p> <ul> <li>Datos vistos: im\u00e1genes y sus etiquetas correspondientes.</li> <li>Datos no vistos: solo etiquetas y sin im\u00e1genes.</li> <li>Informaci\u00f3n auxiliar: informaci\u00f3n adicional proporcionada al modelo durante el entrenamiento que conecta los datos vistos y no vistos. Esto puede ser en forma de descripci\u00f3n textual o incrustaciones de palabras.</li> </ul> <p>Tradicionalmente, la clasificaci\u00f3n de im\u00e1genes ha requierdo entrenar un modelo con un conjunto espec\u00edfico de im\u00e1genes etiquetadas, y este modelo aprende a \u00abasignar\u00bb ciertas caracter\u00edsticas de las im\u00e1genes a las etiquetas. Cuando es necesario utilizar dicho modelo para una tarea de clasificaci\u00f3n que introduce un nuevo conjunto de etiquetas, es necesario realizar un fine-tunning (ajuste fino) para \u00abrecalibrar\u00bb el modelo.</p> <p>Por el contrario, los modelos de clasificaci\u00f3n de im\u00e1genes de vocabulario abierto o sin entrenamiento previo suelen ser modelos multimodales que se han entrenado con un gran conjunto de datos de im\u00e1genes y descripciones asociadas. Estos modelos aprenden representaciones alineadas de visi\u00f3n y lenguaje que pueden utilizarse para muchas tareas posteriores, incluida la clasificaci\u00f3n de im\u00e1genes sin entrenamiento previo.</p> <p>Se trata de un enfoque m\u00e1s flexible de la clasificaci\u00f3n de im\u00e1genes que permite a los modelos generalizar a categor\u00edas nuevas y desconocidas sin necesidad de datos de entrenamiento adicionales y permite a los usuarios consultar im\u00e1genes con descripciones de texto de formato libre de sus objetos de destino.</p> <p>Hugging Face proporciona herramientas y procesos para implementar la clasificaci\u00f3n de im\u00e1genes sin entrenamiento previo utilizando modelos multimodales preentrenados como CLIP (Contrastive Language\u2013Image Pre-training), que se entrenan con grandes conjuntos de datos de im\u00e1genes emparejadas con descripciones en lenguaje natural. Estos modelos aprenden a comprender la relaci\u00f3n entre el contenido visual y el lenguaje, lo que los hace muy eficaces para tareas sin entrenamiento previo. Por ejemplo, un modelo entrenado en categor\u00edas de objetos comunes puede clasificar una imagen de un barco comparando sus caracter\u00edsticas visuales con la incrustaci\u00f3n sem\u00e1ntica de la palabra \u00abbarco\u00bb.</p>"},{"location":"hf/Tasks_vc/#modelos-disponibles-en-hugging-face_1","title":"\ud83e\uddf0 Modelos disponibles en Hugging Face","text":"Modelo Arquitectura Caracter\u00edstica Enlace <code>openai/clip-vit-base-patch32</code> CLIP (ViT + Transformer) Texto + Imagen \ud83d\udd17 Ver modelo <code>laion/CLIP-ViT-B-32-laion2B-s34B-b79K</code> CLIP Entrenado en LAION \ud83d\udd17 Ver modelo"},{"location":"hf/Tasks_vc/#como-usar-la-tarea-en-python","title":"\ud83e\uddea C\u00f3mo usar la tarea en Python","text":"<p>Hugging Face proporciona la tarea mediante <code>pipeline</code>. Para poder ejecuarlo necesitamos instalar previamente las librer\u00edas <code>transformers</code>, <code>torch</code> y <code>PIL</code> (CLIPImageProcessor require la librer\u00eda PIL):</p> <p><pre><code>pip install transformers torch pillow\n</code></pre> A continuaci\u00f3n probamos un ejemplo sencillo en el que pasamos una imagen al pipeline y mostramos el resulado por la consola.</p> <p></p> <p>La imagen la pod\u00e9is descargar desde esta url https://unsplash.com/photos/g8oS8-82DxI/download?ixid=MnwxMjA3fDB8MXx0b3BpY3x8SnBnNktpZGwtSGt8fHx8fDJ8fDE2NzgxMDYwODc&amp;force=true&amp;w=640</p> <p><pre><code>from transformers import pipeline\n\nclassifier = pipeline(\"zero-shot-image-classification\", model=\"openai/clip-vit-base-patch32\")\n\nimage_path = \"./images/jevgeni-fil-g8oS8-82DxI-unsplash.jpg\"\ncandidate_labels = [\"fox\", \"bear\", \"seagull\", \"owl\"] #[\"zorro\", \"oso\", \"gaviota\", \"b\u00faho\"]\n\nresults = classifier(image_path, candidate_labels)\nprint(results)\n</code></pre> Predicciones: <pre><code>Using a slow image processor as \"use_fast\" is unset and a slow processor was saved with this model. \n\"use_fast=True\" will be the default behavior in v4.52, even if the model was saved with a slow processor. \nThis will result in minor differences in outputs. You'll still be able to use a slow processor with \"use_fast=False\".\nDevice set to use cpu\n[\n {'score': 0.9991915822029114, 'label': 'owl'}, \n {'score': 0.00041212819633074105, 'label': 'seagull'}, \n {'score': 0.00024944543838500977, 'label': 'bear'}, \n {'score': 0.00014685299538541585, 'label': 'fox'}\n]\n</code></pre></p> <p>Es posible que veamos una advertencia del <code>transformers</code> indicando que el <code>image processor</code>se guard\u00f3 con el modelo m\u00e1s lento (\u201cslow\u201d), y que en una versi\u00f3n futura (v4.52) el valor por defecto ser\u00e1 <code>use_fast=True</code>.</p> <p>Para eliminar la advertencia instalamos las librer\u00edas <code>accelerate</code> y <code>torchvision</code>: <pre><code>pip install accelerate torchvision\n</code></pre> Y retocamos el c\u00f3digo anterior realizando los siguientes ajustes: <pre><code>from transformers import pipeline, AutoImageProcessor\n\nmodel_name = \"openai/clip-vit-base-patch32\"\n\n# Forzamos el uso del procesador \"fast\"\nimage_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n\nclassifier = pipeline(\n    task=\"zero-shot-image-classification\",\n    model=model_name,\n    image_processor=image_processor,  # usamos el procesador r\u00e1pido\n    device_map=\"auto\"  # si tenemos GPU configurada, la usa. Si no, usa la CPU normal\n)\n\nimage_path = \"./images/jevgeni-fil-g8oS8-82DxI-unsplash.jpg\"\ncandidate_labels = [\"fox\", \"bear\", \"seagull\", \"owl\"]\n\nresults = classifier(image_path, candidate_labels)\nprint(results)\n</code></pre> Resultado (ya no se muestra la advertencia): <pre><code>Device set to use cpu\n[\n    {'score': 0.9991913437843323, 'label': 'owl'}, \n    {'score': 0.0004113616014365107, 'label': 'seagull'}, \n    {'score': 0.00025038630701601505, 'label': 'bear'}, \n    {'score': 0.00014685996575281024, 'label': 'fox'}\n]\n</code></pre></p>"},{"location":"hf/Tasks_vc/#actividad-4-clasificacion-zero-shot-con-clip-usando-gradio","title":"\ud83d\udcdd Actividad 4: Clasificaci\u00f3n Zero-Shot con CLIP usando Gradio","text":"<p>Crea una interfaz interactiva con Gradio que permita subir una imagen, introducir etiquetas personalizadas y mostrar las probabilidades para cada etiqueta.</p> <p></p>"},{"location":"hf/Tasks_vc/#requisitos","title":"Requisitos:","text":"<ul> <li>Usar el modelo <code>openai/clip-vit-base-patch32</code>.</li> <li>Implementar la interfaz con Gradio.</li> <li>Mostrar las etiquetas ordenadas por puntuaci\u00f3n (score).</li> </ul>"},{"location":"hf/Tasks_vc/#extensiones-posibles-optativas","title":"\ud83e\udde9 Extensiones posibles (Optativas)","text":"<ul> <li>A\u00f1adir visualizaci\u00f3n de la imagen con la etiqueta m\u00e1s probable superpuesta.</li> <li>Integrar con una API para obtener etiquetas din\u00e1micas.</li> <li>Comparar resultados con otro modelo CLIP entrenado en LAION.</li> </ul>"},{"location":"hf/Tasks_vc_profundidad/","title":"POR HACER","text":""},{"location":"hf/Tasks_vc_profundidad/#4-estimacion-de-profundidad-depth-estimation","title":"4. Estimaci\u00f3n de Profundidad (Depth Estimation)","text":"<ul> <li>Definici\u00f3n: Predice la distancia de cada p\u00edxel respecto a la c\u00e1mara usando solo una imagen.</li> <li>Aplicaciones: Rob\u00f3tica, realidad aumentada, veh\u00edculos aut\u00f3nomos, etc.</li> <li>Modelos populares: DPT, MiDaS</li> </ul> <pre><code># Utiliza el pipeline:\n\nfrom transformers import pipeline\n\ndepth = pipeline(\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n\nresult = depth(\"ruta_o_url_imagen\")\n</code></pre>"},{"location":"hf/npl/01_intro/","title":"\ud83d\udd25 Introducci\u00f3n: El Poder de los Transformers","text":""},{"location":"hf/npl/01_intro/#demo-en-vivo-5-lineas-de-codigo-sencillas","title":"\ud83c\udfac Demo en Vivo: \"5 L\u00edneas de c\u00f3digo sencillas\"","text":""},{"location":"hf/npl/01_intro/#instalacion-rapida","title":"Instalaci\u00f3n R\u00e1pida","text":"<pre><code>pip install transformers torch\n</code></pre>"},{"location":"hf/npl/01_intro/#ejemplo-sencillo","title":"Ejemplo sencillo \u2728","text":"<p><pre><code>from transformers import pipeline\n\n# \u00a1Una l\u00ednea para crear un analizador de sentimientos!\nclassifier = pipeline(\"sentiment-analysis\")\nresult = classifier(\"I loved Star Wars so much!\")\nprint(result)  # [{'label': 'POSITIVE', 'score': 0.9998}]\n</code></pre> Destacar el siguiente mensaje: <pre><code>No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n</code></pre> \u00bfQu\u00e9 acabamos de hacer? En 3 l\u00edneas hemos creado un sistema de IA que entiende emociones humanas. </p>"},{"location":"hf/npl/01_intro/#tareas-nlp-principales","title":"\ud83c\udfaf Tareas NLP Principales","text":"Tarea Pipeline Ejemplo de Uso An\u00e1lisis de Sentimientos <code>sentiment-analysis</code> Redes sociales, reviews Clasificaci\u00f3n de Texto <code>text-classification</code> Categorizar noticias, emails Generaci\u00f3n de Texto <code>text-generation</code> Chatbots, escritura creativa Traducci\u00f3n <code>translation</code> Apps multiidioma Resumen <code>summarization</code> Res\u00famenes autom\u00e1ticos"},{"location":"hf/npl/01_intro/#arquitectura-simplificada","title":"\ud83c\udfd7\ufe0f Arquitectura Simplificada","text":"<pre><code>Texto de Entrada \u2192 Tokenizaci\u00f3n \u2192 Modelo Transformer \u2192 Post-procesado \u2192 Resultado\n</code></pre>"},{"location":"hf/npl/01_intro/#demo-interactiva","title":"\ud83d\ude80 Demo Interactiva:","text":""},{"location":"hf/npl/01_intro/#experimento-1-sentimientos-multiidioma","title":"Experimento 1: Sentimientos Multiidioma","text":"<p><pre><code>classifier = pipeline(\"sentiment-analysis\")\n\ntextos = [\n    \"I love this workshop!\",\n    \"Este taller es aburrido\",\n    \"Je suis tr\u00e8s content\",\n    \"\ud83d\ude0d\ud83c\udf89\u2728\"\n]\n\nfor texto in textos:\n    resultado = classifier(texto)\n    print(f\"{texto} \u2192 {resultado[0]['label']} ({resultado[0]['score']:.2f})\")\n</code></pre> \u00bfFunciona correctamente?</p> <p>No funciona correctamente porque usamos <code>pipeline(\"sentiment-analysis\")</code> sin especificar modelo, as\u00ed que se carga el modelo por defecto de la librer\u00eda, que suele ser un DistilBERT entrenado para sentimiento en ingl\u00e9s (positivo/negativo) sobre un dataset como SST\u20112. La frase \"I love this workshop!\" probablemente se clasifique bien, pero \"Este taller es aburrido\" o \"Je suis tr\u00e8s content\" pueden recibir resultados menos fiables porque el modelo est\u00e1 optimizado para ingl\u00e9s. Los emojis pueden interpretarse, pero de forma limitada.</p> <p>Vamos a modificar el c\u00f3digo especificando, por ejemplo, el modelo <code>tabularisai/multilingual-sentiment-analysis</code> <code>(model=\"tabularisai/multilingual-sentiment-analysis\")</code>. Un modelo entrenado expl\u00edcitamente para an\u00e1lisis de sentimiento multiling\u00fce, pensado para manejar varios idiomas, incluido el espa\u00f1ol. </p> <p>Modificamos el c\u00f3digo y volvemos a probar.</p> <p><pre><code>from transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\", \n                        model=\"tabularisai/multilingual-sentiment-analysis\")\n\ntextos = [\n    \"I love this workshop!\",\n    \"Este taller es aburrido\",\n    \"Je suis tr\u00e8s content\",\n    \"\ud83d\ude0d\ud83c\udf89\u2728\"\n]\n\nfor texto in textos:\n    resultado = classifier(texto)\n    print(f\"{texto} \u2192 {resultado[0]['label']} ({resultado[0]['score']:.2f})\")\n\n#I love this workshop! \u2192 Positive (0.52)\n#Este taller es aburrido \u2192 Negative (0.73)\n#Je suis tr\u00e8s content \u2192 Positive (0.88)\n#\ud83d\ude0d\ud83c\udf89\u2728 \u2192 Neutral (0.34)\n</code></pre> Antes de ejecutar el ejemplo, veremos unas barras de progreso de Hugging Face mientras descarga el modelo y el tokenizador desde el Hub la primera vez que ejecutamoss el <code>pipeline</code>.</p> <ul> <li> <p><code>config.json</code>, <code>model.safetensors</code>, <code>tokenizer_config.json</code>, <code>vocab.txt</code>, <code>tokenizer.json</code>, <code>special_tokens_map.json</code> son los ficheros que necesita el modelo (arquitectura, pesos, vocabulario, configuraci\u00f3n del tokenizer, etc.).\u200b</p> </li> <li> <p>La descarga puede tardar (en nuestros caso ~541\u202fMB de model.safetensors), pero solo se hace la primera vez; despu\u00e9s se reutiliza desde la cach\u00e9 local y ya no veremos esa descarga completa a menos que borremos la cach\u00e9.\u200b</p> </li> </ul> <p>Si tras esas barras de progreso nuestro script se queda \u201cparado\u201d, normalmente es porque sigue ejecutando el pipeline sobre los textos (inferencia); si no aparece nada, revisa que tengas el print(...) dentro del bucle y que no haya errores posteriores.</p> <pre><code>config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 851/851 [00:00&lt;00:00, 9.89MB/s]\nmodel.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 541M/541M [01:10&lt;00:00, 7.64MB/s]\ntokenizer_config.json: 1.20kB [00:00, 2.69MB/s]\nvocab.txt: 996kB [00:00, 9.65MB/s]\ntokenizer.json: 2.92MB [00:00, 25.5MB/s]\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n</code></pre>"},{"location":"hf/npl/01_intro/#experimento-2-generacion-instantanea","title":"Experimento 2: Generaci\u00f3n Instant\u00e1nea","text":"<p>Otra tarea com\u00fan de NLP es la generaci\u00f3n de textos. La tarea de generaci\u00f3n de texto implica la creaci\u00f3n de texto nuevo, coherente y contextualmente relevante basado en un mensaje o entrada determinados. Esta tarea aprovecha los modelos de aprendizaje autom\u00e1tico, particularmente los basados en el aprendizaje profundo (deep learning) y las redes neuronales, para producir texto similar al humano. En el siguiente fragmento de c\u00f3digo, se muestra c\u00f3mo utilizar el modelo openai-community/gpt2 para generar un p\u00e1rrafo de texto basado en una frase inicial: <pre><code>from transformers import pipeline \n\ngenerator = pipeline(\"text-generation\",  \n                     model=\"openai-community/gpt2\") \n\ngenerator(\"In this course, we will teach you how to\")\n</code></pre> Genera la siguiente salida (tengamos en cuenta que la salida ser\u00e1 diferente cada vez que se ejecute el fragmento de c\u00f3digo):  <pre><code>[{'generated_text': 'In this course, we will teach you how to build the best online games or use it to build your own. After this, this course covers: 1) how to make awesome games in Google Play and 2) how to develop a game based on'}] \n</code></pre> Podemos controlar la salida utilizando los par\u00e1metros <code>max_length</code> (el n\u00famero m\u00e1ximo de tokens en el texto generado) y <code>num_return_sequences</code> (n\u00famero de p\u00e1rrafos generados): </p> <pre><code>generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n\nprompt = \"In the future, artificial intelligence\"\n\nresultado = generator(prompt, max_length=50, num_return_sequences=2)\n\nfor i, texto in enumerate(resultado):\n    print(f\"Versi\u00f3n {i+1}: {texto['generated_text']}\")\n</code></pre>"},{"location":"hf/npl/01_intro/#por-que-funciona-tan-bien","title":"\ud83c\udfaf \u00bfPor Qu\u00e9 funciona tan bien?","text":""},{"location":"hf/npl/01_intro/#el-secreto-modelos-pre-entrenados","title":"El Secreto: Modelos Pre-entrenados","text":"<ul> <li>Millones de par\u00e1metros entrenados en enormes datasets</li> <li>Transfer Learning: Conocimiento general aplicado a tareas espec\u00edficas</li> <li>Fine-tuning: Adaptaci\u00f3n a dominios espec\u00edficos</li> </ul>"},{"location":"hf/npl/01_intro/#ventajas-de-hugging-face","title":"Ventajas de Hugging Face","text":"<ul> <li>\u2705 Simplicidad: Una l\u00ednea de c\u00f3digo para tareas complejas</li> <li>\u2705 Variedad: Miles de modelos disponibles</li> <li>\u2705 Comunidad: Modelos compartidos y mejorados constantemente</li> <li>\u2705 Flexibilidad: Desde uso b\u00e1sico hasta personalizaci\u00f3n avanzada</li> </ul>"},{"location":"hf/npl/01_intro/#preparacion-para-los-retos","title":"\ud83c\udfae Preparaci\u00f3n para los retos","text":""},{"location":"hf/npl/01_intro/#estructura-mental-para-los-retos","title":"Estructura Mental para los Retos","text":"<ol> <li>Identifica el problema \u2192 \u00bfQu\u00e9 tarea NLP necesito?</li> <li>Elige el pipeline \u2192 \u00bfCu\u00e1l es el m\u00e1s adecuado?</li> <li>Experimenta \u2192 Prueba con diferentes textos</li> <li>Optimiza \u2192 Ajusta par\u00e1metros y modelos</li> <li>Eval\u00faa \u2192 \u00bfFunciona bien para mi caso de uso?</li> </ol>"},{"location":"hf/npl/01_intro/#listos-para-el-primer-reto","title":"\ud83c\udfc6 \u00a1Listos para el primer reto!","text":"<p>Ahora que hemos visto algunos modelos de NPL en acci\u00f3n, es hora de crear nuestro primer proyecto real: un detector de emociones para redes sociales.</p> <p>\u00bfEl objetivo? Ayudar a una empresa a monitorizar la percepci\u00f3n de su marca en X.</p> <p>\ud83d\udc49 Ir al Reto 1: Detector de Emociones</p>"},{"location":"hf/npl/01_intro/#recursos-adicionales","title":"\ud83d\udcda Recursos Adicionales","text":"<ul> <li>Documentaci\u00f3n oficial de Transformers</li> <li>Hugging Face Model Hub</li> <li>Curso completo de NLP</li> </ul>"},{"location":"hf/npl/02_reto1_sentimientos/","title":"\ud83c\udfc6 Reto 1: Detector de Emociones en Redes Sociales","text":"<p>\u23f1\ufe0f Tiempo: 30 minutos \ud83c\udfaf Nivel: Principiante \ud83d\ude80 Objetivo: Crear un analizador de sentimientos para monitorizar la percepci\u00f3n de marca en X</p>"},{"location":"hf/npl/02_reto1_sentimientos/#contexto-y-motivacion-5-min","title":"\ud83c\udfac Contexto y Motivaci\u00f3n (5 min)","text":""},{"location":"hf/npl/02_reto1_sentimientos/#el-problema-real","title":"El Problema Real","text":"<p>Una startup de tecnolog\u00eda quiere monitorizar qu\u00e9 dice la gente sobre su nueva app en redes sociales. Necesitan:</p> <ul> <li>Detectar comentarios positivos y negativos autom\u00e1ticamente</li> <li>Identificar crisis de reputaci\u00f3n temprano</li> <li>Medir el impacto de sus campa\u00f1as de marketing</li> </ul>"},{"location":"hf/npl/02_reto1_sentimientos/#por-que-es-importante","title":"\u00bfPor Qu\u00e9 es Importante?","text":"<ul> <li>85% de las empresas usan an\u00e1lisis de sentimientos para tomar decisiones</li> <li>Detecci\u00f3n temprana de problemas puede ahorrar millones</li> <li>Automatizaci\u00f3n permite analizar miles de comentarios por minuto</li> </ul>"},{"location":"hf/npl/02_reto1_sentimientos/#teoria-just-in-time-10-min","title":"\ud83e\udde0 Teor\u00eda Just-in-Time (10 min)","text":""},{"location":"hf/npl/02_reto1_sentimientos/#que-es-el-analisis-de-sentimientos","title":"\u00bfQu\u00e9 es el An\u00e1lisis de Sentimientos?","text":"<p>El an\u00e1lisis de sentimientos clasifica texto seg\u00fan la emoci\u00f3n o actitud que expresa:</p> <pre><code>\"\u00a1Me encanta esta app!\" \u2192 POSITIVO (0.95)\n\"Esta app es terrible\" \u2192 NEGATIVO (0.89)\n\"La app funciona bien\" \u2192 NEUTRAL (0.72)\n</code></pre>"},{"location":"hf/npl/02_reto1_sentimientos/#modelos-disponibles-en-hugging-face","title":"Modelos disponibles en Hugging Face","text":"Modelo Idioma Especialidad Uso Recomendado <code>cardiffnlp/twitter-roberta-base-sentiment-latest</code> EN X (Twitter) Redes sociales <code>nlptown/bert-base-multilingual-uncased-sentiment</code> Multi General Textos variados <code>pysentimiento/robertuito-sentiment-analysis</code> ES Espa\u00f1ol Textos en espa\u00f1ol"},{"location":"hf/npl/02_reto1_sentimientos/#parametros-importantes","title":"Par\u00e1metros importantes","text":"<pre><code>classifier = pipeline(\n    \"sentiment-analysis\",\n    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    return_all_scores=True  # Ver todas las probabilidades\n)\n</code></pre>"},{"location":"hf/npl/02_reto1_sentimientos/#implementacion-guiada-10-min","title":"\ud83d\udcbb Implementaci\u00f3n Guiada (10 min)","text":""},{"location":"hf/npl/02_reto1_sentimientos/#paso-0-instalar-pandas-con-pip","title":"Paso 0: Instalar pandas con PIP","text":"<pre><code>pip install pandas\n</code></pre>"},{"location":"hf/npl/02_reto1_sentimientos/#paso-1-configuracion-basica","title":"Paso 1: Configuraci\u00f3n B\u00e1sica","text":"<pre><code>from transformers import pipeline\nimport pandas as pd\n\n# Crear el clasificador\nclassifier = pipeline(\"sentiment-analysis\")\n\n# Datos de ejemplo (simula tweets reales)\ntweets = [\n    \"\u00a1Esta nueva app es incre\u00edble! \ud83d\ude80\",\n    \"La app se cuelga constantemente \ud83d\ude21\",\n    \"Funciona bien, pero podr\u00eda mejorar\",\n    \"\u00a1Gracias por esta herramienta tan \u00fatil! \u2764\ufe0f\",\n    \"No entiendo c\u00f3mo usarla, muy confusa\",\n    \"Perfecta para lo que necesitaba \ud83d\udc4c\"\n]\n</code></pre>"},{"location":"hf/npl/02_reto1_sentimientos/#paso-2-analisis-basico","title":"Paso 2: An\u00e1lisis B\u00e1sico","text":"<pre><code># Analizar cada tweet\nresultados = []\nfor tweet in tweets:\n    resultado = classifier(tweet)\n    resultados.append({\n        'tweet': tweet,\n        'sentimiento': resultado[0]['label'],\n        'confianza': resultado[0]['score']\n    })\n\n# Mostrar resultados\nfor r in resultados:\n    print(f\"Tweet: {r['tweet']}\")\n    print(f\"Sentimiento: {r['sentimiento']} (Confianza: {r['confianza']:.2f})\")\n    print(\"-\" * 50)\n</code></pre> <p>Resultados: <pre><code>No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nDevice set to use cpu\nTweet: \u00a1Esta nueva app es incre\u00edble! \ud83d\ude80\nSentimiento: NEGATIVE (Confianza: 0.98)\n--------------------------------------------------\nTweet: La app se cuelga constantemente \ud83d\ude21\nSentimiento: NEGATIVE (Confianza: 0.92)\n--------------------------------------------------\nTweet: Funciona bien, pero podr\u00eda mejorar\nSentimiento: POSITIVE (Confianza: 0.99)\n--------------------------------------------------\nTweet: \u00a1Gracias por esta herramienta tan \u00fatil! \u2764\ufe0f\nSentimiento: POSITIVE (Confianza: 0.63)\n--------------------------------------------------\nTweet: No entiendo c\u00f3mo usarla, muy confusa\nSentimiento: NEGATIVE (Confianza: 0.98)\n--------------------------------------------------\nTweet: Perfecta para lo que necesitaba \ud83d\udc4c\nSentimiento: POSITIVE (Confianza: 0.98)\n--------------------------------------------------\n</code></pre></p>"},{"location":"hf/npl/02_reto1_sentimientos/#paso-3-analisis-avanzado-con-multiples-modelos","title":"Paso 3: An\u00e1lisis Avanzado con M\u00faltiples Modelos","text":"<pre><code># Comparar diferentes modelos\nmodelos = [\n    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    \"nlptown/bert-base-multilingual-uncased-sentiment\"\n]\n\ndef comparar_modelos(texto, modelos):\n    resultados = {}\n    for modelo in modelos:\n        classifier = pipeline(\"sentiment-analysis\", model=modelo)\n        resultado = classifier(texto)\n        resultados[modelo.split('/')[-1]] = {\n            'label': resultado[0]['label'],\n            'score': resultado[0]['score']\n        }\n    return resultados\n\n# Probar con un tweet espec\u00edfico\ntweet_test = \"Esta app es genial pero tiene algunos bugs\"\ncomparacion = comparar_modelos(tweet_test, modelos)\n\nprint(f\"Tweet: {tweet_test}\")\nfor modelo, resultado in comparacion.items():\n    print(f\"{modelo}: {resultado['label']} ({resultado['score']:.2f})\")\n</code></pre>"},{"location":"hf/npl/02_reto1_sentimientos/#experimentacion-libre-5-min","title":"\ud83c\udfaf Experimentaci\u00f3n Libre (5 min)","text":""},{"location":"hf/npl/02_reto1_sentimientos/#desafios-para-explorar","title":"Desaf\u00edos para Explorar","text":"<ol> <li> <p>Prueba con Emojis: <pre><code>tweets_emojis = [\"\ud83d\ude0d\", \"\ud83d\ude21\", \"\ud83e\udd14\", \"\ud83d\udc4d\", \"\ud83d\udc94\"]\n# \u00bfC\u00f3mo los interpreta el modelo?\n</code></pre></p> </li> <li> <p>Textos Ambiguos: <pre><code>tweets_ambiguos = [\n    \"Esta app es... interesante\",\n    \"Bueno, funciona\",\n    \"No est\u00e1 mal, supongo\"\n]\n</code></pre></p> </li> <li> <p>Diferentes Idiomas: <pre><code>tweets_multiidioma = [\n    \"I love this app!\",\n    \"J'adore cette application!\",\n    \"\u00a1Me encanta esta aplicaci\u00f3n!\"\n]\n</code></pre></p> </li> </ol>"},{"location":"hf/npl/02_reto1_sentimientos/#preguntas-para-reflexionar","title":"Preguntas para Reflexionar","text":"<ul> <li>\u00bfQu\u00e9 modelo funciona mejor para tu caso de uso?</li> <li>\u00bfC\u00f3mo manejas la incertidumbre (scores bajos)?</li> <li>\u00bfQu\u00e9 har\u00edas con sentimientos neutrales?</li> </ul>"},{"location":"hf/npl/02_reto1_sentimientos/#criterios-de-exito","title":"\ud83c\udfc5 Criterios de \u00c9xito","text":"<p>Al completar este reto, deber\u00edas poder:</p> <ul> <li>\u2705 Implementar an\u00e1lisis de sentimientos b\u00e1sico</li> <li>\u2705 Comparar diferentes modelos</li> <li>\u2705 Interpretar scores de confianza</li> <li>\u2705 Crear visualizaciones simples</li> <li>\u2705 Identificar limitaciones del modelo</li> </ul>"},{"location":"hf/npl/02_reto1_sentimientos/#extensiones-opcionales","title":"\ud83d\ude80 Extensiones Opcionales","text":""},{"location":"hf/npl/02_reto1_sentimientos/#para-los-mas-rapidos","title":"Para los M\u00e1s R\u00e1pidos:","text":"<ol> <li>An\u00e1lisis en tiempo real: Conectar con la API de X</li> <li>Alertas autom\u00e1ticas: Notificar cuando el sentimiento baja del 70%</li> <li>An\u00e1lisis temporal: Seguir la evoluci\u00f3n del sentimiento por horas/d\u00edas</li> </ol>"},{"location":"hf/npl/02_reto1_sentimientos/#codigo-de-extension","title":"C\u00f3digo de Extensi\u00f3n:","text":"<pre><code>def monitor_sentimiento(tweets, umbral=0.7):\n    \"\"\"Monitoriza sentimientos y genera alertas\"\"\"\n    negativos = []\n    for tweet in tweets:\n        resultado = classifier(tweet)\n        if resultado[0]['label'] == 'NEGATIVE' and resultado[0]['score'] &gt; umbral:\n            negativos.append({\n                'tweet': tweet,\n                'score': resultado[0]['score']\n            })\n\n    if negativos:\n        print(f\"\ud83d\udea8 ALERTA: {len(negativos)} tweets muy negativos detectados!\")\n        for neg in negativos:\n            print(f\"- {neg['tweet']} (Score: {neg['score']:.2f})\")\n</code></pre>"},{"location":"hf/npl/02_reto1_sentimientos/#proximo-reto","title":"\ud83c\udfaf Pr\u00f3ximo Reto","text":"<p>\u00a1Excelente trabajo! Has creado tu primer sistema de an\u00e1lisis de sentimientos. </p> <p>En el siguiente reto, subiremos el nivel: clasificaremos noticias autom\u00e1ticamente para crear un sistema de organizaci\u00f3n inteligente.</p> <p>\ud83d\udc49 Ir al Reto 2: Clasificador de Noticias</p>"},{"location":"hf/npl/02_reto1_sentimientos/#recursos-del-reto","title":"\ud83d\udcda Recursos del Reto","text":"<ul> <li>Modelos de Sentimientos en Hugging Face</li> <li>Documentaci\u00f3n de Text Classification</li> </ul>"},{"location":"hf/npl/03_reto2_clasificacion/","title":"\ud83c\udfc6 Reto 2: Clasificador Inteligente de Noticias","text":"<p>\u23f1\ufe0f Tiempo: 30 minutos \ud83c\udfaf Nivel: Intermedio \ud83d\ude80 Objetivo: Construir un sistema de categorizaci\u00f3n autom\u00e1tica de noticias</p>"},{"location":"hf/npl/03_reto2_clasificacion/#contexto-y-motivacion-5-min","title":"\ud83c\udfac Contexto y Motivaci\u00f3n (5 min)","text":""},{"location":"hf/npl/03_reto2_clasificacion/#el-problema-real","title":"El problema real","text":"<p>Un peri\u00f3dico digital recibe 500+ art\u00edculos diarios de diferentes fuentes. Su equipo editorial necesita:</p> <ul> <li>Clasificar autom\u00e1ticamente las noticias por categor\u00edas</li> <li>Priorizar noticias importantes para la portada</li> <li>Detectar noticias duplicadas o similares</li> <li>Organizar el contenido para diferentes secciones</li> </ul>"},{"location":"hf/npl/03_reto2_clasificacion/#por-que-es-crucial","title":"\u00bfPor qu\u00e9 es crucial?","text":"<ul> <li>Ahorro de tiempo: De 4 horas manuales a 10 minutos autom\u00e1ticos</li> <li>Consistencia: Clasificaci\u00f3n uniforme sin sesgos humanos</li> <li>Escalabilidad: Manejar vol\u00famenes masivos de informaci\u00f3n</li> <li>Personalizaci\u00f3n: Contenido relevante para cada usuario</li> </ul>"},{"location":"hf/npl/03_reto2_clasificacion/#teoria-just-in-time-10-min","title":"\ud83e\udde0 Teor\u00eda Just-in-Time (10 min)","text":""},{"location":"hf/npl/03_reto2_clasificacion/#clasificacion-de-texto-vs-analisis-de-sentimientos","title":"Clasificaci\u00f3n de Texto vs An\u00e1lisis de Sentimientos","text":"Aspecto An\u00e1lisis de Sentimientos Clasificaci\u00f3n de Texto Objetivo Detectar emociones Categorizar por tema Clases Positivo/Negativo/Neutral Deportes/Pol\u00edtica/Tecnolog\u00eda/etc Complejidad 2-3 categor\u00edas 10+ categor\u00edas Aplicaciones Redes sociales, reviews Noticias, emails, documentos"},{"location":"hf/npl/03_reto2_clasificacion/#modelos-especializados","title":"Modelos Especializados","text":"<pre><code># Modelos populares para clasificaci\u00f3n\nmodelos_clasificacion = {\n    \"noticias_espa\u00f1ol\": \"bertin-project/bertin-roberta-base-spanish\",\n    \"noticias_general\": \"facebook/bart-large-mnli\",\n    \"multiidioma\": \"microsoft/DialoGPT-medium\",\n    \"zero_shot\": \"facebook/bart-large-mnli\"  # \u00a1Sin entrenamiento previo!\n}\n</code></pre>"},{"location":"hf/npl/03_reto2_clasificacion/#zero-shot-classification-la-magia","title":"Zero-Shot Classification: La Magia","text":"<pre><code># \u00a1Clasificar SIN entrenar el modelo!\nclassifier = pipeline(\"zero-shot-classification\")\ntexto = \"El Real Madrid gan\u00f3 3-1 al Barcelona en el Cl\u00e1sico\"\ncategorias = [\"deportes\", \"pol\u00edtica\", \"tecnolog\u00eda\", \"econom\u00eda\"]\n\nresultado = classifier(texto, categorias)\n# Resultado: \"deportes\" con alta confianza\n</code></pre>"},{"location":"hf/npl/03_reto2_clasificacion/#implementacion-guiada-10-min","title":"\ud83d\udcbb Implementaci\u00f3n guiada (10 min)","text":""},{"location":"hf/npl/03_reto2_clasificacion/#paso-1-configuracion-y-datos","title":"Paso 1: Configuraci\u00f3n y Datos","text":"<pre><code>from transformers import pipeline\nimport pandas as pd\nimport numpy as np\n\n# Datos de ejemplo (noticias reales simuladas)\nnoticias = [\n    {\n        \"titulo\": \"El Real Madrid ficha a Mbapp\u00e9 por 180 millones\",\n        \"contenido\": \"El delantero franc\u00e9s firma por cinco temporadas con el club blanco...\"\n    },\n    {\n        \"titulo\": \"Nueva ley de inteligencia artificial aprobada en Europa\",\n        \"contenido\": \"El Parlamento Europeo aprueba regulaciones para el uso de IA...\"\n    },\n    {\n        \"titulo\": \"Bitcoin alcanza nuevo m\u00e1ximo hist\u00f3rico\",\n        \"contenido\": \"La criptomoneda supera los 70.000 d\u00f3lares por primera vez...\"\n    },\n    {\n        \"titulo\": \"Descubren nueva especie de dinosaurio en Argentina\",\n        \"contenido\": \"Paleont\u00f3logos argentinos encuentran restos de un titanosaurio...\"\n    },\n    {\n        \"titulo\": \"Apple presenta el iPhone 16 con IA integrada\",\n        \"contenido\": \"La nueva generaci\u00f3n incluye procesador neuronal avanzado...\"\n    }\n]\n\n# Categor\u00edas objetivo\ncategorias = [\"deportes\", \"pol\u00edtica\", \"econom\u00eda\", \"ciencia\", \"tecnolog\u00eda\"]\n</code></pre>"},{"location":"hf/npl/03_reto2_clasificacion/#paso-2-clasificacion-zero-shot","title":"Paso 2: Clasificaci\u00f3n Zero-Shot","text":"<pre><code># Crear clasificador zero-shot\nclassifier = pipeline(\"zero-shot-classification\", \n                     model=\"facebook/bart-large-mnli\")\n\ndef clasificar_noticia(noticia, categorias):\n    \"\"\"Clasifica una noticia usando zero-shot learning\"\"\"\n    texto_completo = f\"{noticia['titulo']} {noticia['contenido']}\"\n    resultado = classifier(texto_completo, categorias)\n\n    return {\n        'titulo': noticia['titulo'],\n        'categoria_predicha': resultado['labels'][0],\n        'confianza': resultado['scores'][0],\n        'todas_las_scores': dict(zip(resultado['labels'], resultado['scores']))\n    }\n\n# Clasificar todas las noticias\nresultados = []\nfor noticia in noticias:\n    resultado = clasificar_noticia(noticia, categorias)\n    resultados.append(resultado)\n\n    print(f\"\ud83d\udcf0 {resultado['titulo'][:50]}...\")\n    print(f\"\ud83c\udff7\ufe0f  Categor\u00eda: {resultado['categoria_predicha']} ({resultado['confianza']:.2f})\")\n    print(\"-\" * 60)\n</code></pre>"},{"location":"hf/npl/03_reto2_clasificacion/#paso-3-analisis-avanzado-con-multiples-categorias","title":"Paso 3: An\u00e1lisis Avanzado con M\u00faltiples Categor\u00edas","text":"<pre><code>def clasificacion_multinivel(noticia, categorias_principales, subcategorias):\n    \"\"\"Clasificaci\u00f3n jer\u00e1rquica: primero categor\u00eda principal, luego subcategor\u00eda\"\"\"\n\n    # Paso 1: Categor\u00eda principal\n    texto = f\"{noticia['titulo']} {noticia['contenido']}\"\n    resultado_principal = classifier(texto, categorias_principales)\n    categoria_principal = resultado_principal['labels'][0]\n\n    # Paso 2: Subcategor\u00eda (si existe)\n    if categoria_principal in subcategorias:\n        resultado_sub = classifier(texto, subcategorias[categoria_principal])\n        subcategoria = resultado_sub['labels'][0]\n    else:\n        subcategoria = \"general\"\n\n    return {\n        'categoria_principal': categoria_principal,\n        'subcategoria': subcategoria,\n        'confianza_principal': resultado_principal['scores'][0],\n        'ruta_completa': f\"{categoria_principal}/{subcategoria}\"\n    }\n\n# Definir jerarqu\u00eda de categor\u00edas\ncategorias_principales = [\"deportes\", \"tecnolog\u00eda\", \"ciencia\", \"econom\u00eda\", \"pol\u00edtica\"]\nsubcategorias = {\n    \"deportes\": [\"f\u00fatbol\", \"baloncesto\", \"tenis\", \"otros deportes\"],\n    \"tecnolog\u00eda\": [\"inteligencia artificial\", \"m\u00f3viles\", \"software\", \"hardware\"],\n    \"ciencia\": [\"medicina\", \"f\u00edsica\", \"biolog\u00eda\", \"paleontolog\u00eda\"],\n    \"econom\u00eda\": [\"criptomonedas\", \"bolsa\", \"empresas\", \"comercio\"]\n}\n\n# Probar clasificaci\u00f3n multinivel\nfor noticia in noticias[:3]:  # Solo las primeras 3 para el ejemplo\n    resultado = clasificacion_multinivel(noticia, categorias_principales, subcategorias)\n    print(f\"\ud83d\udcf0 {noticia['titulo']}\")\n    print(f\"\ud83d\uddc2\ufe0f  Ruta: {resultado['ruta_completa']}\")\n    print(f\"\ud83d\udcca Confianza: {resultado['confianza_principal']:.2f}\")\n    print(\"-\" * 50)\n</code></pre>"},{"location":"hf/npl/03_reto2_clasificacion/#paso-4-sistema-de-recomendacion-simple","title":"Paso 4: Sistema de Recomendaci\u00f3n Simple","text":"<pre><code>def recomendar_noticias_similares(noticia_objetivo, todas_las_noticias, top_k=3):\n    \"\"\"Encuentra noticias similares bas\u00e1ndose en la clasificaci\u00f3n\"\"\"\n\n    # Clasificar la noticia objetivo\n    resultado_objetivo = clasificar_noticia(noticia_objetivo, categorias)\n    categoria_objetivo = resultado_objetivo['categoria_predicha']\n\n    # Clasificar todas las noticias\n    noticias_clasificadas = []\n    for noticia in todas_las_noticias:\n        if noticia != noticia_objetivo:  # Excluir la noticia objetivo\n            resultado = clasificar_noticia(noticia, categorias)\n            noticias_clasificadas.append(resultado)\n\n    # Filtrar por misma categor\u00eda y ordenar por confianza\n    similares = [n for n in noticias_clasificadas \n                if n['categoria_predicha'] == categoria_objetivo]\n    similares.sort(key=lambda x: x['confianza'], reverse=True)\n\n    return similares[:top_k]\n\n# Probar recomendaciones\nnoticia_test = noticias[0]  # Noticia de f\u00fatbol\nrecomendaciones = recomendar_noticias_similares(noticia_test, noticias)\n\nprint(f\"\ud83c\udfaf Noticia objetivo: {noticia_test['titulo']}\")\nprint(\"\\n\ud83d\udccb Noticias similares recomendadas:\")\nfor i, rec in enumerate(recomendaciones, 1):\n    print(f\"{i}. {rec['titulo']}\")\n    print(f\"   Categor\u00eda: {rec['categoria_predicha']} ({rec['confianza']:.2f})\")\n</code></pre>"},{"location":"hf/npl/03_reto2_clasificacion/#experimentacion-libre-5-min","title":"\ud83c\udfaf Experimentaci\u00f3n Libre (5 min)","text":""},{"location":"hf/npl/03_reto2_clasificacion/#desafios-para-explorar","title":"Desaf\u00edos para Explorar","text":"<ol> <li> <p>Categor\u00edas Personalizadas: <pre><code># Prueba con tus propias categor\u00edas\nmis_categorias = [\"urgente\", \"no urgente\", \"entretenimiento\", \"educativo\"]\n</code></pre></p> </li> <li> <p>Detecci\u00f3n de Fake News: <pre><code>categorias_veracidad = [\"noticia real\", \"posible fake news\", \"s\u00e1tira\"]\n</code></pre></p> </li> <li> <p>An\u00e1lisis de Sentimiento + Clasificaci\u00f3n: <pre><code>def analisis_completo(noticia):\n    # Combinar clasificaci\u00f3n tem\u00e1tica + an\u00e1lisis de sentimientos\n    pass\n</code></pre></p> </li> </ol>"},{"location":"hf/npl/03_reto2_clasificacion/#experimentos-avanzados","title":"Experimentos Avanzados","text":"<pre><code># 1. Clasificaci\u00f3n con confianza m\u00ednima\ndef clasificar_con_umbral(noticia, categorias, umbral_confianza=0.7):\n    resultado = clasificar_noticia(noticia, categorias)\n    if resultado['confianza'] &lt; umbral_confianza:\n        return \"clasificaci\u00f3n_incierta\"\n    return resultado['categoria_predicha']\n\n# 2. Detecci\u00f3n de noticias at\u00edpicas\ndef detectar_noticias_atipicas(noticias, categorias):\n    confianzas = []\n    for noticia in noticias:\n        resultado = clasificar_noticia(noticia, categorias)\n        confianzas.append(resultado['confianza'])\n\n    umbral_atipico = np.percentile(confianzas, 25)  # 25% m\u00e1s bajas\n    return [n for n, c in zip(noticias, confianzas) if c &lt; umbral_atipico]\n</code></pre>"},{"location":"hf/npl/03_reto2_clasificacion/#criterios-de-exito","title":"\ud83c\udfc5 Criterios de \u00c9xito","text":"<p>Al completar este reto, deber\u00edas poder:</p> <ul> <li>\u2705 Implementar clasificaci\u00f3n zero-shot</li> <li>\u2705 Crear sistemas de clasificaci\u00f3n jer\u00e1rquica</li> <li>\u2705 Construir recomendadores simples basados en categor\u00edas</li> <li>\u2705 Manejar m\u00faltiples categor\u00edas y subcategor\u00edas</li> <li>\u2705 Evaluar la confianza de las predicciones</li> </ul>"},{"location":"hf/npl/03_reto2_clasificacion/#extensiones-opcionales","title":"\ud83d\ude80 Extensiones Opcionales","text":""},{"location":"hf/npl/03_reto2_clasificacion/#para-los-mas-rapidos","title":"Para los m\u00e1s r\u00e1pidos:","text":"<ol> <li> <p>Dashboard Interactivo: <pre><code>import gradio as gr\n\n # Se asume que ya tienes definida esta funci\u00f3n y la lista `categorias`\n # def clasificar_noticia(noticia: dict, categorias: list) -&gt; dict:\n #     # devuelve, por ejemplo: {\"categoria_predicha\": \"Pol\u00edtica\", \"confianza\": 0.87}\n #     ...\n # categorias = [\"Pol\u00edtica\", \"Econom\u00eda\", \"Deportes\", \"Tecnolog\u00eda\", \"Cultura\"]\n\n def clasificar_noticia_interface(texto_noticia):\n     if not texto_noticia.strip():\n         return \"Por favor, pega una noticia para clasificar.\", \"\"\n     resultado = clasificar_noticia({\"titulo\": \"\", \"contenido\": texto_noticia}, categorias)\n     categoria = f\"Categor\u00eda: {resultado['categoria_predicha']}\"\n     confianza = f\"Confianza: {resultado['confianza']:.2f}\"\n     return categoria, confianza\n\n with gr.Blocks(title=\"\ud83d\udcf0 Clasificador de Noticias IA\") as demo:\n     gr.Markdown(\"# \ud83d\udcf0 Clasificador de Noticias IA\")\n\n     texto_noticia = gr.Textbox(\n         label=\"Pega aqu\u00ed tu noticia:\",\n         lines=10,\n         placeholder=\"Copia y pega el texto completo de la noticia...\"\n     )\n\n     boton = gr.Button(\"Clasificar\")\n\n     salida_categoria = gr.Markdown()\n     salida_confianza = gr.Markdown()\n\n     boton.click(\n         fn=clasificar_noticia_interface,\n         inputs=texto_noticia,\n         outputs=[salida_categoria, salida_confianza],\n     )\n\n if __name__ == \"__main__\":\n     demo.launch()\n</code></pre></p> </li> <li> <p>API REST Simple: <pre><code>from flask import Flask, request, jsonify\n\napp = Flask(__name__)\n\n@app.route('/clasificar', methods=['POST'])\ndef clasificar_api():\n    data = request.json\n    resultado = clasificar_noticia(data, categorias)\n    return jsonify(resultado)\n</code></pre></p> </li> <li> <p>An\u00e1lisis de Tendencias: <pre><code>def analizar_tendencias_diarias(noticias_por_dia):\n    \"\"\"Analiza qu\u00e9 categor\u00edas son trending cada d\u00eda\"\"\"\n    tendencias = {}\n    for dia, noticias in noticias_por_dia.items():\n        categorias_dia = [clasificar_noticia(n, categorias)['categoria_predicha'] \n                        for n in noticias]\n        tendencias[dia] = pd.Series(categorias_dia).value_counts()\n    return tendencias\n</code></pre></p> </li> </ol>"},{"location":"hf/npl/03_reto2_clasificacion/#proximo-reto","title":"\ud83c\udfaf Pr\u00f3ximo Reto","text":"<p>Has construido un sistema completo de clasificaci\u00f3n de noticias. </p> <p>Para el reto final, vamos a explorar la frontera m\u00e1s emocionante del NLP: la generaci\u00f3n de texto creativo. Crearemos un asistente de escritura que ayude a generar contenido original.</p> <p>\ud83d\udc49 Ir al Reto 3: Asistente de Escritura Creativa</p>"},{"location":"hf/npl/03_reto2_clasificacion/#recursos-del-reto","title":"\ud83d\udcda Recursos del Reto","text":"<ul> <li>Zero-Shot Classification Guide</li> <li>Modelos de Clasificaci\u00f3n</li> <li>BART Model Documentation</li> </ul>"},{"location":"hf/npl/04_reto3_generacion/","title":"\ud83c\udfc6 Reto 3: Asistente de escritura creativa","text":"<p>\u23f1\ufe0f Tiempo: 30 minutos \ud83c\udfaf Nivel: Avanzado \ud83d\ude80 Objetivo: Desarrollar un generador de texto contextual para escritura creativa</p>"},{"location":"hf/npl/04_reto3_generacion/#contexto-y-motivacion-5-min","title":"\ud83c\udfac Contexto y Motivaci\u00f3n (5 min)","text":""},{"location":"hf/npl/04_reto3_generacion/#el-problema-real","title":"El problema real","text":"<p>Una agencia de marketing digital necesita generar contenido constantemente:</p> <ul> <li>50+ posts para redes sociales semanalmente</li> <li>Art\u00edculos de blog personalizados para diferentes clientes</li> <li>Copys publicitarios creativos y \u00fanicos</li> <li>Historias para campa\u00f1as de storytelling</li> </ul>"},{"location":"hf/npl/04_reto3_generacion/#por-que-es-revolucionario","title":"\u00bfPor qu\u00e9 es revolucionario?","text":"<ul> <li>Creatividad aumentada: IA como co-piloto creativo, no reemplazo</li> <li>Velocidad: De horas a minutos para generar borradores</li> <li>Consistencia de marca: Mantener tono y estilo espec\u00edficos</li> <li>Superaci\u00f3n del bloqueo creativo: Inspiraci\u00f3n infinita</li> </ul>"},{"location":"hf/npl/04_reto3_generacion/#teoria-just-in-time-10-min","title":"\ud83e\udde0 Teor\u00eda Just-in-Time (10 min)","text":""},{"location":"hf/npl/04_reto3_generacion/#generacion-de-texto-tipos-y-aplicaciones","title":"Generaci\u00f3n de Texto: Tipos y Aplicaciones","text":"Tipo Descripci\u00f3n Ejemplo de Uso Completado Contin\u00faa un texto iniciado \"Era una noche oscura y...\" Condicional Genera seg\u00fan condiciones \"Escribe un poema sobre el mar\" Conversacional Di\u00e1logo interactivo Chatbots, asistentes Resumen Condensa informaci\u00f3n Res\u00famenes autom\u00e1ticos"},{"location":"hf/npl/04_reto3_generacion/#modelos-de-generacion-populares","title":"Modelos de generaci\u00f3n populares","text":"<pre><code>modelos_generacion = {\n    \"gpt2\": \"gpt2\",  # Cl\u00e1sico, r\u00e1pido\n    \"gpt2_spanish\": \"DeepESP/gpt2-spanish\",  # Especializado en espa\u00f1ol\n    \"bloom\": \"bigscience/bloom-560m\",  # Multiidioma\n    \"flan_t5\": \"google/flan-t5-base\",  # Instrucciones espec\u00edficas\n}\n</code></pre>"},{"location":"hf/npl/04_reto3_generacion/#parametros-clave-para-controlar-la-generacion","title":"Par\u00e1metros clave para controlar la generaci\u00f3n","text":"<pre><code>generator = pipeline(\"text-generation\", model=\"gpt2\")\n\ntexto = generator(\n    \"Hab\u00eda una vez\",\n    max_length=100,        # Longitud m\u00e1xima\n    num_return_sequences=3, # N\u00famero de variaciones\n    temperature=0.8,       # Creatividad (0.1=conservador, 1.5=muy creativo)\n    do_sample=True,        # Activar sampling\n    top_k=50,             # Top-k sampling\n    top_p=0.95,           # Nucleus sampling\n    repetition_penalty=1.2 # Evitar repeticiones\n)\n</code></pre>"},{"location":"hf/npl/04_reto3_generacion/#tecnicas-de-prompting-efectivo","title":"T\u00e9cnicas de Prompting Efectivo","text":"<pre><code># \u274c Prompt b\u00e1sico\n\"Escribe una historia\"\n\n# \u2705 Prompt estructurado\n\"\"\"\nEscribe una historia corta de ciencia ficci\u00f3n que incluya:\n- Protagonista: Una cient\u00edfica joven\n- Escenario: Marte en el a\u00f1o 2150\n- Conflicto: Descubre algo inesperado\n- Tono: Misterioso pero esperanzador\n- Longitud: 200 palabras aproximadamente\n\"\"\"\n</code></pre>"},{"location":"hf/npl/04_reto3_generacion/#implementacion-guiada-10-min","title":"\ud83d\udcbb Implementaci\u00f3n Guiada (10 min)","text":""},{"location":"hf/npl/04_reto3_generacion/#paso-1-configuracion-y-generacion-basica","title":"Paso 1: Configuraci\u00f3n y generaci\u00f3n b\u00e1sica","text":"<pre><code>from transformers import pipeline\nimport random\n\n# Crear generador\ngenerator = pipeline(\"text-generation\", model=\"gpt2\")\n\n# Prompts creativos de ejemplo\nprompts_creativos = [\n    \"En un mundo donde los sue\u00f1os se pueden comprar y vender\",\n    \"La \u00faltima persona en la Tierra recibe un mensaje de radio\",\n    \"Un detective investiga cr\u00edmenes que a\u00fan no han ocurrido\",\n    \"En una biblioteca infinita, cada libro cuenta una vida diferente\",\n    \"El d\u00eda que los robots aprendieron a mentir\"\n]\n\ndef generar_historia(prompt, longitud=150, creatividad=0.8):\n    \"\"\"Genera una historia creativa basada en un prompt\"\"\"\n    resultado = generator(\n        prompt,\n        max_length=longitud,\n        num_return_sequences=1,\n        temperature=creatividad,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        repetition_penalty=1.2,\n        pad_token_id=generator.tokenizer.eos_token_id\n    )\n\n    return resultado[0]['generated_text']\n\n# Generar historias de ejemplo\nprint(\"\ud83c\udfad GENERADOR DE HISTORIAS CREATIVAS\")\nprint(\"=\" * 50)\n\nfor i, prompt in enumerate(prompts_creativos[:3], 1):\n    historia = generar_historia(prompt)\n    print(f\"\\n\ud83d\udcd6 Historia {i}:\")\n    print(f\"\ud83d\udca1 Prompt: {prompt}\")\n    print(f\"\ud83d\udcdd Historia generada:\")\n    print(historia)\n    print(\"-\" * 50)\n</code></pre>"},{"location":"hf/npl/04_reto3_generacion/#paso-2-generador-de-contenido-para-redes-sociales","title":"Paso 2: Generador de contenido para redes sociales","text":"<pre><code>def generar_post_social(tema, plataforma, tono=\"profesional\"):\n    \"\"\"Genera posts optimizados para diferentes redes sociales\"\"\"\n\n    # Plantillas por plataforma\n    plantillas = {\n        \"X\": f\"Hilo sobre {tema}: \ud83e\uddf5\\n1/\",\n        \"linkedin\": f\"Reflexiones sobre {tema} en el mundo profesional:\",\n        \"instagram\": f\"\u2728 {tema} \u2728\\n\\n\",\n        \"facebook\": f\"\u00bfSab\u00edas que {tema}?\"\n    }\n\n    # Ajustar longitud por plataforma\n    longitudes = {\n        \"X\": 100,\n        \"linkedin\": 200,\n        \"instagram\": 150,\n        \"facebook\": 180\n    }\n\n    # Modificadores de tono\n    modificadores_tono = {\n        \"profesional\": \"Usa un lenguaje formal y datos concretos.\",\n        \"casual\": \"Usa un lenguaje relajado y emojis.\",\n        \"inspiracional\": \"Incluye frases motivadoras y llamadas a la acci\u00f3n.\",\n        \"educativo\": \"Explica conceptos de forma clara y did\u00e1ctica.\"\n    }\n\n    prompt_completo = f\"\"\"\n    {plantillas[plataforma]}\n    Tema: {tema}\n    Tono: {tono}\n    Instrucciones: {modificadores_tono[tono]}\n\n    Contenido:\n    \"\"\"\n\n    post = generator(\n        prompt_completo,\n        max_length=longitudes[plataforma],\n        temperature=0.7,\n        do_sample=True,\n        top_k=40,\n        top_p=0.9\n    )[0]['generated_text']\n\n    return post\n\n# Generar posts para diferentes plataformas\ntema = \"inteligencia artificial\"\nplataformas = [\"X\", \"linkedin\", \"instagram\"]\n\nprint(\"\ud83d\udcf1 GENERADOR DE CONTENIDO PARA REDES SOCIALES\")\nprint(\"=\" * 60)\n\nfor plataforma in plataformas:\n    post = generar_post_social(tema, plataforma, \"inspiracional\")\n    print(f\"\\n\ud83d\udcf2 {plataforma.upper()}:\")\n    print(post)\n    print(\"-\" * 40)\n</code></pre>"},{"location":"hf/npl/04_reto3_generacion/#paso-3-asistente-de-escritura-interactivo","title":"Paso 3: Asistente de Escritura Interactivo","text":"<pre><code>class AsistenteEscritura:\n    def __init__(self, modelo=\"gpt2\"):\n        self.generator = pipeline(\"text-generation\", model=modelo)\n        self.historial = []\n\n    def continuar_texto(self, texto_inicial, palabras=50):\n        \"\"\"Contin\u00faa un texto existente\"\"\"\n        resultado = self.generator(\n            texto_inicial,\n            max_length=len(texto_inicial.split()) + palabras,\n            temperature=0.7,\n            do_sample=True,\n            top_k=50\n        )[0]['generated_text']\n\n        # Extraer solo la parte nueva\n        texto_nuevo = resultado[len(texto_inicial):].strip()\n        return texto_nuevo\n\n    def reescribir_con_estilo(self, texto, estilo):\n        \"\"\"Reescribe texto en un estilo espec\u00edfico\"\"\"\n        estilos = {\n            \"formal\": \"Reescribe este texto de manera formal y acad\u00e9mica:\",\n            \"casual\": \"Reescribe este texto de manera informal y amigable:\",\n            \"poetico\": \"Reescribe este texto como si fuera un poema:\",\n            \"periodistico\": \"Reescribe este texto como una noticia:\"\n        }\n\n        prompt = f\"{estilos[estilo]} {texto}\\n\\nVersi\u00f3n reescrita:\"\n\n        resultado = self.generator(\n            prompt,\n            max_length=200,\n            temperature=0.6,\n            do_sample=True\n        )[0]['generated_text']\n\n        return resultado.split(\"Versi\u00f3n reescrita:\")[-1].strip()\n\n    def generar_variaciones(self, texto, num_variaciones=3):\n        \"\"\"Genera m\u00faltiples variaciones de un texto\"\"\"\n        variaciones = []\n        for i in range(num_variaciones):\n            variacion = self.generator(\n                texto,\n                max_length=len(texto.split()) + 30,\n                temperature=0.8 + (i * 0.1),  # Aumentar creatividad\n                do_sample=True,\n                num_return_sequences=1\n            )[0]['generated_text']\n            variaciones.append(variacion)\n\n        return variaciones\n\n    def sugerir_titulos(self, contenido, num_titulos=5):\n        \"\"\"Sugiere t\u00edtulos para un contenido\"\"\"\n        prompt = f\"\"\"\n        Bas\u00e1ndote en este contenido, sugiere {num_titulos} t\u00edtulos atractivos:\n\n        Contenido: {contenido[:200]}...\n\n        T\u00edtulos sugeridos:\n        1.\n        \"\"\"\n\n        resultado = self.generator(\n            prompt,\n            max_length=150,\n            temperature=0.9,\n            do_sample=True\n        )[0]['generated_text']\n\n        return resultado\n\n# Demostraci\u00f3n del asistente\nasistente = AsistenteEscritura()\n\nprint(\"\ud83e\udd16 ASISTENTE DE ESCRITURA CREATIVA\")\nprint(\"=\" * 50)\n\n# Ejemplo 1: Continuar texto\ntexto_inicial = \"La inteligencia artificial est\u00e1 transformando\"\ncontinuacion = asistente.continuar_texto(texto_inicial, 40)\nprint(f\"\ud83d\udcdd Texto inicial: {texto_inicial}\")\nprint(f\"\ud83d\udd04 Continuaci\u00f3n: {continuacion}\")\n\n# Ejemplo 2: Reescribir con estilo\ntexto_base = \"La IA es muy \u00fatil para automatizar tareas repetitivas\"\nversion_poetica = asistente.reescribir_con_estilo(texto_base, \"poetico\")\nprint(f\"\\n\ud83c\udfad Versi\u00f3n po\u00e9tica: {version_poetica}\")\n\n# Ejemplo 3: Generar variaciones\nprint(f\"\\n\ud83d\udd00 Variaciones del texto:\")\nvariaciones = asistente.generar_variaciones(\"El futuro de la tecnolog\u00eda\", 2)\nfor i, var in enumerate(variaciones, 1):\n    print(f\"{i}. {var}\")\n</code></pre>"},{"location":"hf/npl/04_reto3_generacion/#paso-4-evaluador-de-calidad-del-texto","title":"Paso 4: Evaluador de Calidad del Texto","text":"<pre><code>def evaluar_calidad_texto(texto):\n    \"\"\"Eval\u00faa la calidad de un texto generado\"\"\"\n\n    # M\u00e9tricas b\u00e1sicas\n    palabras = len(texto.split())\n    oraciones = len([s for s in texto.split('.') if s.strip()])\n    palabras_por_oracion = palabras / max(oraciones, 1)\n\n    # Diversidad l\u00e9xica (palabras \u00fanicas / total palabras)\n    palabras_unicas = len(set(texto.lower().split()))\n    diversidad_lexica = palabras_unicas / max(palabras, 1)\n\n    # Detecci\u00f3n de repeticiones\n    palabras_lista = texto.lower().split()\n    repeticiones = len(palabras_lista) - len(set(palabras_lista))\n\n    # Puntuaci\u00f3n de calidad (0-100)\n    puntuacion_longitud = min(100, (palabras / 50) * 100)  # \u00d3ptimo: 50 palabras\n    puntuacion_diversidad = diversidad_lexica * 100\n    puntuacion_repeticion = max(0, 100 - (repeticiones * 10))\n\n    puntuacion_total = (puntuacion_longitud + puntuacion_diversidad + puntuacion_repeticion) / 3\n\n    return {\n        'puntuacion_total': round(puntuacion_total, 2),\n        'palabras': palabras,\n        'oraciones': oraciones,\n        'palabras_por_oracion': round(palabras_por_oracion, 1),\n        'diversidad_lexica': round(diversidad_lexica, 2),\n        'repeticiones': repeticiones,\n        'calidad': 'Excelente' if puntuacion_total &gt; 80 else \n                  'Buena' if puntuacion_total &gt; 60 else \n                  'Regular' if puntuacion_total &gt; 40 else 'Necesita mejoras'\n    }\n\n# Evaluar textos generados\ntextos_prueba = [\n    generar_historia(\"En un laboratorio secreto\", 100, 0.7),\n    generar_historia(\"El \u00faltimo d\u00eda de clases\", 100, 0.9)\n]\n\nprint(\"\\n\ud83d\udcca EVALUACI\u00d3N DE CALIDAD\")\nprint(\"=\" * 40)\n\nfor i, texto in enumerate(textos_prueba, 1):\n    evaluacion = evaluar_calidad_texto(texto)\n    print(f\"\\n\ud83d\udcdd Texto {i}:\")\n    print(f\"Puntuaci\u00f3n: {evaluacion['puntuacion_total']}/100 ({evaluacion['calidad']})\")\n    print(f\"Palabras: {evaluacion['palabras']} | Oraciones: {evaluacion['oraciones']}\")\n    print(f\"Diversidad l\u00e9xica: {evaluacion['diversidad_lexica']}\")\n    print(f\"Texto: {texto[:100]}...\")\n</code></pre>"},{"location":"hf/npl/04_reto3_generacion/#experimentacion-libre-5-min","title":"\ud83c\udfaf Experimentaci\u00f3n Libre (5 min)","text":""},{"location":"hf/npl/04_reto3_generacion/#desafios-para-explorar","title":"Desaf\u00edos para Explorar","text":"<ol> <li> <p>Generaci\u00f3n Condicional: <pre><code># Genera texto basado en m\u00faltiples condiciones\ncondiciones = {\n    \"genero\": \"ciencia ficci\u00f3n\",\n    \"protagonista\": \"robot\",\n    \"escenario\": \"espacio\",\n    \"emocion\": \"nostalgia\"\n}\n</code></pre></p> </li> <li> <p>Chatbot Creativo: <pre><code>def chatbot_creativo(mensaje_usuario):\n    prompt = f\"Usuario: {mensaje_usuario}\\nAsistente creativo:\"\n    # Implementar respuesta contextual\n</code></pre></p> </li> <li> <p>Generador de Poes\u00eda: <pre><code>def generar_poema(tema, estilo=\"libre\"):\n    # Haiku, soneto, verso libre, etc.\n    pass\n</code></pre></p> </li> </ol>"},{"location":"hf/npl/04_reto3_generacion/#experimentos-avanzados","title":"Experimentos Avanzados","text":"<pre><code># 1. Control de creatividad din\u00e1mico\ndef generar_con_creatividad_adaptativa(prompt, contexto=\"profesional\"):\n    creatividad_map = {\n        \"profesional\": 0.3,\n        \"creativo\": 0.8,\n        \"experimental\": 1.2\n    }\n    return generator(prompt, temperature=creatividad_map[contexto])\n\n# 2. Generaci\u00f3n colaborativa\ndef escritura_colaborativa(prompts_multiples):\n    \"\"\"Combina m\u00faltiples prompts para crear una narrativa cohesiva\"\"\"\n    historia_completa = \"\"\n    for prompt in prompts_multiples:\n        continuacion = generar_historia(historia_completa + prompt, 50)\n        historia_completa += continuacion\n    return historia_completa\n\n# 3. An\u00e1lisis de estilo\ndef analizar_estilo_autor(texto_muestra):\n    \"\"\"Analiza el estilo de escritura para replicarlo\"\"\"\n    # Implementar an\u00e1lisis de patrones ling\u00fc\u00edsticos\n    pass\n</code></pre>"},{"location":"hf/npl/04_reto3_generacion/#criterios-de-exito","title":"\ud83c\udfc5 Criterios de \u00c9xito","text":"<p>Al completar este reto, deber\u00edas poder:</p> <ul> <li>\u2705 Generar texto creativo con diferentes niveles de creatividad</li> <li>\u2705 Crear contenido espec\u00edfico para diferentes plataformas</li> <li>\u2705 Implementar un asistente de escritura interactivo</li> <li>\u2705 Evaluar la calidad del texto generado</li> <li>\u2705 Controlar el estilo y tono de la generaci\u00f3n</li> </ul>"},{"location":"hf/npl/04_reto3_generacion/#extensiones-opcionales","title":"\ud83d\ude80 Extensiones Opcionales","text":""},{"location":"hf/npl/04_reto3_generacion/#para-los-mas-rapidos","title":"Para los m\u00e1s r\u00e1pidos:","text":"<ol> <li> <p>Interfaz Web Completa: <pre><code>import gradio as gr\n\n # Se asume que ya tienes definida esta funci\u00f3n\n # def generar_historia(prompt: str, creatividad: float) -&gt; str:\n #     ...\n\n def generar_historia_interface(tipo_contenido, prompt, creatividad):\n     if tipo_contenido != \"Historia\":\n         return \"Por ahora solo est\u00e1 implementado el modo 'Historia'.\"\n     if not prompt:\n         return \"Escribe al menos el inicio de la historia.\"\n     return generar_historia(prompt, creatividad=creatividad)\n\n with gr.Blocks(title=\"\u270d\ufe0f Asistente de Escritura IA\") as demo:\n     gr.Markdown(\"# \u270d\ufe0f Asistente de Escritura IA\")\n\n     tipo_contenido = gr.Dropdown(\n         [\"Historia\", \"Post social\", \"Art\u00edculo\", \"Poema\"],\n         value=\"Historia\",\n         label=\"Tipo de contenido\"\n     )\n\n     prompt = gr.Textbox(\n         label=\"Comienza tu historia:\",\n         placeholder=\"\u00c9rase una vez...\",\n         lines=3\n     )\n\n     creatividad = gr.Slider(\n         minimum=0.1,\n         maximum=1.5,\n         value=0.8,\n         step=0.1,\n         label=\"Nivel de creatividad\"\n     )\n\n     boton = gr.Button(\"Generar\")\n     salida = gr.Textbox(label=\"Resultado\", lines=10)\n\n     boton.click(\n         fn=generar_historia_interface,\n         inputs=[tipo_contenido, prompt, creatividad],\n         outputs=salida,\n     )\n\n if __name__ == \"__main__\":\n     demo.launch()\n</code></pre></p> </li> <li> <p>Sistema de Plantillas: <pre><code>plantillas = {\n    \"email_marketing\": \"Asunto: {asunto}\\n\\nHola {nombre},\\n\\n{contenido_principal}\",\n    \"post_blog\": \"# {titulo}\\n\\n## Introducci\u00f3n\\n{intro}\\n\\n## Desarrollo\\n{desarrollo}\",\n    \"historia_corta\": \"Personaje: {personaje}\\nEscenario: {escenario}\\nConflicto: {conflicto}\"\n}\n</code></pre></p> </li> <li> <p>An\u00e1lisis de Sentimientos del Texto Generado: <pre><code>def analizar_tono_generado(texto):\n    sentiment_analyzer = pipeline(\"sentiment-analysis\")\n    resultado = sentiment_analyzer(texto)\n    return resultado[0]['label'], resultado[0]['score']\n</code></pre></p> </li> </ol>"},{"location":"hf/npl/04_reto3_generacion/#proximos-pasos-recomendados","title":"\ud83d\ude80 Pr\u00f3ximos Pasos Recomendados:","text":"<ol> <li>Combina las tres t\u00e9cnicas en un proyecto integrado</li> <li>Explora modelos m\u00e1s avanzados como GPT-3.5 o Claude</li> <li>Implementa fine-tuning para casos de uso espec\u00edficos</li> <li>Crea APIs para integrar en aplicaciones reales</li> </ol>"},{"location":"hf/npl/04_reto3_generacion/#recursos-del-reto","title":"\ud83d\udcda Recursos del Reto","text":"<ul> <li>Text Generation Guide</li> <li>GPT-2 Documentation</li> <li>Prompt Engineering Best Practices</li> </ul>"}]}