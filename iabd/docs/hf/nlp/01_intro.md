---
title: Tasks NLP con los Transformers y pipelines de Hugging Face - Introducci√≥n
description: Apuntes, pr√°cticas, ejercicio del curso de especializaci√≥n en IA y Big Data. 
---

# Tasks de Natural Language Processing (NLP) en Hugging Face

![Tasks of NLP in Hugging Face](./img/tasks_nlp_hf.png)

[Tasks en Hugging Face](https://huggingface.co/tasks)


# üî• Introducci√≥n: El Poder de los Transformers

## üé¨ Demo en Vivo: "5 L√≠neas de c√≥digo sencillas"

### Instalaci√≥n R√°pida
```bash
pip install transformers torch
```
### Ejemplo sencillo ‚ú®

```python {hl_lines="1 4 5" linenums="1"} 
from transformers import pipeline

# En el pipeline especificamos el modelo de An√°lisis de Sentimientos gen√©rico
classifier = pipeline("sentiment-analysis")
result = classifier("I loved Star Wars so much!")
print(result)  # [{'label': 'POSITIVE', 'score': 0.9998}]
```
Destacar el siguiente mensaje:
```bash
No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
```
**¬øQu√© acabamos de hacer?** En 3 l√≠neas hemos creado un sistema de IA que entiende emociones humanas. 

### üéØ Tareas NLP Principales

| Tarea | Pipeline | Ejemplo de Uso |
|-------|----------|----------------|
| **An√°lisis de Sentimientos** | `sentiment-analysis` | Redes sociales, reviews |
| **Clasificaci√≥n de Texto** | `text-classification` | Categorizar noticias, emails |
| **Generaci√≥n de Texto** | `text-generation` | Chatbots, escritura creativa |
| **Traducci√≥n** | `translation` | Apps multiidioma |
| **Resumen** | `summarization` | Res√∫menes autom√°ticos |

### üèóÔ∏è Arquitectura Simplificada
```
Texto de Entrada ‚Üí Tokenizaci√≥n ‚Üí Modelo Transformer ‚Üí Post-procesado ‚Üí Resultado
```
![Arquitectura simplificada](./img/designer.png)

## üöÄ Demo Interactiva: 

### Experimento 1: Sentimientos Multiidioma
```python {hl_lines="1" linenums="1"} 
classifier = pipeline("sentiment-analysis")

textos = [
    "I love this workshop!",
    "Este taller es aburrido",
    "Je suis tr√®s content",
    "üòçüéâ‚ú®"
]

for texto in textos:
    resultado = classifier(texto)
    print(f"{texto} ‚Üí {resultado[0]['label']} ({resultado[0]['score']:.2f})")
```
**¬øFunciona correctamente?**

No funciona correctamente porque usamos `pipeline("sentiment-analysis")` sin especificar modelo, as√≠ que se carga el modelo por defecto de la librer√≠a, que suele ser un `DistilBERT` entrenado para sentimiento en ingl√©s (positivo/negativo) sobre un dataset como `SST‚Äë2`. La frase **"I love this workshop!"** probablemente se clasifique bien, pero **"Este taller es aburrido"** o **"Je suis tr√®s content"** pueden recibir resultados menos fiables porque el modelo est√° optimizado para ingl√©s. Los emojis pueden interpretarse, pero de forma limitada.

Vamos a modificar el c√≥digo especificando, por ejemplo, el modelo [`tabularisai/multilingual-sentiment-analysis`](https://huggingface.co/tabularisai/multilingual-sentiment-analysis) `(model="tabularisai/multilingual-sentiment-analysis")`. Un modelo entrenado expl√≠citamente para an√°lisis de sentimiento multiling√ºe, pensado para manejar varios idiomas, incluido el espa√±ol. 

Modificamos el c√≥digo y volvemos a probar.

```python {linenums="1"}
from transformers import pipeline

classifier = pipeline("sentiment-analysis", 
                        model="tabularisai/multilingual-sentiment-analysis")

textos = [
    "I love this workshop!",
    "Este taller es aburrido",
    "Je suis tr√®s content",
    "üòçüéâ‚ú®"
]

for texto in textos:
    resultado = classifier(texto)
    print(f"{texto} ‚Üí {resultado[0]['label']} ({resultado[0]['score']:.2f})")

#I love this workshop! ‚Üí Positive (0.52)
#Este taller es aburrido ‚Üí Negative (0.73)
#Je suis tr√®s content ‚Üí Positive (0.88)
#üòçüéâ‚ú® ‚Üí Neutral (0.34)
```
Antes de ejecutar el ejemplo, veremos unas barras de progreso de Hugging Face mientras descarga el modelo y el tokenizador desde el Hub la primera vez que ejecutamoss el `pipeline`.

- `config.json`, `model.safetensors`, `tokenizer_config.json`, `vocab.txt`, `tokenizer.json`, `special_tokens_map.json` son los ficheros que necesita el modelo (arquitectura, pesos, vocabulario, configuraci√≥n del tokenizer, etc.).‚Äã

- La descarga puede tardar (en nuestro caso ~541‚ÄØMB de `model.safetensors`), pero solo se hace la primera vez; despu√©s se reutiliza desde la cach√© local y ya no veremos esa descarga completa a menos que borremos la cach√©.‚Äã

Si tras esas barras de progreso nuestro script se queda ‚Äúparado‚Äù, normalmente es porque sigue ejecutando el pipeline sobre los textos (inferencia); si no aparece nada, revisa que tengas el print(...) dentro del bucle y que no haya errores posteriores.

```bash
config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 851/851 [00:00<00:00, 9.89MB/s]
model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 541M/541M [01:10<00:00, 7.64MB/s]
tokenizer_config.json: 1.20kB [00:00, 2.69MB/s]
vocab.txt: 996kB [00:00, 9.65MB/s]
tokenizer.json: 2.92MB [00:00, 25.5MB/s]
special_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
``` 

### Experimento 2: Generaci√≥n Instant√°nea

Otra tarea com√∫n de NLP es la generaci√≥n de textos. La tarea de generaci√≥n de texto implica la creaci√≥n de texto nuevo, coherente y contextualmente relevante basado en un mensaje o entrada determinados. Esta tarea aprovecha los modelos de aprendizaje autom√°tico, particularmente los basados en el aprendizaje profundo (deep learning) y las redes neuronales, para producir texto similar al humano. En el siguiente fragmento de c√≥digo, se muestra c√≥mo utilizar el modelo `[openai-community/gpt2]`(https://huggingface.co/openai-community/gpt2) para generar un p√°rrafo de texto basado en una frase inicial:
```python {hl_lines="2 3 4" linenums="1"}
from transformers import pipeline 
  
generator = pipeline("text-generation",  
                     model="openai-community/gpt2") 

generator("In this course, we will teach you how to")
```
Genera la siguiente salida (tengamos en cuenta que la salida ser√° diferente cada vez que se ejecute el fragmento de c√≥digo): 
```bash
[{'generated_text': 'In this course, we will teach you how to build the best online games or use it to build your own. After this, this course covers: 1) how to make awesome games in Google Play and 2) how to develop a game based on'}] 
``` 
Podemos controlar la salida utilizando los par√°metros `max_length` (el n√∫mero m√°ximo de tokens en el texto generado) y `num_return_sequences` (n√∫mero de p√°rrafos generados): 

```python {hl_lines="1" linenums="1"}
generator = pipeline("text-generation", model="openai-community/gpt2")

prompt = "In the future, artificial intelligence"

resultado = generator(prompt, max_length=50, num_return_sequences=2)

for i, texto in enumerate(resultado):
    print(f"Versi√≥n {i+1}: {texto['generated_text']}")
```

## üéØ ¬øPor qu√© funciona tan bien?

### El Secreto: Modelos Pre-entrenados
- **Millones de par√°metros** entrenados en enormes datasets
- **Transfer Learning**: Conocimiento general aplicado a tareas espec√≠ficas
- **Fine-tuning**: Adaptaci√≥n a dominios espec√≠ficos

### Ventajas de Hugging Face
- ‚úÖ **Simplicidad**: Una l√≠nea de c√≥digo para tareas complejas
- ‚úÖ **Variedad**: Miles de modelos disponibles
- ‚úÖ **Comunidad**: Modelos compartidos y mejorados constantemente
- ‚úÖ **Flexibilidad**: Desde uso b√°sico hasta personalizaci√≥n avanzada

### Retos a realizar 

- üèÜ Reto 1: Detector de Emociones en Redes Sociales
- üèÜ Reto 2: Clasificador Inteligente de Noticias
- üèÜ Reto 3: Asistente de escritura creativa

Ahora que hemos visto algunos modelos de NLP en acci√≥n, es hora de crear nuestro primer proyecto real: **un detector de emociones para redes sociales**.

**¬øEl objetivo?** Ayudar a una empresa a monitorizar la percepci√≥n de su marca en X.

[üëâ Ir al Reto 1: Detector de Emociones](02_reto1_sentimientos.md)

---

## üìö Recursos Adicionales

- [Documentaci√≥n oficial de Transformers](https://huggingface.co/docs/transformers)
- [Hugging Face Model Hub](https://huggingface.co/models)
- [Curso completo de NLP](https://huggingface.co/learn/nlp-course)
