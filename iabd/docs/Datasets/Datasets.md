---
title: Datasets de Hugging Face
description: Apuntes, pr√°cticas, ejercicio del curso de especializaci√≥n en IA y Big Data. 
---

# üìò Hugging Face Datasets: Gu√≠a + Reto Gamificado

## 1Ô∏è‚É£ Introducci√≥n
El paquete **`datasets`** de Hugging Face es una potente herramienta para **acceder, compartir y procesar conjuntos de datos (datasets)** de IA para una amplia gama de tareas, que incluyen:

- Procesamiento del Lenguaje Natural (PLN)
- Visi√≥n por computadora
- Procesamiento de audio

Est√° dise√±ado para manejar **grandes vol√∫menes de datos** de manera eficiente mediante el uso de **mapeo de memoria** y el formato [**Apache Arrow**](https://arrow.apache.org/), lo que permite trabajar con datos que superan la RAM disponible.

> Arrow Apache Arrow define un formato de memoria columnar independiente del lenguaje para datos planos y anidados, organizado para operaciones anal√≠ticas eficientes en hardware moderno como CPU y GPU. El formato de memoria Arrow tambi√©n admite lecturas sin copia para un acceso a datos ultrarr√°pido sin sobrecarga de serializaci√≥n.
>
>El proyecto del formato Apache Arrow comenz√≥ en febrero de 2016, centr√°ndose en cargas de trabajo de an√°lisis columnar en memoria. A diferencia de formatos de archivo como Parquet o CSV, que especifican c√≥mo se organizan los datos en el disco, Arrow se centra en c√≥mo se organizan los datos en la memoria.
![](./images/arrow_vs_partquet_format.png)
>
>Los creadores buscan consolidar Arrow como un formato est√°ndar en memoria para el an√°lisis de cargas de trabajo. Estos fundamentos atraen a numerosos colaboradores de proyectos como Pandas, Spark, Cassandra, Apache Calcite, Dremio e Ibis.
---

## üîë Caracter√≠sticas Clave
- **Vasto Repositorio (Hub):** Gran cantidad de datasets p√∫blicos y privados.
- **F√°cil Acceso:** Carga en una sola l√≠nea de c√≥digo con `load_dataset`.
- **Procesamiento Eficiente:** M√©todos como `map()` paralelizados.
- **Escalabilidad:** Objetos `Dataset` y `IterableDataset`.
- **Gesti√≥n de Datos:** Crear y subir datasets propios al Hub de Hugging    Face.

Los datasets de Hugging Face sirven para:
## ‚úÖ 1. Acceder a datos listos para IA

Hugging Face ofrece un repositorio enorme de conjuntos de datos p√∫blicos y privados para tareas como:

- Procesamiento del Lenguaje Natural (PLN)
- Visi√≥n por computadora
- Audio y multimodalidad

## ‚úÖ 2. Facilitar el preprocesamiento

Permite aplicar transformaciones como:

- Tokenizaci√≥n de texto
- Filtrado y remuestreo
- Conversi√≥n a formatos como Pandas, NumPy, PyTorch y TensorFlow

## ‚úÖ 3. Escalabilidad y eficiencia

Usa Apache Arrow y mapeo de memoria, lo que permite trabajar con datasets que superan la RAM disponible.
Soporta dos tipos:
- Dataset (acceso aleatorio r√°pido)
- IterableDataset (para streaming de datos grandes)

## ‚úÖ 4. Compartir y colaborar

Podemos crear y subir nuestros propios datasets al Hugging Face Hub, con documentaci√≥n y ejemplos. Esto fomenta la reproducibilidad y el trabajo en equipo.

## ‚úÖ 5. Integraci√≥n directa con modelos

Los datasets se integran f√°cilmente con transformers y otros frameworks para entrenamiento y evaluaci√≥n.

---

## ‚öôÔ∏è Instalaci√≥n
```bash
pip install datasets
pip install datasets[audio]
pip install datasets[vision]
```

---

## üß© Ejemplo: Cargar un dataset local
```python {linenums="1"}
from datasets import load_dataset

squad_dataset = load_dataset("json", data_files="train-v2.0-es.json", field="data")

print(squad_dataset)
```

Salida esperada:
```
DatasetDict({
    train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 })
})
```

---

## 2Ô∏è‚É£ Reto Gamificado: Publica tu primer Dataset en Hugging Face

### üéØ Objetivo
Aprender a trabajar con **datasets en Hugging Face**, realizar transformaciones y publicar un dataset en el **Hugging Face Hub**.

1. (RAPIA.3 / CEPIA.3b, CEPIA.3c / 2.5p) Siguiendo el ejemplo inicial trabajado en la sesi√≥n y s√≥lo empleando *Python*:
    1. Descarga los datos de *SquadES* considerando que los datos remotos son los de entrenamiento y validaci√≥n.
    2. Con lo datos de entrenamiento, div√≠delos en entrenamiento y pruebas.
    3. Tras ello, sobre el dataset de entrenamiento, a√±ade una columna a los datos de entrenamiento con la cantidad de p√°rrafos.
    4. Filtra los datos de entrenamiento para que el *dataset* s√≥lo contenga aquellos registros que tienen m√°s de 10 p√°rrafos.
    5. Elimina la columna con la cantidad de p√°rrafos.
    6. Persiste todo el dataset en formato Parquet.
    7. Finalmente, publ√≠calo en *Hugging Face*, editando la tarjeta y poniendo un documento de ejemplo en la documentaci√≥n.

### üïπÔ∏è Niveles del reto
1. **Descarga y explora:** Cargar `SquadES`.
2. **Divide en train/test:** Crear split adicional.
3. **A√±ade columna:** `num_paragraphs`.
4. **Filtra y persiste:** Guardar en Parquet.
5. **Publica en Hugging Face:** A√±adir documentaci√≥n.

---

## üìÇ Plantilla del ejercicio
```python
from datasets import load_dataset

# Nivel 1: Descargar y explorar


# Nivel 2: Dividir en train/test


# Nivel 3: A√±adir columna con cantidad de p√°rrafos


# Nivel 4: Filtrar y persistir

# Nivel 5: Publicar en Hugging Face

```

---

## üîó Recursos
- [Hugging Face Datasets](https://huggingface.co/datasets)
- [Documentaci√≥n oficial](https://huggingface.co/docs/datasets)
