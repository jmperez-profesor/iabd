---
title: Datasets de Hugging Face
description: Apuntes, prÃ¡cticas, ejercicio del curso de especializaciÃ³n en IA y Big Data. 
---

# ğŸ“˜ Hugging Face Datasets: GuÃ­a + Reto Gamificado

## 1ï¸âƒ£ IntroducciÃ³n
El paquete **`datasets`** de Hugging Face es una potente herramienta para **acceder, compartir y procesar conjuntos de datos (datasets)** de IA para una amplia gama de tareas, que incluyen:

- Procesamiento del Lenguaje Natural (PLN)
- VisiÃ³n por computadora
- Procesamiento de audio

EstÃ¡ diseÃ±ado para manejar **grandes volÃºmenes de datos** de manera eficiente mediante el uso de **mapeo de memoria** y el formato [**Apache Arrow**](https://arrow.apache.org/), lo que permite trabajar con datos que superan la RAM disponible.

> Arrow Apache Arrow define un formato de memoria columnar independiente del lenguaje para datos planos y anidados, organizado para operaciones analÃ­ticas eficientes en hardware moderno como CPU y GPU. El formato de memoria Arrow tambiÃ©n admite lecturas sin copia para un acceso a datos ultrarrÃ¡pido sin sobrecarga de serializaciÃ³n.
>
>El proyecto del formato Apache Arrow comenzÃ³ en febrero de 2016, centrÃ¡ndose en cargas de trabajo de anÃ¡lisis columnar en memoria. A diferencia de formatos de archivo como Parquet o CSV, que especifican cÃ³mo se organizan los datos en el disco, Arrow se centra en cÃ³mo se organizan los datos en la memoria.
![](./images/arrow_vs_partquet_format.png)
>
>Los creadores buscan consolidar Arrow como un formato estÃ¡ndar en memoria para el anÃ¡lisis de cargas de trabajo. Estos fundamentos atraen a numerosos colaboradores de proyectos como Pandas, Spark, Cassandra, Apache Calcite, Dremio e Ibis.
---

## ğŸ”‘ CaracterÃ­sticas Clave
- **Vasto Repositorio (Hub):** Gran cantidad de datasets pÃºblicos y privados.
- **FÃ¡cil Acceso:** Carga en una sola lÃ­nea de cÃ³digo con `load_dataset`.
- **Procesamiento Eficiente:** MÃ©todos como `map()` paralelizados.
- **Escalabilidad:** Objetos `Dataset` y `IterableDataset`.
- **GestiÃ³n de Datos:** Crear y subir datasets propios al Hub de Hugging    Face.

Los datasets de Hugging Face sirven para:
## âœ… 1. Acceder a datos listos para IA

Hugging Face ofrece un repositorio enorme de conjuntos de datos pÃºblicos y privados para tareas como:

- Procesamiento del Lenguaje Natural (PLN)
- VisiÃ³n por computadora
- Audio y multimodalidad

## âœ… 2. Facilitar el preprocesamiento

Permite aplicar transformaciones como:

- TokenizaciÃ³n de texto
- Filtrado y remuestreo
- ConversiÃ³n a formatos como Pandas, NumPy, PyTorch y TensorFlow

## âœ… 3. Escalabilidad y eficiencia

Usa Apache Arrow y mapeo de memoria, lo que permite trabajar con datasets que superan la RAM disponible.
Soporta dos tipos:
- Dataset (acceso aleatorio rÃ¡pido)
- IterableDataset (para streaming de datos grandes)

## âœ… 4. Compartir y colaborar

Podemos crear y subir nuestros propios datasets al Hugging Face Hub, con documentaciÃ³n y ejemplos. Esto fomenta la reproducibilidad y el trabajo en equipo.

## âœ… 5. IntegraciÃ³n directa con modelos

Los datasets se integran fÃ¡cilmente con transformers y otros frameworks para entrenamiento y evaluaciÃ³n.

---

## âš™ï¸ InstalaciÃ³n
```bash
pip install datasets
pip install datasets[audio]
pip install datasets[vision]
```

---

## ğŸ§© Ejemplo: Cargar un dataset local
```python {linenums="1"}
from datasets import load_dataset

squad_dataset = load_dataset("json", data_files="train-v2.0-es.json", field="data")

print(squad_dataset)
```

Salida esperada:
```
DatasetDict({
    train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 })
})
```

---

## 2ï¸âƒ£ Reto Gamificado: Publica tu primer Dataset en Hugging Face

### ğŸ¯ Objetivo
Aprender a trabajar con **datasets en Hugging Face**, realizar transformaciones y publicar un dataset en el **Hugging Face Hub**.

### ğŸ•¹ï¸ Niveles del reto
1. **Descarga y explora:** Cargar `SquadES`.
2. **Divide en train/test:** Crear split adicional.
3. **AÃ±ade columna:** `num_paragraphs`.
4. **Filtra y persiste:** Guardar en Parquet.
5. **Publica en Hugging Face:** AÃ±adir documentaciÃ³n.

---

## ğŸ“‚ Plantilla del ejercicio
```python
from datasets import load_dataset

# Nivel 1: Descargar y explorar


# Nivel 2: Dividir en train/test


# Nivel 3: AÃ±adir columna con cantidad de pÃ¡rrafos


# Nivel 4: Filtrar y persistir

# Nivel 5: Publicar en Hugging Face

```

---

## ğŸ”— Recursos
- [Hugging Face Datasets](https://huggingface.co/datasets)
- [DocumentaciÃ³n oficial](https://huggingface.co/docs/datasets)
