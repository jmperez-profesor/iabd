---
title: Actividad 1 Soluci√≥n - Datasets de Hugging Face
description: Apuntes, pr√°cticas, ejercicio del curso de especializaci√≥n en IA y Big Data. 
---

# 1. Descargar los datos de SquadES desde fuente remota
El primer paso es cargar los archivos de entrenamiento y validaci√≥n desde URLs remotas. Utilizaremos la funci√≥n load_dataset() de la librer√≠a ü§ó Datasets, indicando que queremos cargar archivos JSON alojados en GitHub. Los datos remotos se corresponden con train-v2.0-es.json (entrenamiento) y dev-v2.0-es.json (validaci√≥n).

```python {linenums="1"}
from datasets import load_dataset

url = "https://raw.githubusercontent.com/ccasimiro88/TranslateAlignRetrieve/master/SQuAD-es-v2.0/"
data_files = {
    "train": url + "train-v2.0-es.json",
    "val": url + "dev-v2.0-es.json"
}
squad_es = load_dataset("json", data_files=data_files, field="data")  # field="data" porque los datos est√°n bajo esa clave
print(squad_es)
```
Esto produce un objeto DatasetDict con splits train y val, donde cada elemento tiene las claves title y paragraphs.‚Äã

# 2. Dividir los datos de entrenamiento en entrenamiento y prueba
Para realizar una partici√≥n del split de entrenamiento en dos partes (por ejemplo, 90% entrenamiento y 10% prueba), usamos el m√©todo train_test_split():

```python {linenums="1"}
squad_train_full = squad_es["train"]
split_dataset = squad_train_full.train_test_split(test_size=0.1, seed=42)
squad_train = split_dataset["train"]
squad_test = split_dataset["test"]
print(squad_train)
print(squad_test)
```
Ahora disponemos de squad_train (entrenamiento 90%) y squad_test (prueba 10%).‚Äã

# 3. A√±adir una columna con el n√∫mero de p√°rrafos
Podemos emplear el m√©todo map para agregar una columna llamada, por ejemplo, num_paragraphs, contando los elementos en la clave paragraphs de cada ejemplo.

```python {linenums="1"}
squad_train = squad_train.map(lambda x: {"num_paragraphs": len(x["paragraphs"])})
print(squad_train.column_names)  # Debe incluir 'num_paragraphs'
print(squad_train[0]["num_paragraphs"])
```
De este modo, cada registro en el split de entrenamiento tiene la columna con el n√∫mero de p√°rrafos.‚Äã

# 4. Filtrar los ejemplos con m√°s de 10 p√°rrafos
Usaremos el m√©todo filter, pasando una funci√≥n lambda que conserve solo aquellos ejemplos cuya columna num_paragraphs sea mayor que 10:

```python {linenums="1"}
squad_train_large = squad_train.filter(lambda x: x["num_paragraphs"] > 10)
print(squad_train_large)
```
As√≠, el dataset de entrenamiento contiene √∫nicamente los registros relevantes para el criterio pedido.‚Äã

# 5. Eliminar la columna num_paragraphs
Para dejar el dataset limpio, eliminamos la columna extra:

```python {linenums="1"}
squad_train_final = squad_train_large.remove_columns("num_paragraphs")
print(squad_train_final.column_names)
```
Esto deja √∫nicamente las columnas originales: title y paragraphs.‚Äã

# 6. Persistir el dataset en formato Parquet
El m√©todo to_parquet() permite guardar el dataset resultante en disco en formato Parquet, que es eficiente y compatible para grandes vol√∫menes de datos.

```python {linenums="1"}
squad_train_final.to_parquet("squad_train_filtered.parquet")
```
Esto crea el archivo Parquet con los ejemplos filtrados.‚Äã

# 7. Publicar el dataset en Hugging Face
Antes de publicar necesitas autenticarte con tu cuenta (aseg√∫rate de tener instalado huggingface_hub y un token de escritura):

```python {linenums="1"}
from huggingface_hub import login
login()  # Te pedir√° el token
```
A continuaci√≥n, puedes usar el m√©todo push_to_hub del dataset. Opcionalmente, crea primero un DatasetDict si quieres incluir tambi√©n el split de validaci√≥n o test:

```python {linenums="1"}
from datasets import DatasetDict

final_dataset = DatasetDict({
    "train": squad_train_final,
    "test": squad_test
})
```

Sube el dataset (reemplaza <tu_usuario>/squad_es_filtrado por tu nombre de usuario/repositorio en Hugging Face)
final_dataset.push_to_hub("<tu_usuario>/squad_es_filtrado")

# Opcionalmente, a√±ade un ejemplo en la documentaci√≥n editando la "Dataset Card" en la propia web de Hugging Face, tal como recomienda la sesi√≥n[attached_file:1].

> Notas finales
- Durante el proceso, imprime ejemplos y utiliza peque√±os prints para comprobar cada paso.
- La edici√≥n de la tarjeta del dataset ("Dataset Card") se realiza desde la web de Hugging Face: ah√≠ puedes a√±adir un ejemplo, uso previsto y detalles del proceso seguido, favoreciendo la comprensi√≥n de terceros usuarios.
- Todos estos pasos est√°n alineados con la metodolog√≠a y ejemplos detallados en la sesi√≥n te√≥rica enlazada.‚Äã

---

## üîó Recursos
- [Hugging Face Datasets](https://huggingface.co/datasets)
- [Documentaci√≥n oficial](https://huggingface.co/docs/datasets)
